[
["index.html", "Point count data analysis: How to violate assumptions and get away with it Preface About the book and the course About the author Installing R and RStudio Installing required packages Installing the book Acknowledgments", " Point count data analysis: How to violate assumptions and get away with it Peter Solymos 2019-06-24 Preface This book provides material for the workshop Analysis of point-count data in the presence of variable survey methodologies and detection error at the AOS 2019 conference by Peter Solymos. The book and related materials in this repository is the basis of a full day workshop (8 hours long with 3 breaks). Prior exposure to R language is necessary (i.e. basic R object types and their manipulation, such as arrays, data frames, indexing) because this is not covered as part of the course. Check this intro. About the book and the course You’ll learn how to analyze your point count data when it combines different methodologies/protocols/technologies, how to violate assumptions and get away with it. This book/course is aimed towards ornithologists analyzing field observations, who are often faced by data heterogeneities due to field sampling protocols changing from one project to another, or through time over the lifespan of projects, or trying to combine ‘legacy’ data sets with new data collected by recording units. Such heterogeneities can bias analyses when data sets are integrated inadequately, or can lead to information loss when filtered and standardized to common standards. Accounting for these issues is important for better inference regarding status and trend of bird species and communities. Analysts of such ‘messy’ data sets need to feel comfortable with manipulating the data, need a full understanding the mechanics of the models being used (i.e. critically interpreting the results and acknowledging assumptions and limitations), and should be able to make informed choices when faced with methodological challenges. The course emphasizes critical thinking and active learning. Participants will be asked to take part in the analysis: first hand analytics experience from start to finish. We will use publicly available data sets to demonstrate the data manipulation and analysis. We will use freely available and open-source R packages. The expected outcome of the course is a solid foundation for further professional development via increased confidence in applying these methods for field observations. About the author Peter Solymos is an ecologist (molluscs, birds), he is pretty good at stats (modeling, detectability, data cloning, multivariate), an R programmer (vegan, detect, ResourceSelection, pbapply), sometimes he teaches (like the contents of this book). Installing R and RStudio Follow the instructions at the R website to download and install the most up-to-date base R version suitable for your operating system (the latest R version at the time of writing these instructions is 3.6.0). Having RStudio is not absolutely necessary, but some of our course material will follow a syntax that is close to RStudio’s R markdown notation, so having RStudio will make our life easier. RStudio is also available for different operating systems. Pick the open source desktop edition from here (the latest RStudio Desktop version at the time of writing these instructions is 1.2.1335). Installing required packages pkgs &lt;- c(&quot;bookdown&quot;, &quot;detect&quot;, &quot;devtools&quot;, &quot;dismo&quot;, &quot;Distance&quot;, &quot;forecast&quot;, &quot;glmnet&quot;, &quot;gbm&quot;, &quot;intrval&quot;, &quot;knitr&quot;, &quot;lme4&quot;, &quot;maptools&quot;, &quot;mefa4&quot;, &quot;mgcv&quot;, &quot;MuMIn&quot;, &quot;opticut&quot;, &quot;partykit&quot;, &quot;pscl&quot;, &quot;raster&quot;, &quot;ResourceSelection&quot;, &quot;shiny&quot;, &quot;sp&quot;, &quot;unmarked&quot;, &quot;visreg&quot;) to_inst &lt;- setdiff(pkgs, rownames(installed.packages())) if (length(to_inst)) install.packages(to_inst, repos=&quot;https://cloud.r-project.org/&quot;) devtools::install_github(&quot;psolymos/bSims&quot;) devtools::install_github(&quot;psolymos/QPAD&quot;) devtools::install_github(&quot;borealbirds/paired&quot;) devtools::install_github(&quot;borealbirds/lhreg&quot;) still_missing &lt;- setdiff(c(pkgs, &quot;bSims&quot;, &quot;paired&quot;, &quot;lhreg&quot;, &quot;QPAD&quot;), rownames(installed.packages())) if (length(still_missing)) { cat(&quot;The following packages could not be installed:\\n&quot;, paste(&quot;\\t-&quot;, pkgs, collapse=&quot;\\n&quot;), &quot;\\n&quot;) } else { cat(&quot;You are all set! See you at the workshop.\\n&quot;) } Here is a preprint version of Norman Matloff’s The Art of R Programming book: http://heather.cs.ucdavis.edu/~matloff/132/NSPpart.pdf. Check out Chapters 1–6 if you want to brush up your R skills. Installing the book Get a copy of the GitHub repository by cloning it or downloading as zip file from https://github.com/psolymos/qpad-book. We will be taking notes here: https://hackmd.io/@psolymos/aos-2019/ The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) ## clean up bookdown::clean_book(TRUE) ## rendering the book bookdown::render_book(&#39;index.Rmd&#39;, &#39;bookdown::gitbook&#39;) bookdown::render_book(&#39;index.Rmd&#39;, &#39;bookdown::pdf_book&#39;) bookdown::render_book(&#39;index.Rmd&#39;, &#39;bookdown::epub_book&#39;) To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. How this works This is an exercise. This is a note. This is a warning. Acknowledgments List here all the wonderful folks who helped with this book. "],
["intro.html", "Chapter 1 Introduction 1.1 Design-based approaches 1.2 Model-based approaches 1.3 Our approach", " Chapter 1 Introduction All assumptions are violated, but some are more than others A comparison of apples and oranges occurs when two items or groups of items are compared that cannot be practically compared (Wikipedia). The way we measure things can have a big impact on the outcome of that measurement. For example, you might say that “I saw 5 robins walking down the road”, while I might say that “I only saw one robin while sitting on my porch”. Who say more robins? If looking at only the numeric results, you saw more robins than me. But this seems like an apples to oranges comparison. To compare apples to apples, we need to agree on a comparable measurement scheme, or at least figure out how does effort affect the observations. Effort in our example can depend on, e.g. the area of the physical space searched, the amount of time spent, etc. The outcome might further affected by weather, time of year, time of day, location, experience and skill level of the observer. All these factors can affect the observed count. Which brings us to the definition of a point count: a trained observer records all the birds seen and heard from a point count station for a set period of time within a defined distance radius. Point count duration and distance have profound effect on the counts, as shown in Figure 1.1 showing that a 10-min unlimited distance count is roughly 300% increased compared to 3-min 50-m counts (averaged across 54 species of boreal songbirds, (Matsuoka et al. 2014)). Figure 1.1: Effects of duration and distance on mean counts, from (Matsuoka et al. 2014). Point counts are commonly used to answer questions like: How many? (Abundance, density, population size) Is this location part of the range? (0/1) How is abundance changing in space? (Distribution) How is abundance changing in time? (Trend) What is the effect of a treatment on abundance? 1.1 Design-based approaches Standards and recommendations can maximize efficiency in the numbers of birds and species counted, minimize extraneous variability in the counts. But programs started to deviate from standards: “For example, only 3% of 196,000 point counts conducted during the period 1992–2011 across Alaska and Canada followed the standards recommended for the count period and count radius” ((Matsuoka et al. 2014)). Figure 1.2 show how point count protocol varies across the boreal region of North America. Figure 1.2: Survey methodology variation (colors) among contributed projects in the Boreal Avian Modelling (BAM) data base, from (Barker et al. 2015). Exercise In what regard can protocols differ? What might drive protocol variation among projects? Why have we abandoned following protocols? 1.2 Model-based approaches Detection probabilities might vary even with fixed effort (we’ll cover this more later), and programs might have their own goals and constraints (access, training, etc). These constraints would make it almost impossible, and potentially costly to set up very specific standards. Labour intensive methods for unmarked populations have come to the forefront, and computing power of personal computers opened the door for model-based approaches, that can accomodate more variation given enough information in the observed data. These methods often rely on ancillary information and often some sort of replication. Some of the commonly used model-based approaches are: double observer (Nichols et al. 2000), distance sampling (Buckland et al. 2001), removal sampling (Farnsworth et al. 2002), multiple visit occupancy (MacKenzie et al. 2002), multiple visit abundance (Royle 2004). Models come with assumptions, such as: population is closed during multiple visits, observers are independent, all individuals emit cues with identical rates, spatial distribution of individuals is uniform, etc. Although assumptions are everywhere, we are really good at ignoring and violating them. Exercise Can you mention some assumptions from everyday life? Can you explain why we neglect/violate assumptions in these situations? Assumptions are violated, because we seek simplicity. The main question we have to ask: does it matter in practice if we violate the assumptions? 1.3 Our approach In this book and course, we will critically evaluate common assumptions made when analyzing point count data using the following approach: we will introduce a concept, understand how we can infer it from data, then we recreate the situation in silico, and see how the outcome changes as we make different assumptions. It is guaranteed that we will violate every assumption we make. To get away with it, we need to understand how much is too much, and whether it has an impact in practice. If there is a practical consequence, we will look at ways to minimize that effects – so that we can safely ignore the assumption. References "],
["pcdata.html", "Chapter 2 Organizing and Processing Point Count Data 2.1 Introduction 2.2 Prerequisites 2.3 R basics 2.4 JOSM data set 2.5 Cross tabulating species counts 2.6 Joining species data with predictors 2.7 Explore predictor variables 2.8 Derived variables", " Chapter 2 Organizing and Processing Point Count Data All data are messy, but some are missing 2.1 Introduction It is often called data processing, data munging, data wrangling, data cleaning. None of these expressions capture the dread associated with the actual activity. Luckily, this book and course is not about data manipulation, and we can gladly skip these steps. But keep in mind, in real life, you’ll often have to worry about at least four things that can go wrong: space (e.g. wrong UTM zones, errors), time (ISO format please), taxonomy (unknowns, mis-identifications), something else (if there were no errors, check again). This chapter sets you up for the required data skills for the rest of the book/course. 2.2 Prerequisites library(mefa4) # data manipulation library(raster) # reading raster files library(sp) # spatial data manipulation load(&quot;_data/josm/josm.rda&quot;) # load bird data rr &lt;- stack(&quot;_data/josm/landcover-hfi2016.grd&quot;) # rasters 2.3 R basics This short document is intended to help you brush up your R skills. If you feel that these R basics are not very familiar, I suggest to take a look at some introductory R books, sich as this preprint version of Norman Matloff’s The Art of R Programming book: http://heather.cs.ucdavis.edu/~matloff/132/NSPpart.pdf, check out Chapters 1–6. R is a great calculator: 1 + 2 Assign a value and print an object using = or &lt;- (preferred in this book): (x = 2) # shorthand for print print(x) x == 2 # logical operator, not assignment y &lt;- x + 0.5 y # another way to print Logical operators come handy: x == y # equal x != y # not eaqual x &lt; y # smaller than x &gt;= y # greater than or equal Vectors and sequences are created most often by the functions c, :, seq, and rep: x &lt;- c(1, 2, 3) x 1:3 seq(1, 3, by = 1) rep(1, 5) rep(1:2, 5) rep(1:2, each = 5) When doing operations with vectors remember that values of the shorter object are recycled: x + 0.5 x * c(10, 11, 12, 13) Indexing and ordering vectors is a a fundamental skill: x[1] x[c(1, 1, 1)] # a way of repeatig values x[1:2] x[x != 2] x[x == 2] x[x &gt; 1 &amp; x &lt; 3] order(x, decreasing=TRUE) x[order(x, decreasing=TRUE)] rev(x) # reverse See how NA values can influence sorting character vectors: z &lt;- c(&quot;b&quot;, &quot;a&quot;, &quot;c&quot;, NA) z[z == &quot;a&quot;] z[!is.na(z) &amp; z == &quot;a&quot;] z[is.na(z) | z == &quot;a&quot;] is.na(z) which(is.na(z)) sort(z) sort(z, na.last=TRUE) There are a few special values: as.numeric(c(&quot;1&quot;, &quot;a&quot;)) # NA: not available (missing or invalid) 0/0 # NaN: not a number 1/0 # Inf -1/0 # -Inf Matrices and arrays are vectors with dimensions, elements are in same mode: (m &lt;- matrix(1:12, 4, 3)) matrix(1:12, 4, 3, byrow=TRUE) array(1:12, c(2, 2, 3)) Many objects have attributes: dim(m) dim(m) &lt;- NULL m dim(m) &lt;- c(4, 3) m dimnames(m) &lt;- list(letters[1:4], LETTERS[1:3]) m attributes(m) Matrice and indices: m[1:2,] m[1,2] m[,2] m[,2,drop=FALSE] m[2] m[rownames(m) == &quot;c&quot;,] m[rownames(m) != &quot;c&quot;,] m[rownames(m) %in% c(&quot;a&quot;, &quot;c&quot;, &quot;e&quot;),] m[!(rownames(m) %in% c(&quot;a&quot;, &quot;c&quot;, &quot;e&quot;)),] Lists and indexing: l &lt;- list(m = m, x = x, z = z) l l$ddd &lt;- sqrt(l$x) l[2:3] l[[&quot;ddd&quot;]] Data frames are often required for statistical modeling. A data frame is a list where length of elements match and elements can be in different mode. d &lt;- data.frame(x = x, sqrt_x = sqrt(x)) d Inspect structure of R objects: str(x) str(z) str(m) str(l) str(d) str(as.data.frame(m)) str(as.list(d)) Get summaries of these objects: summary(x) summary(z) summary(m) summary(l) summary(d) 2.4 JOSM data set The data is based on the project Cause-Effect Monitoring Migratory Landbirds at Regional Scales: understand how boreal songbirds are affected by human activity in the oil sands area (2.1 and 2.2, (Mahon et al. 2016)). JOSM stands for Joint Oil Sands Monitoring. Look at the source code in the _data/josm directory of the book if you are interested in data processing details. We skip that for now. Figure 2.1: JOSM bird data survey locations from (Mahon et al. 2016). Figure 2.2: Survey area boundary, habitat types and human footprint mapping (Mahon et al. 2016). Surveys were spatially replicated because: we want to make inferences about a population, full census is out of reach, thus we take a sample of the population that is representative and random. Ideally, sample size should be as large as possible, it reduces variability and increases statistical power. Survey locations were pucked based on various criteria: stratification (land cover), gradients (disturbance levels), random location (control for unmeasured effects), take into account historical surveys (avoid, or revisit), access, cost (clusters). The josm obejct is a list with 3 elements: surveys: data frame with survey specific information, species: lookup table for species, counts: individual counts by survey and species. names(josm) ## [1] &quot;surveys&quot; &quot;species&quot; &quot;counts&quot; Species info: species codes, common and scientific names. The table could also contain taxonomic, trait, etc. information as well. head(josm$species) At the survey level, we have coordinates, date/time info, variables capturing survey conditions, and land cover info extracted from 1 km\\(^2\\) resolution rasters. colnames(josm$surveys) ## [1] &quot;SiteID&quot; &quot;SurveyArea&quot; &quot;Longitude&quot; &quot;Latitude&quot; ## [5] &quot;Date&quot; &quot;StationID&quot; &quot;ObserverID&quot; &quot;TimeStart&quot; ## [9] &quot;VisitID&quot; &quot;WindStart&quot; &quot;PrecipStart&quot; &quot;TempStart&quot; ## [13] &quot;CloudStart&quot; &quot;WindEnd&quot; &quot;PrecipEnd&quot; &quot;TempEnd&quot; ## [17] &quot;CloudEnd&quot; &quot;TimeFin&quot; &quot;Noise&quot; &quot;OvernightRain&quot; ## [21] &quot;DateTime&quot; &quot;SunRiseTime&quot; &quot;SunRiseFrac&quot; &quot;TSSR&quot; ## [25] &quot;OrdinalDay&quot; &quot;DAY&quot; &quot;Open&quot; &quot;Water&quot; ## [29] &quot;Agr&quot; &quot;UrbInd&quot; &quot;SoftLin&quot; &quot;Roads&quot; ## [33] &quot;Decid&quot; &quot;OpenWet&quot; &quot;Conif&quot; &quot;ConifWet&quot; The count table contains one row for each unique individual of a species (SpeciesID links to the species lookup table) observed during a survey (StationID links to the survey attribute table). Check the data dictionary in _data/josm folder for a detailed explanation of each column. str(josm$counts) ## &#39;data.frame&#39;: 52372 obs. of 18 variables: ## $ ObservationID: Factor w/ 57024 levels &quot;CL10102-130622-001&quot;,..: 1 2 3 4 5 6 8 9 10 11 ... ## $ SiteID : Factor w/ 4569 levels &quot;CL10102&quot;,&quot;CL10106&quot;,..: 1 1 1 1 1 1 1 1 2 2 ... ## $ StationID : Factor w/ 4569 levels &quot;CL10102-1&quot;,&quot;CL10106-1&quot;,..: 1 1 1 1 1 1 1 1 2 2 ... ## $ TimeInterval : int 1 1 1 1 5 5 1 1 1 1 ... ## $ Direction : int 1 2 2 2 1 4 4 4 1 1 ... ## $ Distance : int 1 2 2 1 3 3 2 1 1 1 ... ## $ DetectType1 : Factor w/ 3 levels &quot;C&quot;,&quot;S&quot;,&quot;V&quot;: 2 2 2 2 1 1 2 2 2 2 ... ## $ DetectType2 : Factor w/ 3 levels &quot;C&quot;,&quot;S&quot;,&quot;V&quot;: NA NA NA NA NA NA NA NA NA NA ... ## $ DetectType3 : Factor w/ 3 levels &quot;C&quot;,&quot;S&quot;,&quot;V&quot;: NA NA NA NA NA NA NA NA NA NA ... ## $ Sex : Factor w/ 4 levels &quot;F&quot;,&quot;M&quot;,&quot;P&quot;,&quot;U&quot;: 2 2 2 2 4 4 2 2 2 2 ... ## $ Age : Factor w/ 6 levels &quot;A&quot;,&quot;F&quot;,&quot;J&quot;,&quot;JUV&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Activity1 : Factor w/ 17 levels &quot;BE&quot;,&quot;CF&quot;,&quot;CH&quot;,..: 5 5 5 5 NA NA NA 5 5 NA ... ## $ Activity2 : Factor w/ 17 levels &quot;48&quot;,&quot;BE&quot;,&quot;CF&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## $ Activity3 : Factor w/ 7 levels &quot;CF&quot;,&quot;DC&quot;,&quot;DR&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## $ ActivityNote : Factor w/ 959 levels &quot;AGITATED&quot;,&quot;AGITATED CALLING&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## $ Dur : Factor w/ 3 levels &quot;0-3min&quot;,&quot;3-5min&quot;,..: 1 1 1 1 3 3 1 1 1 1 ... ## $ Dis : Factor w/ 3 levels &quot;0-50m&quot;,&quot;50-100m&quot;,..: 1 2 2 1 3 3 2 1 1 1 ... ## $ SpeciesID : Factor w/ 150 levels &quot;ALFL&quot;,&quot;AMBI&quot;,..: 107 95 95 107 46 43 140 95 125 38 ... 2.5 Cross tabulating species counts Take the following dummy data frame (long format): (d &lt;- data.frame( sample=factor(paste0(&quot;S&quot;, c(1,1,1,2,2)), paste0(&quot;S&quot;, 1:3)), species=c(&quot;BTNW&quot;, &quot;OVEN&quot;, &quot;CANG&quot;, &quot;AMRO&quot;, &quot;CANG&quot;), abundance=c(1, 1, 2, 1, 1), behavior=rep(c(&quot;heard&quot;,&quot;seen&quot;), c(4, 1)))) str(d) ## &#39;data.frame&#39;: 5 obs. of 4 variables: ## $ sample : Factor w/ 3 levels &quot;S1&quot;,&quot;S2&quot;,&quot;S3&quot;: 1 1 1 2 2 ## $ species : Factor w/ 4 levels &quot;AMRO&quot;,&quot;BTNW&quot;,..: 2 4 3 1 3 ## $ abundance: num 1 1 2 1 1 ## $ behavior : Factor w/ 2 levels &quot;heard&quot;,&quot;seen&quot;: 1 1 1 1 2 We want to add up the abundances for each sample (rows) and species (column): (y &lt;- Xtab(abundance ~ sample + species, d)) ## 3 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## AMRO BTNW CANG OVEN ## S1 . 1 2 1 ## S2 1 . 1 . ## S3 . . . . y is a sparse matrix, that is a very compact representation: object.size(d[,1:3]) ## 2328 bytes object.size(y) ## 2160 bytes Notice that we have 3 rows, but d$sample did not have an S3 value, but it was a level. We can drop such unused levels, but it is generally not recommended, and we need to be careful not to drop samples where no species was detected (this can happen quite often depending on timing of surveys) Xtab(abundance ~ sample + species, d, drop.unused.levels = TRUE) ## 2 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## AMRO BTNW CANG OVEN ## S1 . 1 2 1 ## S2 1 . 1 . A sparse matrix can be converted to ordinary matrix as.matrix(y) ## AMRO BTNW CANG OVEN ## S1 0 1 2 1 ## S2 1 0 1 0 ## S3 0 0 0 0 The nice thing about this cross tabulation is that we can finter the records without changing the structure (rows, columns) of the table: Xtab(abundance ~ sample + species, d[d$behavior == &quot;heard&quot;,]) ## 3 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## AMRO BTNW CANG OVEN ## S1 . 1 2 1 ## S2 1 . . . ## S3 . . . . Xtab(abundance ~ sample + species, d[d$behavior == &quot;seen&quot;,]) ## 3 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## AMRO BTNW CANG OVEN ## S1 . . . . ## S2 . . 1 . ## S3 . . . . Now let’s do this for the real data. We have no abundance column, because each row stands for exactly one individual. We can add a column with 1’s, or we can just count the number of rows by using only the right-hand-side of the formula in Xtab. ytot will be our total count matrix for now. We also want to filter the records to contain only Songs and Calls, without Vvisual detections: table(josm$counts$DetectType1, useNA=&quot;always&quot;) ## ## C S V &lt;NA&gt; ## 9180 41808 1384 0 We use SiteID for row names, because only 1 station and visit was done at each site: ytot &lt;- Xtab(~ SiteID + SpeciesID, josm$counts[josm$counts$DetectType1 != &quot;V&quot;,]) See how not storing 0’s affect size compared to the long formar and an ordinary wide matrix ## 2-column data frame as reference tmp &lt;- as.numeric(object.size( josm$counts[josm$counts$DetectType1 != &quot;V&quot;, c(&quot;StationID&quot;, &quot;SpeciesID&quot;)])) ## spare matrix as.numeric(object.size(ytot)) / tmp ## [1] 0.1366 ## dense matrix as.numeric(object.size(as.matrix(ytot))) / tmp ## [1] 1.106 ## matrix fill sum(ytot &gt; 0) / prod(dim(ytot)) ## [1] 0.04911 Check if counts are as expected: max(ytot) # this is interesting ## [1] 200 sort(apply(as.matrix(ytot), 2, max)) # it is CANG ## BUFF BWTE COGO COHA DCCO GWTE HOLA NHOW NSHO RTHU WWSC CANV NOPI AMBI AMCO ## 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 ## AMGO BAEA BAOR BEKI BOWA CONI CSWA EAPH GBHE GCTH GGOW GHOW HOWR LEOW MERL ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## NESP NOGO NOHA NSWO PBGR RBGU RTHA SAVS SPSA WBNU BRBL CAGU MYWA SNBU VEER ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## AMKE AMWI BADO BARS BBWO BHCO BLBW BLPW BLTE BWHA COGR DOWO EAKI HAWO KILL ## 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## LEYE NAWA NOPO OSFL OSPR PIWO PUFI RNDU SORA SSHA COSN AMCR AMRO ATTW BHVI ## 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 ## BOCH BRCR BTNW CMWA FOSP FRGU GCKI MAWR MOWA NOFL PHVI SACR SOSA SOSP SPGR ## 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## TRES WETA WIWA WIWR YBSA FOTE BAWW BBWA BCCH BLJA CAWA CONW COTE GRYE NOWA ## 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 ## NRWS OCWA REVI RNGR RUBL RWBL WAVI WEWP WISN YBFL YWAR ALFL AMRE CHSP CORA ## 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 ## EVGR HETH LCSP RBGR RBNU RCKI SWSP CCSP COYE DEJU LEFL LISP MAWA OVEN RUGR ## 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 ## SWTH BOGU MALL GRAJ PAWA WTSP YRWA COLO TEWA AMPI WWCR CEDW PISI RECR CANG ## 6 7 7 8 8 8 8 9 12 12 20 23 50 51 200 ## lyover (FO) flock (FL) beyond 100m distance head(josm$counts[ josm$counts$SiteID == rownames(ytot)[which(ytot[,&quot;CANG&quot;] == 200)] &amp; josm$counts$SpeciesID == &quot;CANG&quot;,]) We can check overall mean counts: round(sort(colMeans(ytot)), 4) ## BUFF BWTE COGO COHA DCCO GWTE HOLA NHOW NSHO RTHU ## 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 ## WWSC CANV NOPI GBHE GCTH GHOW LEOW NOHA RBGU BRBL ## 0.0000 0.0000 0.0000 0.0002 0.0002 0.0002 0.0002 0.0002 0.0002 0.0002 ## CAGU AMCO BAEA BARS NESP NOGO NOPO NSWO RNDU SNBU ## 0.0002 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 ## VEER BEKI CSWA MERL SAVS SSHA MYWA AMKE BAOR OSPR ## 0.0004 0.0007 0.0007 0.0007 0.0007 0.0007 0.0007 0.0009 0.0009 0.0009 ## SPGR WBNU AMGO AMWI BOWA CONI EAPH HOWR NRWS BLTE ## 0.0009 0.0009 0.0011 0.0011 0.0011 0.0011 0.0011 0.0011 0.0011 0.0013 ## COGR EAKI GGOW NAWA COSN COTE FRGU MAWR FOTE KILL ## 0.0013 0.0013 0.0013 0.0013 0.0013 0.0015 0.0015 0.0015 0.0015 0.0018 ## RTHA BADO BLBW AMBI PBGR SPSA AMPI BHCO BWHA SOSP ## 0.0020 0.0024 0.0024 0.0028 0.0028 0.0028 0.0028 0.0031 0.0037 0.0042 ## RUBL MALL PUFI DOWO SORA LEYE ATTW HAWO RNGR BBWO ## 0.0044 0.0046 0.0048 0.0059 0.0068 0.0094 0.0096 0.0101 0.0101 0.0107 ## BLJA BOGU AMCR EVGR RWBL OSFL LCSP TRES FOSP WEWP ## 0.0134 0.0140 0.0166 0.0169 0.0169 0.0186 0.0193 0.0201 0.0217 0.0232 ## WIWA PIWO RECR SOSA YWAR GCKI BLPW CAWA SACR BTNW ## 0.0236 0.0256 0.0269 0.0269 0.0291 0.0304 0.0306 0.0315 0.0322 0.0335 ## NOWA OCWA BRCR CCSP COLO PHVI CONW CEDW RUGR MOWA ## 0.0341 0.0359 0.0381 0.0385 0.0387 0.0394 0.0429 0.0449 0.0475 0.0477 ## WAVI BCCH BOCH NOFL SWSP GRYE WWCR AMRO RBNU BBWA ## 0.0582 0.0593 0.0593 0.0622 0.0659 0.0685 0.0751 0.0757 0.0766 0.0810 ## CMWA BHVI COYE YBFL YBSA AMRE BAWW LEFL WETA WISN ## 0.0812 0.0814 0.0814 0.0873 0.0878 0.0889 0.0963 0.0974 0.1086 0.1280 ## CORA WIWR ALFL MAWA PISI RBGR LISP DEJU GRAJ CANG ## 0.1401 0.1466 0.1582 0.1727 0.1775 0.1832 0.2169 0.2725 0.2898 0.3018 ## PAWA REVI RCKI HETH CHSP SWTH WTSP OVEN YRWA TEWA ## 0.3053 0.3344 0.3898 0.4344 0.4460 0.7402 0.8091 0.8831 0.8934 1.2221 2.6 Joining species data with predictors Let’s join the species counts with the survey attributes. This is how we can prepare the input data for regression analysis. spp &lt;- &quot;OVEN&quot; # which species josm$species[spp,] compare_sets(rownames(josm$surveys),rownames(ytot)) ## xlength ylength intersect union xbutnoty ybutnotx ## labels 4569 4569 4569 4569 0 0 ## unique 4569 4569 4569 4569 0 0 x &lt;- josm$surveys x$y &lt;- as.numeric(ytot[rownames(x), spp]) 2.7 Explore predictor variables Put the locations on the map: xy &lt;- x[,c(&quot;Longitude&quot;, &quot;Latitude&quot;)] coordinates(xy) &lt;- ~ Longitude + Latitude proj4string(xy) &lt;- &quot;+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs&quot; xy &lt;- spTransform(xy, proj4string(rr)) col &lt;- colorRampPalette(c(&quot;lightgrey&quot;, &quot;blue&quot;))(100) plot(rr[[&quot;Water&quot;]], col=col, axes=FALSE, box=FALSE, legend=FALSE) plot(xy, add=TRUE, pch=19, cex=0.5) Exercise Explore the data to understand the distributions and associations. Use summary, table, hist, plot (bivariate, scatterplot matrix), etc. 2.8 Derived variables Add up some of the compositional variables into meaningful units: x$FOR &lt;- x$Decid + x$Conif+ x$ConifWet # forest x$AHF &lt;- x$Agr + x$UrbInd + x$Roads # &#39;alienating&#39; human footprint x$WET &lt;- x$OpenWet + x$ConifWet + x$Water # wet + water Classify surveys locations based on dominant land cover type: cn &lt;- c(&quot;Open&quot;, &quot;Water&quot;, &quot;Agr&quot;, &quot;UrbInd&quot;, &quot;SoftLin&quot;, &quot;Roads&quot;, &quot;Decid&quot;, &quot;OpenWet&quot;, &quot;Conif&quot;, &quot;ConifWet&quot;) h &lt;- find_max(x[,cn]) hist(h$value) table(h$index) ## ## Open Water Agr UrbInd SoftLin Roads Decid OpenWet ## 12 10 4 14 0 2 2084 160 ## Conif ConifWet ## 745 1538 x$HAB &lt;- droplevels(h$index) # drop empty levels References "],
["regression.html", "Chapter 3 A Primer in Regression Techniques 3.1 Introduction 3.2 Prerequisites 3.3 Poisson null model 3.4 Exploring covariates 3.5 Single covariate 3.6 Additive model 3.7 Nonlinear terms 3.8 Categorical variables 3.9 Multiple main effects 3.10 Interaction 3.11 Different error distributions 3.12 Count duration effects 3.13 Count radius effects 3.14 Offsets", " Chapter 3 A Primer in Regression Techniques All models are wrong, but some are useful – Box 3.1 Introduction This chapter will provide all the foundations we need for the coming chapters. It is not intended as a general and all-exhaustive introduction to regression techniques, but rather the minimum requirement moving forwards. We will also hone our data processing and plotting skills. 3.2 Prerequisites library(mefa4) # data manipulation library(mgcv) # GAMs library(pscl) # zero-inflated models library(lme4) # GLMMs library(MASS) # Negative Binomial GLM library(partykit) # regression trees library(intrval) # interval magic library(opticut) # optimal partitioning library(visreg) # regression visualization library(ResourceSelection) # marginal effects library(MuMIn) # multi-model inference source(&quot;functions.R&quot;) # some useful stuff load(&quot;_data/josm/josm.rda&quot;) # JOSM data Let’s pick a species, Ovenbird (OVEN), that is quite common and abundant in the data set. We put together a little data set to work with: spp &lt;- &quot;OVEN&quot; ytot &lt;- Xtab(~ SiteID + SpeciesID, josm$counts[josm$counts$DetectType1 != &quot;V&quot;,]) ytot &lt;- ytot[,colSums(ytot &gt; 0) &gt; 0] x &lt;- data.frame( josm$surveys, y=as.numeric(ytot[rownames(josm$surveys), spp])) x$FOR &lt;- x$Decid + x$Conif+ x$ConifWet # forest x$AHF &lt;- x$Agr + x$UrbInd + x$Roads # &#39;alienating&#39; human footprint x$WET &lt;- x$OpenWet + x$ConifWet + x$Water # wet + water cn &lt;- c(&quot;Open&quot;, &quot;Water&quot;, &quot;Agr&quot;, &quot;UrbInd&quot;, &quot;SoftLin&quot;, &quot;Roads&quot;, &quot;Decid&quot;, &quot;OpenWet&quot;, &quot;Conif&quot;, &quot;ConifWet&quot;) x$HAB &lt;- droplevels(find_max(x[,cn])$index) # drop empty levels x$DEC &lt;- ifelse(x$HAB == &quot;Decid&quot;, 1, 0) table(x$y) ## ## 0 1 2 3 4 5 6 ## 2493 883 656 363 132 29 13 3.3 Poisson null model The null model states that the expected values of the count at all locations are identical: \\(E[Y_i]=\\lambda\\) (\\(i=1,...,n\\)), where \\(Y_i\\) is a random variable that follows a Poisson distribution with mean \\(\\lambda\\): \\((Y_i \\mid \\lambda) \\sim Poisson(\\lambda)\\). The observation (\\(y_i\\)) is a realization of the random variables \\(Y\\) at site \\(i\\), these observations are independent and identically distributed (i.i.d.), and we have \\(n\\) observations in total. Saying the the distribution is Poisson is an assumption in itself. For example we assume that the variance equals the mean (\\(V(\\mu)=\\mu\\)). mP0 &lt;- glm(y ~ 1, data=x, family=poisson) mean(x$y) ## [1] 0.8831 mean(fitted(mP0)) ## [1] 0.8831 exp(coef(mP0)) ## (Intercept) ## 0.8831 summary(mP0) ## ## Call: ## glm(formula = y ~ 1, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.33 -1.33 -1.33 1.02 3.57 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.1243 0.0157 -7.89 2.9e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 7424.8 on 4568 degrees of freedom ## Residual deviance: 7424.8 on 4568 degrees of freedom ## AIC: 12573 ## ## Number of Fisher Scoring iterations: 6 The family=poisson specification implicitly assumes that we use a logarithmic link functions, that is to say that \\(log(\\lambda) = \\beta_0\\), or equivalently: \\(\\lambda = e^{\\beta_0}\\). The mean of the observations equal the mean of the fitted values, as expected. The logarithmic function is called the link function, its inverse, the exponential function is called the inverse link function. The model family has these convenently stored for us: mP0$family ## ## Family: poisson ## Link function: log mP0$family$linkfun ## function (mu) ## log(mu) ## &lt;environment: namespace:stats&gt; mP0$family$linkinv ## function (eta) ## pmax(exp(eta), .Machine$double.eps) ## &lt;environment: namespace:stats&gt; 3.4 Exploring covariates Now, in the absence of info about species biology, we are looking at a blank page. How should we proceed? What kind of covariate (linear predictor) should we use? We can do a quick and dirty exploration to see what are the likely candidates. We use a regression tree (ctree refers to conditional trees). It is a nonparametric method based on binary recursive partitioning in a conditional inference framework. This means that binary splits are made along the predictor variables, and the explanatory power of the split is assessed based on how it maximized difference between the splits and minimized the difference inside the splits. It is called conditional, because every new split is conditional on the previous splits (difference can be measured in many different ways, think e.g. sum of squares). The stopping rule in this implementation is based on permutation tests (see ?ctree or details and references). mCT &lt;- ctree(y ~ Open + Water + Agr + UrbInd + SoftLin + Roads + Decid + OpenWet + Conif + ConifWet, data=x) plot(mCT) The model can be seen as a piecewise constant regression, where each bucket (defined by the splits along the tree) yields a constant predictions based on the mean of the observations in the bucket. Any new data classified into the same bucket will get the same value. There is no notion of uncertainty (confidence or prediction intervals) in this nonparameric model. But we see something very useful: the proportion of deciduous forest in the landscape seems to be vary influential for Ovenbird abundance. 3.5 Single covariate With this new found knowledge, let’s fit a parametric (Poisson) linear model using Decid as a predictor: mP1 &lt;- glm(y ~ Decid, data=x, family=poisson) mean(x$y) ## [1] 0.8831 mean(fitted(mP0)) ## [1] 0.8831 coef(mP1) ## (Intercept) Decid ## -1.164 2.134 Same as before, the mean of the observations equal the mean of the fitted values. But instead of only the intercapt, now we have 2 coefficients estimated. Our linear predictor thus looks like: \\(log(\\lambda_i) = \\beta_0 + \\beta_1 x_{1i}\\). This means that expected abundance is \\(e^{\\beta_0}\\) where Decid=0, \\(e^{\\beta_0}e^{\\beta_1}\\) where Decid=1, and \\(e^{\\beta_0+\\beta_1 x_{1}}\\) in between. The relationship can be visualized by plotting the fitted values against the predictor, or using the coefficients to make predictions using our formula: dec &lt;- seq(0, 1, 0.01) lam &lt;- exp(coef(mP1)[1] + coef(mP1)[2] * dec) plot(fitted(mP1) ~ Decid, x, pch=19, col=&quot;grey&quot;) lines(lam ~ dec, col=2) rug(x$Decid) The model summary tells us that resudials are not quite right (we would expect 0 median and symmertic tails), in line with residual deviance being much higher than residual degrees of freedom (these should be close if the Poisson assumption holds). But, the Decid effect is significant (meaning that the effect size is large compared to the standard error): summary(mP1) ## ## Call: ## glm(formula = y ~ Decid, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.291 -0.977 -0.790 0.469 4.197 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.1643 0.0352 -33.1 &lt;2e-16 *** ## Decid 2.1338 0.0537 39.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 7424.8 on 4568 degrees of freedom ## Residual deviance: 5736.9 on 4567 degrees of freedom ## AIC: 10887 ## ## Number of Fisher Scoring iterations: 6 We can compare this model to the null (constant, intercept-only) model: AIC(mP0, mP1) BIC(mP0, mP1) model.sel(mP0, mP1) ## Model selection table ## (Intrc) Decid df logLik AICc delta weight ## mP1 -1.1640 2.134 2 -5442 10887 0 1 ## mP0 -0.1243 1 -6285 12573 1686 0 ## Models ranked by AICc(x) R2dev(mP0, mP1) ## R2 R2adj Deviance Dev0 DevR df0 dfR p_value ## mP0 0.00 0.00 0.00 7424.78 7424.78 4568.00 4568.00 &lt;2e-16 *** ## mP1 0.23 0.23 1687.87 7424.78 5736.91 4568.00 4567.00 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 AIC uses the negative log likelihood and the number of parameters as penalty. Smaller value indicate a model that is closer to the (unknowable) true model (caveat: this statement is true only asymptotically, i.e. it holds for very large sample sizes). For small samples, we of ten use BIC (more penalty for complex models when sample size is small), or AICc (as in MuMIn::model.sel). The other little table returned by R2dev shows deviance based (quasi) \\(R^2\\) and adjusted \\(R^2\\) for some GLM classes, just for the sake of completeness. The Chi-squared based test indicates good fit when the \\(p\\)-value is high (probability of being distributed according the Poisson). None of these two models is a particularly good fit in terms of the parametric distribution. This, however does not mean these models are not useful for making inferential statements about ovenbirds. How useful these statements are, that is another question. Let’s dive into cinfidence and prediction intervals a bit. B &lt;- 2000 alpha &lt;- 0.05 xnew &lt;- data.frame(Decid=seq(0, 1, 0.01)) CI0 &lt;- predict_sim(mP0, xnew, interval=&quot;confidence&quot;, level=1-alpha, B=B) PI0 &lt;- predict_sim(mP0, xnew, interval=&quot;prediction&quot;, level=1-alpha, B=B) CI1 &lt;- predict_sim(mP1, xnew, interval=&quot;confidence&quot;, level=1-alpha, B=B) PI1 &lt;- predict_sim(mP1, xnew, interval=&quot;prediction&quot;, level=1-alpha, B=B) ## nominal coverage is 95% sum(x$y %[]% predict_sim(mP0, interval=&quot;prediction&quot;, level=1-alpha, B=B)[,c(&quot;lwr&quot;, &quot;upr&quot;)]) / nrow(x) ## [1] 0.9619 sum(x$y %[]% predict_sim(mP1, interval=&quot;prediction&quot;, level=1-alpha, B=B)[,c(&quot;lwr&quot;, &quot;upr&quot;)]) / nrow(x) ## [1] 0.9711 A model is said to have good coverage when the prediction intervals encompass the right amount of the observations. When the nominal level is 95% (\\(100 \\times (1-\\alpha)\\), where \\(\\alpha\\) is Type I. error rate), we expect 95% of the observations fall within the 95% prediction interval. The prediction interval includes the uncertainty around the coefficients (confidence intervals, uncertainty in \\(\\hat{\\lambda}\\)) and the stochasticity coming from the Poisson distribution (\\(Y_i \\sim Poisson(\\hat{\\lambda})\\)). The code above calculate the confidence and prediction intervals for the two models. We also compared the prediction intervals and the nomial levels, and those were quite close (ours being a bit more conservative), hinting that maybe the Poisson distributional assumption is not very bad after all, but we’ll come back to this later. Let’s see our confidence and prediction intervals for the two models: yj &lt;- jitter(x$y, 0.5) plot(yj ~ Decid, x, xlab=&quot;Decid&quot;, ylab=&quot;E[Y]&quot;, ylim=c(0, max(PI1$upr)+1), pch=19, col=&quot;#bbbbbb33&quot;, main=&quot;P0&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(PI0$lwr, rev(PI0$upr)), border=NA, col=&quot;#0000ff44&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(CI0$lwr, rev(CI0$upr)), border=NA, col=&quot;#0000ff44&quot;) lines(CI0$fit ~ xnew$Decid, lty=1, col=4) polygon(c(xnew$Decid, rev(xnew$Decid)), c(PI1$lwr, rev(PI1$upr)), border=NA, col=&quot;#ff000044&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(CI1$lwr, rev(CI1$upr)), border=NA, col=&quot;#ff000044&quot;) lines(CI1$fit ~ xnew$Decid, lty=1, col=2) legend(&quot;topleft&quot;, bty=&quot;n&quot;, fill=c(&quot;#0000ff44&quot;, &quot;#ff000044&quot;), lty=1, col=c(4,2), border=NA, c(&quot;Null&quot;, &quot;Decid&quot;)) Exercise What can we conclude from this plot? Coverage is comparable, so what is the difference then? Which model should I use for prediction and why? (Hint: look at the non overlapping regions.) 3.6 Additive model Generalized additive models (GAMs) are semiparametric, meaning that parametric assumptions apply, but responses are modelled more flexibly. mGAM &lt;- mgcv::gam(y ~ s(Decid), x, family=poisson) summary(mGAM) ## ## Family: poisson ## Link function: log ## ## Formula: ## y ~ s(Decid) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.5606 0.0283 -19.8 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(Decid) 8.56 8.94 1193 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.239 Deviance explained = 29% ## UBRE = 0.15808 Scale est. = 1 n = 4569 plot(mGAM) fitCT &lt;- predict(mCT, x[order(x$Decid),]) fitGAM &lt;- predict(mGAM, xnew, type=&quot;response&quot;) plot(yj ~ Decid, x, xlab=&quot;Decid&quot;, ylab=&quot;E[Y]&quot;, ylim=c(0, max(PI1$upr)+1), pch=19, col=&quot;#bbbbbb33&quot;, main=&quot;P0&quot;) lines(CI0$fit ~ xnew$Decid, lty=1, col=1) lines(CI1$fit ~ xnew$Decid, lty=1, col=2) lines(fitCT ~ x$Decid[order(x$Decid)], lty=1, col=3) lines(fitGAM ~ xnew$Decid, lty=1, col=4) legend(&quot;topleft&quot;, bty=&quot;n&quot;, lty=1, col=1:4, legend=c(&quot;Null&quot;, &quot;Decid&quot;, &quot;ctree&quot;, &quot;GAM&quot;)) Exercise Play with GAM and other variables to understand response curves: plot(mgcv::gam(y ~ s(&lt;variable_name&gt;), data=x, family=poisson)) 3.7 Nonlinear terms We can use polynomial terms to approximate the GAM fit: mP12 &lt;- glm(y ~ Decid + I(Decid^2), data=x, family=poisson) mP13 &lt;- glm(y ~ Decid + I(Decid^2) + I(Decid^3), data=x, family=poisson) mP14 &lt;- glm(y ~ Decid + I(Decid^2) + I(Decid^3) + I(Decid^4), data=x, family=poisson) model.sel(mP1, mP12, mP13, mP14, mGAM) ## Model selection table ## (Int) Dcd Dcd^2 Dcd^3 Dcd^4 s(Dcd) class df logLik AICc ## mGAM -0.5606 + gam 9 -5209 10438 ## mP14 -2.6640 16.640 -38.60 41.470 -16.31 glm 5 -5215 10441 ## mP13 -2.3910 11.400 -16.31 8.066 glm 4 -5226 10461 ## mP12 -1.9240 6.259 -3.97 glm 3 -5269 10544 ## mP1 -1.1640 2.134 glm 2 -5442 10887 ## delta weight ## mGAM 0.00 0.84 ## mP14 3.31 0.16 ## mP13 23.42 0.00 ## mP12 106.04 0.00 ## mP1 449.60 0.00 ## Models ranked by AICc(x) Not a surprise that the most complex model won. GAM was more complex than that. pr &lt;- cbind( predict(mP1, xnew, type=&quot;response&quot;), predict(mP12, xnew, type=&quot;response&quot;), predict(mP13, xnew, type=&quot;response&quot;), predict(mP14, xnew, type=&quot;response&quot;), fitGAM) matplot(xnew$Decid, pr, lty=1, type=&quot;l&quot;, xlab=&quot;Decid&quot;, ylab=&quot;E[Y]&quot;) legend(&quot;topleft&quot;, lty=1, col=1:5, bty=&quot;n&quot;, legend=c(&quot;Linear&quot;, &quot;Quadratic&quot;, &quot;Cubic&quot;, &quot;Quartic&quot;, &quot;GAM&quot;)) Let’s see how these affect our prediction intervals: CI12 &lt;- predict_sim(mP12, xnew, interval=&quot;confidence&quot;, level=1-alpha, B=B) PI12 &lt;- predict_sim(mP12, xnew, interval=&quot;prediction&quot;, level=1-alpha, B=B) CI13 &lt;- predict_sim(mP13, xnew, interval=&quot;confidence&quot;, level=1-alpha, B=B) PI13 &lt;- predict_sim(mP13, xnew, interval=&quot;prediction&quot;, level=1-alpha, B=B) CI14 &lt;- predict_sim(mP14, xnew, interval=&quot;confidence&quot;, level=1-alpha, B=B) PI14 &lt;- predict_sim(mP14, xnew, interval=&quot;prediction&quot;, level=1-alpha, B=B) op &lt;- par(mfrow=c(2,2)) plot(yj ~ Decid, x, xlab=&quot;Decid&quot;, ylab=&quot;E[Y]&quot;, ylim=c(0, max(PI1$upr)+1), pch=19, col=&quot;#bbbbbb33&quot;, main=&quot;Linear&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(PI1$lwr, rev(PI1$upr)), border=NA, col=&quot;#0000ff44&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(CI1$lwr, rev(CI1$upr)), border=NA, col=&quot;#0000ff88&quot;) lines(CI1$fit ~ xnew$Decid, lty=1, col=4) lines(fitGAM ~ xnew$Decid, lty=2, col=1) plot(yj ~ Decid, x, xlab=&quot;Decid&quot;, ylab=&quot;E[Y]&quot;, ylim=c(0, max(PI1$upr)+1), pch=19, col=&quot;#bbbbbb33&quot;, main=&quot;Quadratic&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(PI12$lwr, rev(PI12$upr)), border=NA, col=&quot;#0000ff44&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(CI12$lwr, rev(CI12$upr)), border=NA, col=&quot;#0000ff88&quot;) lines(CI12$fit ~ xnew$Decid, lty=1, col=4) lines(fitGAM ~ xnew$Decid, lty=2, col=1) plot(yj ~ Decid, x, xlab=&quot;Decid&quot;, ylab=&quot;E[Y]&quot;, ylim=c(0, max(PI1$upr)+1), pch=19, col=&quot;#bbbbbb33&quot;, main=&quot;P0&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(PI13$lwr, rev(PI13$upr)), border=NA, col=&quot;#0000ff44&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(CI13$lwr, rev(CI13$upr)), border=NA, col=&quot;#0000ff88&quot;) lines(CI13$fit ~ xnew$Decid, lty=1, col=4) lines(fitGAM ~ xnew$Decid, lty=2, col=1) plot(yj ~ Decid, x, xlab=&quot;Decid&quot;, ylab=&quot;E[Y]&quot;, ylim=c(0, max(PI1$upr)+1), pch=19, col=&quot;#bbbbbb33&quot;, main=&quot;P0&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(PI14$lwr, rev(PI14$upr)), border=NA, col=&quot;#0000ff44&quot;) polygon(c(xnew$Decid, rev(xnew$Decid)), c(CI14$lwr, rev(CI14$upr)), border=NA, col=&quot;#0000ff88&quot;) lines(CI14$fit ~ xnew$Decid, lty=1, col=4) lines(fitGAM ~ xnew$Decid, lty=2, col=1) par(op) 3.8 Categorical variables Categorical variables are expanded into a model matrix before estimation. The model matrix usually contains indicator variables for each level (value 1 when factor value equals a particular label, 0 otherwise) except for the reference category (check relevel if you want to change the reference category). The estimate for the reference category comes from the intercept, the rest of the estimates are relative to the reference category. In the log-linear model example this means a ratio. head(model.matrix(~DEC, x)) ## (Intercept) DEC ## CL10102 1 1 ## CL10106 1 0 ## CL10108 1 0 ## CL10109 1 1 ## CL10111 1 1 ## CL10112 1 1 mP2 &lt;- glm(y ~ DEC, data=x, family=poisson) summary(mP2) ## ## Call: ## glm(formula = y ~ DEC, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.691 -0.921 -0.921 0.449 4.543 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8577 0.0308 -27.8 &lt;2e-16 *** ## DEC 1.2156 0.0358 33.9 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 7424.8 on 4568 degrees of freedom ## Residual deviance: 6095.5 on 4567 degrees of freedom ## AIC: 11246 ## ## Number of Fisher Scoring iterations: 6 coef(mP2) ## (Intercept) DEC ## -0.8577 1.2156 The estimate for a non-deciduous landscape is \\(e^{\\beta_0}\\), and it is \\(e^{\\beta_0}e^{\\beta_1}\\) for deciduous landscapes. Of course such binary classification at the landscape (1 km\\(^2\\)) level doesn’t really makes sense for various reasons: boxplot(Decid ~ DEC, x) model.sel(mP1, mP2) ## Model selection table ## (Intrc) Decid DEC df logLik AICc delta weight ## mP1 -1.1640 2.134 2 -5442 10887 0.0 1 ## mP2 -0.8577 1.216 2 -5621 11246 358.6 0 ## Models ranked by AICc(x) R2dev(mP1, mP2) ## R2 R2adj Deviance Dev0 DevR df0 dfR p_value ## mP1 0.23 0.23 1687.87 7424.78 5736.91 4568.00 4567.00 &lt;2e-16 *** ## mP2 0.18 0.18 1329.23 7424.78 6095.55 4568.00 4567.00 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Having estimates for each land cover type improves the model, but the model using continuous variable is still better mP3 &lt;- glm(y ~ HAB, data=x, family=poisson) summary(mP3) ## ## Call: ## glm(formula = y ~ HAB, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.691 -0.873 -0.817 0.449 4.832 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.386 0.577 -2.40 0.0163 * ## HABWater 1.030 0.690 1.49 0.1357 ## HABAgr 0.693 0.913 0.76 0.4477 ## HABUrbInd 0.134 0.764 0.17 0.8612 ## HABRoads -10.916 201.285 -0.05 0.9567 ## HABDecid 1.744 0.578 3.02 0.0025 ** ## HABOpenWet 0.422 0.591 0.71 0.4755 ## HABConif 0.913 0.579 1.58 0.1150 ## HABConifWet 0.288 0.579 0.50 0.6185 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 7424.8 on 4568 degrees of freedom ## Residual deviance: 5997.2 on 4560 degrees of freedom ## AIC: 11161 ## ## Number of Fisher Scoring iterations: 10 model.sel(mP1, mP2, mP3) ## Model selection table ## (Intrc) Decid DEC HAB df logLik AICc delta weight ## mP1 -1.1640 2.134 2 -5442 10887 0.0 1 ## mP3 -1.3860 + 9 -5572 11162 274.4 0 ## mP2 -0.8577 1.216 2 -5621 11246 358.6 0 ## Models ranked by AICc(x) R2dev(mP1, mP2, mP3) ## R2 R2adj Deviance Dev0 DevR df0 dfR p_value ## mP1 0.23 0.23 1687.87 7424.78 5736.91 4568.00 4567.00 &lt;2e-16 *** ## mP2 0.18 0.18 1329.23 7424.78 6095.55 4568.00 4567.00 &lt;2e-16 *** ## mP3 0.19 0.19 1427.55 7424.78 5997.23 4568.00 4560.00 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The prediction in this case would look like: \\(log(\\lambda_i)=\\beta_0 + \\sum_{j=1}^{k-1} \\beta_j x_{ji}\\), where we have \\(k\\) factor levels (and \\(k-1\\) indicator variables besides the intercept). Here is a general way of calculating fitted values or making predictions based on the design matrix (X) and the coefficients (b) (column ordering in X must match the elements in b) given a parametric log-linear model object and data frame df: b &lt;- coef(object) X &lt;- model.matrix(formula(object), df) exp(X %*% b) 3.9 Multiple main effects We can keep adding variables to the model in a forwards-selection fashion. add1 adds variables one at a time, selecting from the scope defined by the formula: scope &lt;- as.formula(paste(&quot;~ FOR + WET + AHF +&quot;,paste(cn, collapse=&quot;+&quot;))) tmp &lt;- add1(mP1, scope) tmp$AIC_drop &lt;- tmp$AIC-tmp$AIC[1] # current model tmp[order(tmp$AIC),] ## Single term additions ## ## Model: ## y ~ Decid ## Df Deviance AIC AIC_drop ## ConifWet 1 5638 10791 -96.5 ## Conif 1 5685 10838 -49.4 ## WET 1 5687 10839 -48.1 ## Water 1 5721 10873 -13.7 ## FOR 1 5724 10876 -11.0 ## OpenWet 1 5728 10880 -6.9 ## Open 1 5730 10882 -4.7 ## Roads 1 5733 10885 -1.9 ## AHF 1 5734 10886 -0.7 ## &lt;none&gt; 5737 10887 0.0 ## Agr 1 5736 10888 1.2 ## UrbInd 1 5736 10889 1.5 ## SoftLin 1 5737 10889 1.6 It looks like ConifWet is the best covariate to add next because it leads to the biggest drop in AIC, and both effects are significant. mP4 &lt;- glm(y ~ Decid + ConifWet, data=x, family=poisson) summary(mP4) ## ## Call: ## glm(formula = y ~ Decid + ConifWet, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.237 -0.996 -0.679 0.447 4.439 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7014 0.0556 -12.61 &lt;2e-16 *** ## Decid 1.6224 0.0719 22.57 &lt;2e-16 *** ## ConifWet -0.9785 0.0993 -9.86 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 7424.8 on 4568 degrees of freedom ## Residual deviance: 5638.4 on 4566 degrees of freedom ## AIC: 10791 ## ## Number of Fisher Scoring iterations: 6 drop1 is the function opposite of add1, it assesses which term should be dropped from a more saturated model: formula_all &lt;- y ~ Open + Agr + UrbInd + SoftLin + Roads + Decid + OpenWet + Conif + ConifWet + OvernightRain + TSSR + DAY + Longitude + Latitude tmp &lt;- drop1(glm(formula_all, data=x, family=poisson)) tmp$AIC_drop &lt;- tmp$AIC-tmp$AIC[1] # current model tmp[order(tmp$AIC),] ## Single term deletions ## ## Model: ## y ~ Open + Agr + UrbInd + SoftLin + Roads + Decid + OpenWet + ## Conif + ConifWet + OvernightRain + TSSR + DAY + Longitude + ## Latitude ## Df Deviance AIC AIC_drop ## OvernightRain 1 5500 10674 -2.0 ## Roads 1 5500 10674 -1.9 ## SoftLin 1 5500 10675 -1.6 ## Agr 1 5501 10675 -1.4 ## &lt;none&gt; 5500 10676 0.0 ## Decid 1 5505 10679 3.0 ## OpenWet 1 5505 10679 3.1 ## Conif 1 5508 10682 6.0 ## UrbInd 1 5511 10685 8.7 ## Longitude 1 5519 10693 16.5 ## TSSR 1 5524 10698 21.8 ## ConifWet 1 5528 10703 26.4 ## DAY 1 5529 10703 26.7 ## Open 1 5531 10705 28.7 ## Latitude 1 5580 10754 78.2 The step function can be used to automatically select the best model based on adding/dropping terms: mPstep &lt;- step(glm(formula_all, data=x, family=poisson), trace=0) # use trace=1 to see all the steps summary(mPstep) ## ## Call: ## glm(formula = y ~ Open + UrbInd + Decid + OpenWet + Conif + ConifWet + ## TSSR + DAY + Longitude + Latitude, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.763 -0.986 -0.674 0.451 4.624 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.88293 1.30223 -4.52 6.3e-06 *** ## Open -3.47428 0.65867 -5.27 1.3e-07 *** ## UrbInd -1.66883 0.54216 -3.08 0.00208 ** ## Decid 0.83372 0.25957 3.21 0.00132 ** ## OpenWet -0.74076 0.30238 -2.45 0.01430 * ## Conif -0.88558 0.26566 -3.33 0.00086 *** ## ConifWet -1.89423 0.27170 -6.97 3.1e-12 *** ## TSSR -1.23416 0.24984 -4.94 7.8e-07 *** ## DAY -2.87970 0.52686 -5.47 4.6e-08 *** ## Longitude 0.03831 0.00877 4.37 1.2e-05 *** ## Latitude 0.20930 0.02309 9.06 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 7424.8 on 4568 degrees of freedom ## Residual deviance: 5501.1 on 4558 degrees of freedom ## AIC: 10669 ## ## Number of Fisher Scoring iterations: 6 3.10 Interaction When we consider interactions between two variables (say \\(x_1\\) and \\(x_2\\)), we really referring to adding another variable to the model matrix that is a product of the two variables (\\(x_{12}=x_1 x_2\\)): head(model.matrix(~x1 * x2, data.frame(x1=1:4, x2=10:7))) ## (Intercept) x1 x2 x1:x2 ## 1 1 1 10 10 ## 2 1 2 9 18 ## 3 1 3 8 24 ## 4 1 4 7 28 Let’s consider interaction between our two predictors from before: mP5 &lt;- glm(y ~ Decid * ConifWet, data=x, family=poisson) summary(mP5) ## ## Call: ## glm(formula = y ~ Decid * ConifWet, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.081 -1.022 -0.484 0.374 4.321 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.5604 0.0566 -9.9 &lt;2e-16 *** ## Decid 1.2125 0.0782 15.5 &lt;2e-16 *** ## ConifWet -2.3124 0.1490 -15.5 &lt;2e-16 *** ## Decid:ConifWet 5.3461 0.3566 15.0 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 7424.8 on 4568 degrees of freedom ## Residual deviance: 5395.2 on 4565 degrees of freedom ## AIC: 10549 ## ## Number of Fisher Scoring iterations: 6 model.sel(mP0, mP1, mP4, mP5) ## Model selection table ## (Int) Dcd CnW CnW:Dcd df logLik AICc delta weight ## mP5 -0.5604 1.213 -2.3120 5.346 4 -5271 10549 0.0 1 ## mP4 -0.7014 1.622 -0.9785 3 -5392 10791 241.2 0 ## mP1 -1.1640 2.134 2 -5442 10887 337.7 0 ## mP0 -0.1243 1 -6285 12573 2023.6 0 ## Models ranked by AICc(x) The model with the interaction is best supported, but how do we make sense of this relationship? We can’t easily visualize it in a single plot. We can either fix all variables (at their mean/meadian) and see how the response is changing along a single variable: this is called a conditional effect (conditional on fixing other variables), this is what visreg::visreg does; or plot the fitted values against the predictor variables, this is called a marginal effects, and this is what ResourceSelection::mep does. visreg(mP5, scale=&quot;response&quot;, xvar=&quot;ConifWet&quot;, by=&quot;Decid&quot;) mep(mP5) Let’s use GAM to fit a bivariate spline: mGAM2 &lt;- mgcv::gam(y ~ s(Decid, ConifWet), data=x, family=poisson) plot(mGAM2, scheme=2, rug=FALSE) Final battle of Poisson models: model.sel(mP0, mP1, mP12, mP13, mP14, mP2, mP3, mP4, mP5, mGAM, mGAM2) ## Model selection table ## (Int) Dcd Dcd^2 Dcd^3 Dcd^4 DEC HAB CnW CnW:Dcd s(Dcd) ## mGAM2 -0.6251 ## mGAM -0.5606 + ## mP14 -2.6640 16.640 -38.60 41.470 -16.31 ## mP13 -2.3910 11.400 -16.31 8.066 ## mP12 -1.9240 6.259 -3.97 ## mP5 -0.5604 1.213 -2.3120 5.346 ## mP4 -0.7014 1.622 -0.9785 ## mP1 -1.1640 2.134 ## mP3 -1.3860 + ## mP2 -0.8577 1.216 ## mP0 -0.1243 ## s(Dcd,CnW) class df logLik AICc delta weight ## mGAM2 + gam 27 -5160 10376 0.00 1 ## mGAM gam 9 -5209 10438 61.11 0 ## mP14 glm 5 -5215 10441 64.42 0 ## mP13 glm 4 -5226 10461 84.53 0 ## mP12 glm 3 -5269 10544 167.14 0 ## mP5 glm 4 -5271 10549 172.99 0 ## mP4 glm 3 -5392 10791 414.18 0 ## mP1 glm 2 -5442 10887 510.71 0 ## mP3 glm 9 -5572 11162 785.06 0 ## mP2 glm 2 -5621 11246 869.35 0 ## mP0 glm 1 -6285 12573 2196.58 0 ## Models ranked by AICc(x) R2dev(mP0, mP1, mP12, mP13, mP14, mP2, mP3, mP4, mP5, mGAM, mGAM2) ## R2 R2adj Deviance Dev0 DevR df0 dfR p_value ## mP0 0.00 0.00 0.00 7424.78 7424.78 4568.00 4568.00 &lt; 2e-16 *** ## mP1 0.23 0.23 1687.87 7424.78 5736.91 4568.00 4567.00 &lt; 2e-16 *** ## mP12 0.27 0.27 2033.44 7424.78 5391.34 4568.00 4566.00 &lt; 2e-16 *** ## mP13 0.29 0.28 2118.06 7424.78 5306.72 4568.00 4565.00 7.6e-14 *** ## mP14 0.29 0.29 2140.17 7424.78 5284.61 4568.00 4564.00 3.4e-13 *** ## mP2 0.18 0.18 1329.23 7424.78 6095.55 4568.00 4567.00 &lt; 2e-16 *** ## mP3 0.19 0.19 1427.55 7424.78 5997.23 4568.00 4560.00 &lt; 2e-16 *** ## mP4 0.24 0.24 1786.40 7424.78 5638.38 4568.00 4566.00 &lt; 2e-16 *** ## mP5 0.27 0.27 2029.60 7424.78 5395.18 4568.00 4565.00 &lt; 2e-16 *** ## mGAM 0.29 0.29 2152.63 7424.78 5272.15 4568.00 4559.00 5.5e-13 *** ## mGAM2 0.30 0.30 2250.35 7424.78 5174.43 4568.00 4539.00 8.4e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Of course, the most complex model wins but the Chi-square test is still significant (indicating lack of fit). Let’s try different error distribution. 3.11 Different error distributions We will use the 2-variable model with interaction: mP &lt;- glm(y ~ Decid * ConifWet, data=x, family=poisson) Let us try the Negative Binomial distribution first. This distribution is related to Binomial experiments (number of trials required to get a fixed number of successes given a binomial probability). It can also be derived as a mixture of Poisson and Gamma distributions (see Wikipedia), which is a kind of hierarchical model. In this case, the Gamma distribution acts as an i.i.d. random effect for the intercept: \\(Y_i\\sim Poisson(\\lambda_i)\\), \\(\\lambda_i \\sim Gamma(e^{\\beta_0+\\beta_1 x_{1i}}, \\gamma)\\), where \\(\\gamma\\) is the Gamma variance. The Negative Binomial variance (using the parametrization common in R functions) is a function of the mean and the scale: \\(V(\\mu) = \\mu + \\mu^2/\\theta\\). mNB &lt;- glm.nb(y ~ Decid * ConifWet, data=x) summary(mNB) ## ## Call: ## glm.nb(formula = y ~ Decid * ConifWet, data = x, init.theta = 3.5900635, ## link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.860 -0.985 -0.451 0.317 3.803 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.5905 0.0630 -9.38 &lt;2e-16 *** ## Decid 1.2459 0.0892 13.97 &lt;2e-16 *** ## ConifWet -2.3545 0.1605 -14.67 &lt;2e-16 *** ## Decid:ConifWet 5.6945 0.4009 14.20 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(3.59) family taken to be 1) ## ## Null deviance: 6089.3 on 4568 degrees of freedom ## Residual deviance: 4387.7 on 4565 degrees of freedom ## AIC: 10440 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 3.590 ## Std. Err.: 0.425 ## ## 2 x log-likelihood: -10430.448 Next, we look at zero-inflated models. In this case, the mixture distribution is a Bernoulli distribution and a count distribution (Poisson or Negative Binomial, for example). The 0’s can come from both the zero and the count distributions, whereas the &gt;0 values can only come from the count distribution: \\(A_i \\sim Bernoulli(\\varphi)\\), \\(Y_i \\sim Poisson(A_i \\lambda_i)\\). The zero part of the zero-inflated models are often parametrized as probability of zero (\\(1-\\varphi\\)), as in the pscl::zeroinfl function: ## Zero-inflated Poisson mZIP &lt;- zeroinfl(y ~ Decid * ConifWet | 1, x, dist=&quot;poisson&quot;) summary(mZIP) ## ## Call: ## zeroinfl(formula = y ~ Decid * ConifWet | 1, data = x, dist = &quot;poisson&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.218 -0.700 -0.339 0.378 8.951 ## ## Count model coefficients (poisson with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.3241 0.0651 -4.98 6.5e-07 *** ## Decid 1.0700 0.0860 12.44 &lt; 2e-16 *** ## ConifWet -2.4407 0.1564 -15.60 &lt; 2e-16 *** ## Decid:ConifWet 5.9373 0.3840 15.46 &lt; 2e-16 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.613 0.103 -15.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Number of iterations in BFGS optimization: 12 ## Log-likelihood: -5.21e+03 on 5 Df ## Zero-inflated Negative Binomial mZINB &lt;- zeroinfl(y ~ Decid * ConifWet | 1, x, dist=&quot;negbin&quot;) summary(mZINB) ## ## Call: ## zeroinfl(formula = y ~ Decid * ConifWet | 1, data = x, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.190 -0.689 -0.338 0.361 8.956 ## ## Count model coefficients (negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.3873 0.0732 -5.29 1.2e-07 *** ## Decid 1.1195 0.0911 12.29 &lt; 2e-16 *** ## ConifWet -2.4230 0.1589 -15.25 &lt; 2e-16 *** ## Decid:ConifWet 5.9227 0.3963 14.94 &lt; 2e-16 *** ## Log(theta) 2.6504 0.5305 5.00 5.9e-07 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.861 0.193 -9.64 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta = 14.159 ## Number of iterations in BFGS optimization: 22 ## Log-likelihood: -5.2e+03 on 6 Df Now we compare the four different parametric models: AIC(mP, mNB, mZIP, mZINB) Our best model is the Zero-inflated Negative Binomial. The probability of observing a zero as part of the zero distribution is back transformed from the zero coefficient using the inverse logit function: unname(plogis(coef(mZINB, &quot;zero&quot;))) # P of 0 ## [1] 0.1346 Now we use the scale parameter to visualize the variance functions for the Negative Binomial models (the 1:1 line is the Poisson model): mNB$theta ## [1] 3.59 mZINB$theta ## [1] 14.16 mu &lt;- seq(0, 5, 0.01) plot(mu, mu + mu^2/mNB$theta, type=&quot;l&quot;, col=2, ylab=expression(V(mu)), xlab=expression(mu)) lines(mu, mu + mu^2/mZINB$theta, type=&quot;l&quot;, col=4) abline(0,1, lty=2) legend(&quot;topleft&quot;, bty=&quot;n&quot;, lty=1, col=c(2,4), legend=paste(c(&quot;NB&quot;, &quot;ZINB&quot;), round(c(mNB$theta, mZINB$theta), 2))) Exercise How can we interpret these different kinds of overdispersion (zero-inflation and higher than Poisson variance)? What are some of the biological mechanisms that can contribute to the overdispersion? It is also common practice to consider generalized linear mixed models (GLMMs) for count data. These mixed models are usually considered as Poisson-Lognormal mixtures. The simplest, so called i.i.d., case is similar to the Negative Binomial, but instead of Gamma, we have Lognormal distribution: \\(Y_i\\sim Poisson(\\lambda_i)\\), \\(log(\\lambda_i) = \\beta_0+\\beta_1 x_{1i}+\\epsilon_i\\), \\(\\epsilon_i \\sim Normal(0, \\sigma^2)\\), where \\(\\sigma^2\\) is the Lognormal variance on the log scale. We can use the lme4::glmer function: use SiteID as random effect (we have exactly \\(n\\) random effects). mPLN1 &lt;- glmer(y ~ Decid * ConifWet + (1 | SiteID), data=x, family=poisson) summary(mPLN1) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: y ~ Decid * ConifWet + (1 | SiteID) ## Data: x ## ## AIC BIC logLik deviance df.resid ## 10423 10455 -5206 10413 4564 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.150 -0.629 -0.288 0.418 5.469 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SiteID (Intercept) 0.294 0.542 ## Number of obs: 4569, groups: SiteID, 4569 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7518 0.0675 -11.1 &lt;2e-16 *** ## Decid 1.2847 0.0920 14.0 &lt;2e-16 *** ## ConifWet -2.3380 0.1625 -14.4 &lt;2e-16 *** ## Decid:ConifWet 5.6326 0.4118 13.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Decid ConfWt ## Decid -0.895 ## ConifWet -0.622 0.644 ## Decid:CnfWt 0.176 -0.379 -0.700 Note The number of unknowns we have to somehow estimate is now more than the number of observations we have. How is that possible? Alternatively, we can use SurveyArea as a grouping variable. We have now \\(m &lt; n\\) random effects, and survey areas can be seen as larger landscapes within which the sites are clustered: \\(Y_ij\\sim Poisson(\\lambda_ij)\\), \\(log(\\lambda_ij) = \\beta_0+\\beta_1 x_{1ij}+\\epsilon_i\\), \\(\\epsilon_i \\sim Normal(0, \\sigma^2)\\). The index \\(i\\) (\\(i=1,...,m\\)) defines the cluster (survey area), the \\(j\\) (\\(j=1,...,n_i\\)) defines the sites within survey area \\(i\\) (\\(n = \\sum_{i=1}^m n_i\\)). mPLN2 &lt;- glmer(y ~ Decid * ConifWet + (1 | SurveyArea), data=x, family=poisson) summary(mPLN2) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: y ~ Decid * ConifWet + (1 | SurveyArea) ## Data: x ## ## AIC BIC logLik deviance df.resid ## 10021 10053 -5006 10011 4564 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.739 -0.643 -0.320 0.355 6.535 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SurveyArea (Intercept) 0.295 0.543 ## Number of obs: 4569, groups: SurveyArea, 271 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7459 0.0783 -9.53 &lt;2e-16 *** ## Decid 1.1967 0.0984 12.16 &lt;2e-16 *** ## ConifWet -2.3213 0.1686 -13.77 &lt;2e-16 *** ## Decid:ConifWet 5.5346 0.3977 13.92 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Decid ConfWt ## Decid -0.808 ## ConifWet -0.610 0.628 ## Decid:CnfWt 0.162 -0.325 -0.670 In the battle of distributions (keeping the linear predictor part the same) the clustered GLMM was best supported: tmp &lt;- AIC(mP, mNB, mZIP, mZINB, mPLN1, mPLN2) tmp$delta_AIC &lt;- tmp$AIC - min(tmp$AIC) tmp[order(tmp$AIC),] Exercise What are some of the biological mechanisms that can lead to the clustered GLMM bi be the best model? 3.12 Count duration effects Let’s change gears a bit now, and steer closer to the main focus of this book. We want to account for methodological differences among samples. One aspect of mathodologies involve variation in total counting duration. We’ll now inspect what that does to our observations. First, we create a list of matrices where counts are tabulated by surveys and time intervals for each species: ydur &lt;- Xtab(~ SiteID + Dur + SpeciesID , josm$counts[josm$counts$DetectType1 != &quot;V&quot;,]) We use the same species (spp) as before and create a data frame indluring the cumulative counts during 3, 5, and 10 minutes: y &lt;- as.matrix(ydur[[spp]]) head(y) ## 0-3min 3-5min 5-10min ## CL10102 3 0 0 ## CL10106 0 0 0 ## CL10108 0 0 0 ## CL10109 2 0 1 ## CL10111 2 0 0 ## CL10112 2 0 0 colMeans(y) # mean count of new individuals ## 0-3min 3-5min 5-10min ## 0.67367 0.09346 0.11600 cumsum(colMeans(y)) # cumulative counts ## 0-3min 3-5min 5-10min ## 0.6737 0.7671 0.8831 x &lt;- data.frame( josm$surveys, y3=y[,&quot;0-3min&quot;], y5=y[,&quot;0-3min&quot;]+y[,&quot;3-5min&quot;], y10=rowSums(y)) table(x$y3) ## ## 0 1 2 3 4 5 6 ## 2768 922 576 226 61 14 2 table(x$y5) ## ## 0 1 2 3 4 5 6 ## 2643 894 632 285 87 24 4 table(x$y10) ## ## 0 1 2 3 4 5 6 ## 2493 883 656 363 132 29 13 If we fit single-predictor GLMs to these 3 responses, we get different fitted values, consistent with our mean counts: m3 &lt;- glm(y3 ~ Decid, data=x, family=poisson) m5 &lt;- glm(y5 ~ Decid, data=x, family=poisson) m10 &lt;- glm(y10 ~ Decid, data=x, family=poisson) mean(fitted(m3)) ## [1] 0.6737 mean(fitted(m5)) ## [1] 0.7671 mean(fitted(m10)) ## [1] 0.8831 Using the multiple time interval data, we can pretend that we have a mix of methodologies with respect to count duration: set.seed(1) x$meth &lt;- as.factor(sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), nrow(x), replace=TRUE)) x$y &lt;- x$y3 x$y[x$meth == &quot;B&quot;] &lt;- x$y5[x$meth == &quot;B&quot;] x$y[x$meth == &quot;C&quot;] &lt;- x$y10[x$meth == &quot;C&quot;] boxplot(y ~ meth, x) sb &lt;- sum_by(x$y, x$meth) points(1:3, sb[,1]/sb[,2], col=2, type=&quot;b&quot;, pch=4) We can estimate the effect of the methodology: mm &lt;- glm(y ~ meth - 1, data=x, family=poisson) summary(mm) ## ## Call: ## glm(formula = y ~ meth - 1, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.309 -1.263 -1.162 0.369 3.616 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## methA -0.3929 0.0309 -12.70 &lt; 2e-16 *** ## methB -0.2255 0.0289 -7.79 6.6e-15 *** ## methC -0.1550 0.0277 -5.60 2.1e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 7225.2 on 4569 degrees of freedom ## Residual deviance: 6941.8 on 4566 degrees of freedom ## AIC: 11657 ## ## Number of Fisher Scoring iterations: 6 exp(coef(mm)) ## methA methB methC ## 0.6751 0.7981 0.8564 Or the effect of the continuous predictor and the method (discrete): mm &lt;- glm(y ~ Decid + meth, data=x, family=poisson) summary(mm) ## ## Call: ## glm(formula = y ~ Decid + meth, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.278 -0.939 -0.736 0.457 4.201 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.4416 0.0457 -31.56 &lt; 2e-16 *** ## Decid 2.1490 0.0574 37.43 &lt; 2e-16 *** ## methB 0.1347 0.0424 3.18 0.0015 ** ## methC 0.2705 0.0415 6.51 7.3e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 6976.3 on 4568 degrees of freedom ## Residual deviance: 5442.7 on 4565 degrees of freedom ## AIC: 10159 ## ## Number of Fisher Scoring iterations: 6 boxplot(fitted(mm) ~ meth, x) exp(coef(mm)) ## (Intercept) Decid methB methC ## 0.2365 8.5766 1.1442 1.3106 The fixed effects adjusts the means well: cumsum(colMeans(y)) ## 0-3min 3-5min 5-10min ## 0.6737 0.7671 0.8831 mean(y[,1]) * c(1, exp(coef(mm))[3:4]) ## methB methC ## 0.6737 0.7708 0.8829 But it is all relative, depends on reference methodology/protocol. The problem is, we can’t easily extrapolate to a methodology with count duration of 12 minutes, or interpolate to a mathodology with count duration of 2 or 8 minutes. We need somehow to express time expediture in minutes to make that work. Let’s try something else: x$tmax &lt;- c(3, 5, 10)[as.integer(x$meth)] mm &lt;- glm(y ~ Decid + I(log(tmax)), data=x, family=poisson) summary(mm) ## ## Call: ## glm(formula = y ~ Decid + I(log(tmax)), family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.284 -0.939 -0.731 0.453 4.195 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.6777 0.0702 -23.91 &lt; 2e-16 *** ## Decid 2.1504 0.0574 37.48 &lt; 2e-16 *** ## I(log(tmax)) 0.2218 0.0340 6.53 6.7e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 6976.3 on 4568 degrees of freedom ## Residual deviance: 5443.0 on 4566 degrees of freedom ## AIC: 10158 ## ## Number of Fisher Scoring iterations: 6 tmax &lt;- seq(0, 20, 0.01) plot(tmax, exp(log(tmax) * coef(mm)[3]), type=&quot;l&quot;, ylab=&quot;Method effect&quot;, col=2) Now we are getting somewhere. But still, this function keep increasing monotonically. Exercise What kind of function would we need and why? What is the underlying biological mechanism? 3.13 Count radius effects Before solving the count duration issue, let us look at the effect of survey area. We get a similar count breakdown, but now by distance band: ydis &lt;- Xtab(~ SiteID + Dis + SpeciesID , josm$counts[josm$counts$DetectType1 != &quot;V&quot;,]) y &lt;- as.matrix(ydis[[spp]]) head(y) ## 0-50m 50-100m 100+m ## CL10102 1 2 0 ## CL10106 0 0 0 ## CL10108 0 0 0 ## CL10109 1 2 0 ## CL10111 1 0 1 ## CL10112 0 2 0 colMeans(y) # mean count of new individuals ## 0-50m 50-100m 100+m ## 0.29241 0.49223 0.09849 cumsum(colMeans(y)) # cumulative counts ## 0-50m 50-100m 100+m ## 0.2924 0.7846 0.8831 x &lt;- data.frame( josm$surveys, y50=y[,&quot;0-50m&quot;], y100=y[,&quot;0-50m&quot;]+y[,&quot;50-100m&quot;]) table(x$y50) ## ## 0 1 2 3 4 5 ## 3521 792 228 25 2 1 table(x$y100) ## ## 0 1 2 3 4 5 6 ## 2654 833 647 316 92 20 7 We don’t consider the unlimited distance case, because the survey area there is unknown (although we will ultimately address this problem mater). We compare the counts within the 0-50 and 0-100 m circles: m50 &lt;- glm(y50 ~ Decid, data=x, family=poisson) m100 &lt;- glm(y100 ~ Decid, data=x, family=poisson) mean(fitted(m50)) ## [1] 0.2924 mean(fitted(m100)) ## [1] 0.7846 coef(m50) ## (Intercept) Decid ## -2.265 2.126 coef(m100) ## (Intercept) Decid ## -1.327 2.209 3.14 Offsets Offsets are constant terms in the linear predictor, e.g. \\(log(\\lambda_i) = \\beta_0 + \\beta_1 x_{1i} + o_i\\), where \\(o_i\\) is an offset. In the survey area case, an offset might be the log of area surveyed. The logic for this is based on point processes: intensity is a linear function of area under a homogeneous Poisson point process. So we can assume that \\(o_i = log(A_i)\\), where \\(A\\) stands for area. Let’s see if using area as offset makes our models comparable: m50 &lt;- glm(y50 ~ Decid, data=x, family=poisson, offset=rep(log(0.5^2*pi), nrow(x))) m100 &lt;- glm(y100 ~ Decid, data=x, family=poisson, offset=rep(log(1^2*pi), nrow(x))) coef(m50) ## (Intercept) Decid ## -2.024 2.126 coef(m100) ## (Intercept) Decid ## -2.471 2.209 mean(exp(model.matrix(m50) %*% coef(m50))) ## [1] 0.3723 mean(exp(model.matrix(m100) %*% coef(m100))) ## [1] 0.2498 These coefficients and mean predictions are much closer to each other, but something else is going on. Exercise Can you guess why we cannot make abundances comparable using log area as as offset? We pretend again, that survey area varies in our data set: set.seed(1) x$meth &lt;- as.factor(sample(c(&quot;A&quot;, &quot;B&quot;), nrow(x), replace=TRUE)) x$y &lt;- x$y50 x$y[x$meth == &quot;B&quot;] &lt;- x$y100[x$meth == &quot;B&quot;] boxplot(y ~ meth, x) Methodology effect: mm &lt;- glm(y ~ meth - 1, data=x, family=poisson) summary(mm) ## ## Call: ## glm(formula = y ~ meth - 1, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.256 -1.256 -0.775 0.228 3.731 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## methA -1.2040 0.0375 -32.10 &lt;2e-16 *** ## methB -0.2370 0.0240 -9.87 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 7299.4 on 4569 degrees of freedom ## Residual deviance: 5587.9 on 4567 degrees of freedom ## AIC: 9066 ## ## Number of Fisher Scoring iterations: 6 exp(coef(mm)) ## methA methB ## 0.300 0.789 Predictor and method effects: mm &lt;- glm(y ~ Decid + meth, data=x, family=poisson) summary(mm) ## ## Call: ## glm(formula = y ~ Decid + meth, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.185 -0.847 -0.584 0.274 4.347 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.2719 0.0554 -41.0 &lt;2e-16 *** ## Decid 2.1706 0.0690 31.4 &lt;2e-16 *** ## methB 0.9804 0.0445 22.0 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 6110.2 on 4568 degrees of freedom ## Residual deviance: 4531.0 on 4566 degrees of freedom ## AIC: 8011 ## ## Number of Fisher Scoring iterations: 6 boxplot(fitted(mm) ~ meth, x) exp(coef(mm)) ## (Intercept) Decid methB ## 0.1031 8.7632 2.6654 cumsum(colMeans(y))[1:2] ## 0-50m 50-100m ## 0.2924 0.7846 mean(y[,1]) * c(1, exp(coef(mm))[3]) ## methB ## 0.2924 0.7794 Use log area as continuous predictor: we would expect a close to 1:1 relationship on the abundance scale. x$logA &lt;- log(ifelse(x$meth == &quot;A&quot;, 0.5, 1)^2*pi) mm &lt;- glm(y ~ Decid + logA, data=x, family=poisson) summary(mm) ## ## Call: ## glm(formula = y ~ Decid + logA, family = poisson, data = x) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.185 -0.847 -0.584 0.274 4.347 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.1011 0.0513 -40.9 &lt;2e-16 *** ## Decid 2.1706 0.0690 31.4 &lt;2e-16 *** ## logA 0.7072 0.0321 22.0 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 6110.2 on 4568 degrees of freedom ## Residual deviance: 4531.0 on 4566 degrees of freedom ## AIC: 8011 ## ## Number of Fisher Scoring iterations: 6 A &lt;- seq(0, 2, 0.01) # in ha plot(A, exp(log(A) * coef(mm)[3]), type=&quot;l&quot;, ylab=&quot;Method effect&quot;, col=2) abline(0, 1, lty=2) The offset forces the relationship to be 1:1 (it is like fixing the logA coefficient to be 1): mm &lt;- glm(y ~ Decid, data=x, family=poisson, offset=x$logA) summary(mm) ## ## Call: ## glm(formula = y ~ Decid, family = poisson, data = x, offset = x$logA) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.302 -0.836 -0.512 0.260 4.219 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.3374 0.0453 -51.6 &lt;2e-16 *** ## Decid 2.1758 0.0690 31.5 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 5671.1 on 4568 degrees of freedom ## Residual deviance: 4609.2 on 4567 degrees of freedom ## AIC: 8087 ## ## Number of Fisher Scoring iterations: 6 boxplot(fitted(mm) ~ meth, x) cumsum(colMeans(y))[1:2] ## 0-50m 50-100m ## 0.2924 0.7846 c(0.5, 1)^2*pi * mean(exp(model.matrix(mm) %*% coef(mm))) # /ha ## [1] 0.2200 0.8798 Exercise Why did we get a logA coefficient that was less than 1 when theoretically we should have gotten 1? Predictions using offsets in glm can be tricky. The safest way is to use the matrix product (exp(model.matrix(mm) %*% coef(mm) + &lt;offset&gt;)). We can often omit the offset, e.g. in the log area case we can express the prediction per unit area. If the unit is 1 ha, as in our case, log(1)=0, which means the mean abundance per unit area can be calculated by omitting the offsets all together. "],
["behavior.html", "Chapter 4 Behavioral Complexities 4.1 Introduction 4.2 Prerequisites 4.3 Birds in the forest 4.4 Survival model 4.5 Vocalization events 4.6 Removal model 4.7 Finite mixtures 4.8 Let the best model win 4.9 Estimating abundance", " Chapter 4 Behavioral Complexities 4.1 Introduction We have reviewed so far how to fit naive models to estimate the expected value of the observed counts, \\(\\lambda\\). So what is this \\(\\lambda\\)? Here are some deifinitions for further discussion: relative abundance: \\(\\lambda\\) without any reference to nuisance variables, but possibly standardized by design, or nuisance variables used as fixed effects, abundance: \\(N=\\lambda/C\\), \\(C\\) is a correction factor and \\(N\\) refers to the number of individuals within the area surveyed – the problem is that we cannot measure this directly (this is a latent variable), moreover the survey area is also often unknown (i.e. for unlimited distance counts), occupancy: the probability that the survey area is occupied, this is really equivalent to the indicator function \\(N&gt;0\\), density \\(D = N/A = \\lambda/AC\\), abundance per unit area – same problems as above: both \\(N\\) and \\(A\\) are unknowns. Our objective in the following chapters is to work out the details of estimating abundance and density in some clever ways through learning about the nature of the mechanisms contributing to \\(C\\). 4.2 Prerequisites library(bSims) # simulations library(detect) # multinomial models load(&quot;_data/josm/josm.rda&quot;) # JOSM data 4.3 Birds in the forest Build a landscape: extent is given in 100 m units (l &lt;- bsims_init(extent=10)) ## bSims landscape ## 1 km x 1 km ## stratification: H plot(l) We have a 100 ha landscape that we populate with birds, 1 bird / ha using a Poisson spatial point process. As a result, we have \\(N\\) birds in the landscape, \\(N \\sim Poisson(\\lambda)\\), \\(\\lambda = DA\\): set.seed(1) (a &lt;- bsims_populate(l, density=0.5)) ## bSims population ## 1 km x 1 km ## stratification: H ## total abundance: 52 plot(a) The locations can be seen as nest locations (a$nests stores the locations). But birds don’t just stay put in one place. They move and vocalize: (b &lt;- bsims_animate(a, vocal_rate=0.5, duration=10, move_rate=1, movement=0.25)) ## bSims events ## 1 km x 1 km ## stratification: H ## total abundance: 52 ## duration: 10 min plot(b) The get_events function, as the name implies, extracts the events: movements ($v is 0) and vocalizations ($v is 1) alike, unless filtered for vocalization events only. Besides the coordinates, we also have the time of event ($t) and the individual identifier ($i linking to the rows of the b$nests table): e &lt;- get_events(b, event_type=&quot;both&quot;) head(e) v &lt;- get_events(b, event_type=&quot;vocal&quot;) head(v) 4.4 Survival model Survival models assess time-to-event data which is often censored (some event has not occurred at the time the data collection ended). Event time (\\(T\\)) is a continuous random variable. In the simplest case, its probability density function is the Exponential distribution: \\(f(t)=\\phi e^{-t\\phi}\\). The corresponding cumulative distribution function is: \\(F(t)=\\int_{0}^{t} f(t)dt=1-e^{-t\\phi}\\), giving the probability that the event has occurred by duration \\(t\\) and we will refer to this probability as \\(p_t\\). The parameter \\(\\phi\\) is the rate of the Exponential distribution with mean \\(1/\\phi\\) and variance \\(1/\\phi^2\\). In survival model, the complement of \\(F(t)\\) is called the survival function (\\(S(t)=1-F(t)\\), \\(S(0)=1\\)), which gives the probability that the event has not occurred by duration \\(t\\). The the hazard function (\\(\\lambda(t)=f(t)/S(t)\\)) which defines the instantaneous rate of occurrence of the event (the density of events at \\(t\\) divided by the probability of surviving). The cumulative hazard (cumulative risk) the sum of the risks between doration 0 and \\(t\\) (\\(\\Lambda(t)=\\int_{0}^{t} \\lambda(t)dt\\)). The simplest survival distribution assumes constant risk over time (\\(\\lambda(t)=\\phi\\)), which corresponds to the Exponential distribution. The Exponential distribution also happens to describe the lengths of the inter-event times in a homogeneous Poisson process (events are independent, ‘memory-less’ process). 4.5 Vocalization events Event times in our bSims example follow a Poisson process with rate \\(\\phi\\) (vocal_rate) within duration \\(t=10\\) minutes. Let’s subset the vocalization events to include the time of first detections for each individual (v1). The estimated rate should match our setting, the plot shows the Exponential probability density function on top of the event times: v1 &lt;- v[!duplicated(v$i),] tmp &lt;- v1 tmp$o &lt;- seq_len(nrow(v1)) plot(o ~ t, tmp, type=&quot;n&quot;, ylab=&quot;Individuals&quot;, main=&quot;Vocalization events&quot;, ylim=c(1, nrow(b$nests)), xlim=c(0,10)) for (i in tmp$o) { tmp2 &lt;- v[v$i == v1$i[i],] lines(c(tmp2$t[1], 10), c(i,i), col=&quot;grey&quot;) points(tmp2$t, rep(i, nrow(tmp2)), cex=0.5) points(tmp2$t[1], i, pch=19, cex=0.5) } (phi &lt;- b$vocal_rate[1]) ## [1] 0.5 (phi_hat &lt;- fitdistr(v1$t, &quot;exponential&quot;)$estimate) ## rate ## 0.4808 hist(v1$t, xlab=&quot;Time of first detection (min)&quot;, freq=FALSE, main=&quot;&quot;, col=&quot;lightgrey&quot;, ylab=&quot;f(t)&quot;) curve(dexp(x, phi), add=TRUE, col=2) curve(dexp(x, phi_hat), add=TRUE, col=4) legend(&quot;topright&quot;, bty=&quot;n&quot;, lty=1, col=c(2,4), legend=c(&quot;Expected&quot;, &quot;Estimated&quot;)) Now let’s visualize the corresponding cumulative distribution function. We also bin the events into time intervals defined by interval end times in the vector br (breaks to be used with cut): br &lt;- c(3, 5, 10) i &lt;- cut(v1$t, c(0, br), include.lowest = TRUE) table(i) ## i ## [0,3] (3,5] (5,10] ## 38 11 3 plot(stepfun(v1$t, (0:nrow(v1))/nrow(v1)), do.points=FALSE, xlim=c(0,10), xlab=&quot;Time of first detection (min)&quot;, ylab=&quot;F(t)&quot;, main=&quot;&quot;) curve(1-exp(-phi*x), add=TRUE, col=2) curve(1-exp(-phi_hat*x), add=TRUE, col=4) legend(&quot;bottomright&quot;, bty=&quot;n&quot;, lty=c(1,1,1,NA), col=c(1,2,4,3), pch=c(NA,NA,NA,21), legend=c(&quot;Empirical&quot;, &quot;Expected&quot;, &quot;Estimated&quot;, &quot;Binned&quot;)) points(br, cumsum(table(i))/sum(table(i)), cex=2, col=3, pch=21) 4.6 Removal model The time-removal model, originally developed for estimating wildlife and fish abundances from mark-recapture studies, was later reformulated for avian surveys with the goal of improving estimates of bird abundance by accounting for the availability bias inherent in point-count data. The removal model applied to point-count surveys estimates the probability that a bird is available for detection as a function of the average number of detectable cues that an individual bird gives per minute (singing rate, \\(\\phi\\)), and the known count duration (\\(t\\)). Time-removal models are based on a removal experiment whereby animals are trapped and thereby removed from the closed population of animals being sampled. When applying a removal model to avian point-count surveys, the counts of singing birds (\\(Y_{ij}, \\ldots, Y_{iJ}\\)) within a given point-count survey \\(i\\) (\\(i = 1,\\ldots, n\\)) are tallied relative to when each bird is first detected in multiple and consecutive time intervals, with the survey start time \\(t_{i0} = 0\\), the end times of the time intervals \\(t_{ij}\\) (\\(j = 1, 2,\\ldots, J\\)), and the total count duration of the survey \\[t_{iJ}\\]. We count each individual bird once, so individuals are ‘mentally removed’ from a closed population of undetected birds by the surveyor. The continuous-time formulation of the removal model is identical to the Exponential survival model formulation with respect to the cumulative density function, which defines probability of availability for sampling given the occurrence of the species. The response variable in the removal model follows multinomial distribution with cell probabilities derived from the cumulative probability function. We will use the detect::cmulti function to fit multinomial models using conditional maximum likelihood procedure (the conditioning means that we only use observations where the total count is not 0, i.e. the species was present). The Y matrix lists the number of new individuals counted in each time interval, the D matrix gives the interval end times. (We use the detect::cmulti.fit function to be able to fit the model to a single survey.) (y &lt;- matrix(as.numeric(table(i)), nrow=1)) ## [,1] [,2] [,3] ## [1,] 38 11 3 (d &lt;- matrix(br, nrow=1)) ## [,1] [,2] [,3] ## [1,] 3 5 10 (phi_hat1 &lt;- exp(cmulti.fit(y, d, type=&quot;rem&quot;)$coef)) ## [1] 0.4683 phi # setting ## [1] 0.5 phi_hat # from time-to-event data ## rate ## 0.4808 4.6.1 Real data Let’s pick a species from the JOSM data set. For predictors, we will use a variable capturing date (DAY; standardized ordinal day of the year) and an other one capturing time of day (TSSR; time since local sunrise). The data frame X contains the predictors. The matrix Y contains the counts of newly counted individuals binned into consecutive time intervals: cell values are the \\(Y_{ij}\\)’s. The D object is another matrix mirroring the structure of Y but instead of counts, it contains the interval end times: cell values are the \\(t_{ij}\\)’s. yall &lt;- Xtab(~ SiteID + Dur + SpeciesID, josm$counts[josm$counts$DetectType1 != &quot;V&quot;,]) yall &lt;- yall[sapply(yall, function(z) sum(rowSums(z) &gt; 0)) &gt; 100] spp &lt;- &quot;TEWA&quot; Y &lt;- as.matrix(yall[[spp]]) D &lt;- matrix(c(3, 5, 10), nrow(Y), 3, byrow=TRUE, dimnames=dimnames(Y)) X &lt;- josm$surveys[rownames(Y), c(&quot;DAY&quot;, &quot;TSSR&quot;)] head(Y[rowSums(Y) &gt; 0,]) ## 0-3min 3-5min 5-10min ## CL10106 4 0 0 ## CL10112 2 0 0 ## CL10120 1 1 0 ## CL10170 1 0 0 ## CL10172 0 0 2 ## CL10181 0 0 1 head(D) ## 0-3min 3-5min 5-10min ## CL10102 3 5 10 ## CL10106 3 5 10 ## CL10108 3 5 10 ## CL10109 3 5 10 ## CL10111 3 5 10 ## CL10112 3 5 10 summary(X) ## DAY TSSR ## Min. :0.392 Min. :-0.0285 ## 1st Qu.:0.422 1st Qu.: 0.0506 ## Median :0.452 Median : 0.1041 ## Mean :0.450 Mean : 0.1040 ## 3rd Qu.:0.474 3rd Qu.: 0.1568 ## Max. :0.504 Max. : 0.2357 The D matrix can take different methodologies for each row. The leftover values in each row must be filled with NAs and the pattern of NAs must match between the Y and D matrices (i.e. you should’t have observation in a non-existing time interval). Integrating data becomes really easy this way, for example: matrix(c(3, 5, 10, NA, NA, 1:5, 4, 8, NA, NA, NA), 3, byrow=TRUE) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 3 5 10 NA NA ## [2,] 1 2 3 4 5 ## [3,] 4 8 NA NA NA 4.6.2 Time-invariant conventional model Time-invariant means that the rate is constant over time (i.e. no difference between morning and midnight), while conventional refers to the assumption that all individuals share the same rate (their behaviour is identical in this regard). In the time-invariant conventional removal model (Me0), the individuals of a species at a given location and time are assumed to be homogeneous in their singing rates. The time to first detection follows the Exponential distribution, and the cumulative density function of times to first detection in time interval (0, \\(t_{iJ}\\)) gives us the probability that a bird sings at least once during the point count as \\(p(t_{iJ}) = 1 - exp(-t_{iJ} \\phi)\\). We fit this model by specifying intercep-only in the right hand side of the formula, and type=&quot;rem&quot; as part of the cmulti call: Me0 &lt;- cmulti(Y | D ~ 1, type=&quot;rem&quot;) summary(Me0) ## ## Call: ## cmulti(formula = Y | D ~ 1, type = &quot;rem&quot;) ## ## Removal Sampling (homogeneous singing rate) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.phi_(Intercept) -0.8547 0.0174 -49.1 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -3.2e+03 ## BIC = 6.42e+03 (phi_Me0 &lt;- exp(coef(Me0))) ## log.phi_(Intercept) ## 0.4254 curve(1-exp(-x*phi_Me0), xlim=c(0, 10), ylim=c(0, 1), col=4, xlab=&quot;Duration (min)&quot;, ylab=expression(p(t[J])), main=paste(spp, &quot;Me0&quot;)) points(D[1,], cumsum(colSums(Y))/sum(Y), cex=2, col=3, pch=21) 4.6.3 Time-varying conventional removal model Singing rates of birds vary with time of day, time of year, breeding status, and stage of the nesting cycle. Thus, removal model estimates of availability may be improved by accounting for variation in singing rates using covariates for day of year and time of day. In this case \\(p(t_{iJ}) = 1 - e^{-t_{iJ} \\phi_{i}}\\) and \\(log(\\phi_{i}) = \\beta_{0} + \\sum^{K}_{k=1} \\beta_{k} x_{ik}\\) is the linear predictor with \\(K\\) covariates and the corresponding unknown coefficients (\\(\\beta_{k}\\), \\(k = 0,\\ldots, K\\)). Let’s fit a couple of time-varying models using DAY and TSSR as covariates: Me1 &lt;- cmulti(Y | D ~ DAY, X, type=&quot;rem&quot;) Me2 &lt;- cmulti(Y | D ~ TSSR, X, type=&quot;rem&quot;) Now compare the three conventional models based on AIC and inspect the summary for the best supported model with the JDAY effect. Me_AIC &lt;- AIC(Me0, Me1, Me2) Me_AIC$delta_AIC &lt;- Me_AIC$AIC - min(Me_AIC$AIC) Me_AIC[order(Me_AIC$AIC),] Me_Best &lt;- get(rownames(Me_AIC)[Me_AIC$delta_AIC == 0]) summary(Me_Best) ## ## Call: ## cmulti(formula = Y | D ~ DAY, data = X, type = &quot;rem&quot;) ## ## Removal Sampling (homogeneous singing rate) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.phi_(Intercept) 0.0784 0.2615 0.30 0.76427 ## log.phi_DAY -2.0910 0.5866 -3.56 0.00036 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -3.2e+03 ## BIC = 6.41e+03 To visually capture the time-varying effects, we make some plots using base graphics, colors matching the time-varying predictor. This way we can not only assess how availability probability (given a fixed time interval) is changing with the values of the predictor, but also how the cumulative distribution changes with time. b &lt;- coef(Me_Best) n &lt;- 100 DAY &lt;- seq(min(X$DAY), max(X$DAY), length.out=n+1) TSSR &lt;- seq(min(X$TSSR), max(X$TSSR), length.out=n+1) Duration &lt;- seq(0, 10, length.out=n) col &lt;- colorRampPalette(c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;))(n) op &lt;- par(mfrow=c(1,2)) p1 &lt;- 1-exp(-3*exp(b[1]+b[2]*DAY)) plot(DAY, p1, ylim=c(0,1), type=&quot;n&quot;, main=paste(spp, rownames(Me_AIC)[Me_AIC$delta_AIC == 0]), ylab=&quot;P(availability)&quot;) for (i in seq_len(n)) { lines(DAY[c(i,i+1)], p1[c(i,i+1)], col=col[i], lwd=2) } abline(h=range(p1), col=&quot;grey&quot;) plot(Duration, Duration, type=&quot;n&quot;, ylim=c(0,1), ylab=&quot;P(availability)&quot;) for (i in seq_len(n)) { p2 &lt;- 1-exp(-Duration*exp(b[1]+b[2]*DAY[i])) lines(Duration, p2, col=col[i]) } abline(v=3, h=range(p1), col=&quot;grey&quot;) par(op) 4.7 Finite mixtures Let’s relax the assumption that all individuals vocalize at the same rate. We can think about this as different groups in the population. The individuals within the groups have homogenerous rates, but the group level rates are different. We can introduce such heterogeneity into our bSims world by specifying the group level rates (phi vector) and the proportion of individuals belonging to the groups (mix). phi &lt;- c(10, 0.5) mix &lt;- c(0.25, 0.75) set.seed(1) (a2 &lt;- bsims_populate(l, density=1)) # increase density ## bSims population ## 1 km x 1 km ## stratification: H ## total abundance: 104 (b2 &lt;- bsims_animate(a2, vocal_rate=phi, mixture=mix)) ## bSims events ## 1 km x 1 km ## stratification: H ## total abundance: 104 ## mixture, duration: 10 min b2$vocal_rate ## G1 G2 ## H 10 0.5 ## E 10 0.5 ## R 10 0.5 If we plot the time to first detection data, we can see how expected distribution (red) is different from the fitted Exponential distribution assuming homogeneity: v &lt;- get_events(b2, event_type=&quot;vocal&quot;) v1 &lt;- v[!duplicated(v$i),] (phi_hat &lt;- fitdistr(v1$t, &quot;exponential&quot;)$estimate) ## rate ## 0.6068 hist(v1$t, xlab=&quot;Time of first detection (min)&quot;, freq=FALSE, main=&quot;&quot;, col=&quot;lightgrey&quot;, ylab=&quot;f(t)&quot;) curve(mix[1]*dexp(x, phi[1])+mix[2]*dexp(x, phi[2]), add=TRUE, col=2) curve(dexp(x, phi_hat), add=TRUE, col=4) legend(&quot;topright&quot;, bty=&quot;n&quot;, lty=1, col=c(2,4), legend=c(&quot;Expected (mixture)&quot;, &quot;Estimated (exponential)&quot;)) Now let’s visualize the corresponding cumulative distribution function: br &lt;- 1:10 i &lt;- cut(v1$t, c(0, br), include.lowest = TRUE) table(i) ## i ## [0,1] (1,2] (2,3] (3,4] (4,5] (5,6] (6,7] (7,8] (8,9] (9,10] ## 49 22 17 5 3 3 0 2 1 1 plot(stepfun(v1$t, (0:nrow(v1))/nrow(v1)), do.points=FALSE, xlim=c(0,10), xlab=&quot;Time of first detection (min)&quot;, ylab=&quot;F(t)&quot;, main=&quot;&quot;) curve(1-mix[2]*exp(-phi[2]*x), add=TRUE, col=2) curve(1-exp(-phi_hat*x), add=TRUE, col=4) legend(&quot;bottomright&quot;, bty=&quot;n&quot;, lty=c(1,1,1,NA), col=c(1,2,4,3), pch=c(NA,NA,NA,21), legend=c(&quot;Empirical&quot;, &quot;Expected (mixture)&quot;, &quot;Estimated (exponential)&quot;, &quot;Binned&quot;)) points(br, cumsum(table(i))/sum(table(i)), cex=2, col=3, pch=21) We use the detect::cmulti function to fit the finite mixture model: (y &lt;- matrix(as.numeric(table(i)), nrow=1)) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 49 22 17 5 3 3 0 2 1 1 (d &lt;- matrix(br, nrow=1)) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 2 3 4 5 6 7 8 9 10 cf &lt;- cmulti.fit(y, d, type=&quot;fmix&quot;)$coef # log.phi, logit.c c(phi=phi[2], c=mix[2]) # setting ## phi c ## 0.50 0.75 c(phi_hat=exp(cf[1]), c_hat=plogis(cf[2])) # estimate ## phi_hat c_hat ## 0.5176 0.8837 4.7.1 Time-invariant finite mixture removal model The removal model can accommodate behavioral heterogeneity in singing by subdividing the sampled population for a species at a given point into a finite mixture of birds with low and high singing rates, which requires the additional estimation of the proportion of birds in the sampled population with low singing rates. In the continuous-time formulation of the finite mixture (or two-point mixture) removal model, the cumulative density function during a point count is given by \\(p(t_{iJ}) = (1 - c) 1 + c (1 - e^{-t_{iJ} \\phi}) = 1 - c e^{-t_{iJ} \\phi}\\), where \\(\\phi\\) is the singing rate for the group of infrequently singing birds, and \\(c\\) is the proportion of birds during the point count that are infrequent singers. The remaining proportions (\\(1 - c\\); the intercept of the cumulative density function) of the frequent singers are assumed to be detected instantaneously at the start of the first time interval. In the simplest form of the finite mixture model, the proportion and singing rate of birds that sing infrequently is homogeneous across all times and locations (model Mf0). We are using the type = &quot;fmix&quot; for finite mixture removal models. Here, for the read bird data set: Mf0 &lt;- cmulti(Y | D ~ 1, type=&quot;fmix&quot;) summary(Mf0) ## ## Call: ## cmulti(formula = Y | D ~ 1, type = &quot;fmix&quot;) ## ## Removal Sampling (heterogeneous singing rate) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.phi_(Intercept) -1.7146 0.0970 -17.68 &lt;2e-16 *** ## logit.c 0.0742 0.0598 1.24 0.21 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -3.1e+03 ## BIC = 6.22e+03 cf_Mf0 &lt;- coef(Mf0) curve(1-plogis(cf_Mf0[2]) * exp(-x*exp(cf_Mf0[1])), xlim=c(0, 10), ylim=c(0, 1), col=4, main=paste(spp, &quot;Mf0&quot;), xlab=&quot;Duration (min)&quot;, ylab=expression(p(t[J]))) points(D[1,], cumsum(colSums(Y))/sum(Y), cex=2, col=3, pch=21) 4.7.2 Time-varying finite mixture removal models Previously, researchers have applied covariate effects on the parameter \\(\\phi_{i}\\) of the finite mixture model, similarly to how we modeled these effects in conventional models. This model assumes that the parameter \\(c\\) is constant irrespective of time and location (i.e. only the infrequent singer group changes its singing behavior). We can fit finite mixture models with DAY and TSSR as covariates on \\(\\phi\\). In this case \\(p(t_{iJ}) = 1 - c e^{-t_{iJ} \\phi_{i}}\\) and \\(log(\\phi_{i}) = \\beta_{0} + \\sum^{K}_{k=1} \\beta_{k} x_{ik}\\) is the linear predictor with \\(K\\) covariates and the corresponding unknown coefficients (\\(\\beta_{k}\\), \\(k = 0,\\ldots, K\\)). Mf1 &lt;- cmulti(Y | D ~ DAY, X, type=&quot;fmix&quot;) Mf2 &lt;- cmulti(Y | D ~ TSSR, X, type=&quot;fmix&quot;) Compare the three finite mixture models based on AIC and inspect the summary for the best supported model: Mf_AIC &lt;- AIC(Mf0, Mf1, Mf2) Mf_AIC$delta_AIC &lt;- Mf_AIC$AIC - min(Mf_AIC$AIC) Mf_Best &lt;- get(rownames(Mf_AIC)[Mf_AIC$delta_AIC == 0]) Mf_AIC[order(Mf_AIC$AIC),] summary(Mf_Best) ## ## Call: ## cmulti(formula = Y | D ~ DAY, data = X, type = &quot;fmix&quot;) ## ## Removal Sampling (heterogeneous singing rate) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.phi_(Intercept) 0.754 0.848 0.89 0.3739 ## log.phi_DAY -5.412 1.938 -2.79 0.0052 ** ## logit.c 0.119 0.062 1.92 0.0548 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -3.1e+03 ## BIC = 6.22e+03 We produce a similar plot as before. b &lt;- coef(Mf_Best) op &lt;- par(mfrow=c(1,2)) p1 &lt;- 1-plogis(b[3])*exp(-3*exp(b[1]+b[2]*DAY)) plot(DAY, p1, ylim=c(0,1), type=&quot;n&quot;, main=paste(spp, rownames(Mf_AIC)[Mf_AIC$delta_AIC == 0]), ylab=&quot;P(availability)&quot;) for (i in seq_len(n)) { lines(DAY[c(i,i+1)], p1[c(i,i+1)], col=col[i], lwd=2) } abline(h=range(p1), col=&quot;grey&quot;) plot(Duration, Duration, type=&quot;n&quot;, ylim=c(0,1), ylab=&quot;P(availability)&quot;) for (i in seq_len(n)) { p2 &lt;- 1-plogis(b[3])*exp(-Duration*exp(b[1]+b[2]*DAY[i])) lines(Duration, p2, col=col[i]) } abline(v=3, h=range(p1), col=&quot;grey&quot;) par(op) An alternative parametrization is that \\(c_{i}\\) rather than \\(\\phi\\) be the time-varying parameter, allowing the individuals to switch between the frequent and infrequent group depending on covariates. We can fit this class of finite mixture model with DAY and TSSR as covariates on \\(c\\) using type = &quot;mix&quot; (instead of &quot;fmix&quot;). In this case \\(p(t_{iJ}) = 1 - c_{i} e^{-t_{iJ} \\phi}\\) and \\(logit(c_{i}) = \\beta_{0} + \\sum^{K}_{k=1} \\beta_{k} x_{ik}\\) is the linear predictor with \\(K\\) covariates and the corresponding unknown coefficients (\\(\\beta_{k}\\), \\(k = 0,\\ldots, K\\)). Because \\(c_{i}\\) is a proportion, we model it on the logit scale. Mm1 &lt;- cmulti(Y | D ~ DAY, X, type=&quot;mix&quot;) Mm2 &lt;- cmulti(Y | D ~ TSSR, X, type=&quot;mix&quot;) We did not fit a null model for this parametrization, because it is identical to the Mf0 model, so that model Mf0 is what we use to compare AIC values and inspect the summary for the best supported model: Mm_AIC &lt;- AIC(Mf0, Mm1, Mm2) Mm_AIC$delta_AIC &lt;- Mm_AIC$AIC - min(Mm_AIC$AIC) Mm_Best &lt;- get(rownames(Mm_AIC)[Mm_AIC$delta_AIC == 0]) Mm_AIC[order(Mm_AIC$AIC),] summary(Mm_Best) ## ## Call: ## cmulti(formula = Y | D ~ DAY, data = X, type = &quot;mix&quot;) ## ## Removal Sampling (heterogeneous singing rate) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.phi -1.716 0.097 -17.69 &lt;2e-16 *** ## logit.c_(Intercept) -2.070 0.692 -2.99 0.0028 ** ## logit.c_DAY 4.804 1.558 3.08 0.0020 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -3.1e+03 ## BIC = 6.22e+03 We produce a similar plot as before: b &lt;- coef(Mm_Best) op &lt;- par(mfrow=c(1,2)) p1 &lt;- 1-plogis(b[2]+b[3]*DAY)*exp(-3*exp(b[1])) plot(DAY, p1, ylim=c(0,1), type=&quot;n&quot;, main=paste(spp, rownames(Mm_AIC)[Mm_AIC$delta_AIC == 0]), ylab=&quot;P(availability)&quot;) for (i in seq_len(n)) { lines(DAY[c(i,i+1)], p1[c(i,i+1)], col=col[i], lwd=2) } abline(h=range(p1), col=&quot;grey&quot;) plot(Duration, Duration, type=&quot;n&quot;, ylim=c(0,1), ylab=&quot;P(availability)&quot;) for (i in seq_len(n)) { p2 &lt;- 1-plogis(b[2]+b[3]*DAY[i])*exp(-Duration*exp(b[1])) lines(Duration, p2, col=col[i]) } abline(v=3, h=range(p1), col=&quot;grey&quot;) par(op) 4.8 Let the best model win So which of the 3 parametrizations proved to be best for our data? It was the finite mixture with time-varying proportion of infrequent singers. Second was the other finite mixture model, while the conventional model was lagging behind. M_AIC &lt;- AIC(Me_Best, Mf_Best, Mm_Best) M_AIC$delta_AIC &lt;- M_AIC$AIC - min(M_AIC$AIC) M_AIC[order(M_AIC$AIC),] Finite mixture models provide some really nice insight into how singing behavior changes over time and, due to more parameters, they provide a better fit and thus minimize bias in population size estimates. But all this improvement comes with a price: sample size requirements (or more precisely, the number of detections required) are really high. To have all the benefits with reduced variance, one needs about 1000 non-zero observations to fit finite mixture models, 20 times more than needed to reliably fit conventional removal models. This is much higher than previously suggested minimum sample sizes. Our findings also indicate that lengthening the count duration from 3 minutes to 5–10 minutes is an important consideration when designing field surveys to increase the accuracy and precision of population estimates. Well-informed survey design combined with various forms of removal sampling are useful in accounting for availability bias in point counts, thereby improving population estimates, and allowing for better integration of disparate studies at larger spatial scales. Exercise Compare different durations, numbers and lengths of time intervals when estimating vocalization rates. Estimate vocalization rates for other species (e.g. rare species, specias with less frequent vocalizations). Compare linear and polynomial DAY effects for migratory and resident species (e.g. BCCH, BOCH, BRCR, CORA, GRAJ, RBNU). 4.9 Estimating abundance Let us use the bSims approach to see how well we can estimate abundance after accounting for availability. We set Den as density (\\(D\\)), and because area is \\(A\\) = 100 ha by default, the expected value of the abundance (\\(\\lambda\\)) bacomes \\(AD\\), while the actual abundance (\\(N\\)) is a realization of that based on Poisson distribution (\\(N \\sim Poisson(\\lambda)\\)): phi &lt;- 0.5 Den &lt;- 1 set.seed(1) l &lt;- bsims_init() a &lt;- bsims_populate(l, density=Den) (b &lt;- bsims_animate(a, vocal_rate=phi, move_rate=0)) ## bSims events ## 1 km x 1 km ## stratification: H ## total abundance: 104 ## duration: 10 min The next function we use is bsims_transcribe which takes the events data and bins it according to time intervals, tint defines the end times of each interval: tint &lt;- c(1, 2, 3, 4, 5) (tr &lt;- bsims_transcribe(b, tint=tint)) ## bSims transcript ## 1 km x 1 km ## stratification: H ## total abundance: 104 ## duration: 10 min ## detected: 104 heard ## 1st event detected by bins: ## [0-1, 1-2, 2-3, 3-4, 4-5 min] ## [0+ m] tr$removal # binned new individuals ## 0-1min 1-2min 2-3min 3-4min 4-5min ## 0+m 35 28 16 12 6 (Y &lt;- sum(tr$removal)) # detected in 0-3 min ## [1] 97 After max(tint) duration, we detected \\(Y\\) individuals. Because \\(E[Y] = NC\\), we only have to estimate the correction factor \\(C\\), that happens to be \\(C=p\\) in this case because our bSims world ignored the observation process so far. \\(p\\) is estimated based on \\(\\phi\\): fit &lt;- cmulti.fit(tr$removal, matrix(tint, nrow=1), type=&quot;rem&quot;) c(true=phi, estimate=exp(fit$coef)) ## true estimate ## 0.5000 0.4083 (p &lt;- 1-exp(-max(tint)*exp(fit$coef))) ## [1] 0.8702 tt &lt;- seq(0, 10, 0.01) plot(tt, 1-exp(-tt*phi), type=&quot;l&quot;, ylim=c(0, 1), ylab=&quot;P(availability)&quot;, xlab=&quot;Duration&quot;, lty=2) lines(tt, 1-exp(-tt*exp(fit$coef))) for (i in seq_len(length(tint))) { ii &lt;- c(0, tint)[c(i, i+1)] ss &lt;- tt &gt;= ii[1] &amp; tt &lt;= ii[2] xi &lt;- tt[ss] yi &lt;- 1-exp(-xi*exp(fit$coef)) polygon(c(xi, xi[length(xi)]), c(yi, yi[1]), border=NA, col=&quot;#0000ff33&quot;) } legend(&quot;bottomright&quot;, bty=&quot;n&quot;, lty=c(2, 1, NA), fill=c(NA, NA, &quot;#0000ff33&quot;), border=NA, legend=c(&quot;True&quot;, &quot;Estimated&quot;, &quot;&#39;New individuals&#39;&quot;)) Our estimate of \\(N\\) becomes \\(Y/C=Y/p\\): N &lt;- sum(a$abundance) Nhat &lt;- Y/p c(true=N, estimate=Nhat) ## true estimate ## 104.0 111.5 In this case, area is known, so density becomes: A &lt;- sum(a$area) c(true=N / A, estimate=Nhat / A) ## true estimate ## 1.040 1.115 Next we use the Best model from our real JOSM bird data analysis: spp &lt;- &quot;TEWA&quot; Y &lt;- as.matrix(yall[[spp]]) D &lt;- matrix(c(3, 5, 10), nrow(Y), 3, byrow=TRUE, dimnames=dimnames(Y)) X &lt;- josm$surveys[rownames(Y), c(&quot;DAY&quot;, &quot;TSSR&quot;)] Best &lt;- get(rownames(M_AIC)[M_AIC$delta_AIC == 0]) summary(Best) ## ## Call: ## cmulti(formula = Y | D ~ DAY, data = X, type = &quot;mix&quot;) ## ## Removal Sampling (heterogeneous singing rate) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.phi -1.716 0.097 -17.69 &lt;2e-16 *** ## logit.c_(Intercept) -2.070 0.692 -2.99 0.0028 ** ## logit.c_DAY 4.804 1.558 3.08 0.0020 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -3.1e+03 ## BIC = 6.22e+03 In this case, availability varies due to DAY. Our estimate of \\(N_i\\) becomes \\(Y_i/C_i=Y_i/p_i\\): p &lt;- 1 - plogis(model.matrix(Best) %*% coef(Best)[-1]) * exp(-10 * exp(coef(Best)[1])) summary(p) ## V1 ## Min. :0.903 ## 1st Qu.:0.909 ## Median :0.913 ## Mean :0.914 ## 3rd Qu.:0.919 ## Max. :0.925 We can now calculate mean abundance, where ytot tallies up the counts across the 3 time intervals: ytot &lt;- rowSums(Y) table(ytot) ## ytot ## 0 1 2 3 4 5 6 7 8 12 ## 1782 1151 887 466 188 71 20 2 1 1 mean(ytot / p) ## [1] 1.337 Alternatively, we can fit a GLM and use log(p) as an offset: mod &lt;- glm(ytot ~ 1, family=poisson, offset=log(p)) summary(mod) ## ## Call: ## glm(formula = ytot ~ 1, family = poisson, offset = log(p)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.573 -1.560 -0.207 0.645 5.777 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.2910 0.0134 21.8 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 7087.6 on 4568 degrees of freedom ## Residual deviance: 7087.6 on 4568 degrees of freedom ## AIC: 14054 ## ## Number of Fisher Scoring iterations: 5 The GLM based estimate comes from the intercept, because \\(E[Y_i]=N_i C_i\\) is equivalent to \\(\\lambda_i=e^{\\beta_0} e^{o_i}\\), this \\(\\hat{N_i}=e^{\\hat{\\beta_0}}\\): exp(coef(mod)) ## (Intercept) ## 1.338 This result tells us mean abundance after correcting for availability bias, but we don’t know what area was effectively sampled, and detection of individuals given availability is probably less than 1 because this happens to be a real data set and it is guaranteed that humans in the forest cannot detect birds that are very far (say &gt; 500 m away). We shall address these problem in the next chapter. "],
["detection.html", "Chapter 5 The Detection Process 5.1 Introduction 5.2 Prerequisites 5.3 Distance functions 5.4 Distance sampling 5.5 Average detection 5.6 Binned distances 5.7 Availability bias 5.8 Estimating density with truncation 5.9 Unlimited distance 5.10 Replicating landscapes 5.11 JOSM data", " Chapter 5 The Detection Process 5.1 Introduction As part of the detection process, a skilled observer counts individual birds at a count station. New individuals are assigned to time and distance categories, the type of behavior also registered. During this process, auditory cues travel through the distance between the bird and the observer. As the pover of the sound fades away, the chanches of being detected also decreases. If the detection process is based on visual detections, vegetation can block line of sight, etc. In this chapter, we scrutinize how this detection process contributes to the factor \\(C\\). 5.2 Prerequisites library(bSims) # simulations library(detect) # multinomial models library(Distance) # distance sampling load(&quot;_data/josm/josm.rda&quot;) # JOSM data source(&quot;functions.R&quot;) # some useful stuff 5.3 Distance functions The distance function (\\(g(d)\\) describes the probability of detecting an individual given the distance between the observer and the individual (\\(d\\)). The detection itself is often triggered by visual or auditory cues, and thus depend on the individuals being available for detection (and of course being present in the survey area). Distance functions have some characteristics: It is a monotonic decreasing function of distance, \\(g(0)=1\\): detection at 0 distance is perfect. Here are some common distance function and rationale for their use (i.e. mechanisms leading to such distance shapes): Negative Exponential: a one-parameter function (\\(g(d) = e^{-d/\\tau}\\)), probability quickly decreases with distance, this mirrors sound attenuation under spherical spreading, so might be a suitable form for acoustic recoding devices (we will revisit this later), but not a very useful form for human based counts, as explained below; Half-Normal: this is also a one-parameter function (\\(g(d) = e^{-(d/\\tau)^2}\\)) where probability initially remain high (the shoulder), reflecting an increased chance of detecting individuals closer to the observer, this form has also sone practical advantages that we will discuss shortly (\\(\\tau^2\\) is variance of the unfolded Normal distribution, \\(\\tau^2/2\\) is the variance of the Half-Normal distribution – both the Negative Exponential and the Half-Normal being special cases of \\(g(d) = e^{-(d/\\tau)^b}\\) that have the parameter \\(b\\) [\\(b &gt; 0\\)] affecting the shoulder); Hazard rate: this is a two-parameter model (\\(g(d) = 1-e^{-(d/\\tau)^-b}\\)) that have the parameter \\(b\\) (\\(b &gt; 0\\)) affecting the more pronounced and sharp shoulder. d &lt;- seq(0, 2, 0.01) plot(d, exp(-d/0.8), type=&quot;l&quot;, col=4, ylim=c(0,1), xlab=&quot;Distance (100 m)&quot;, ylab=&quot;P(detection)&quot;, main=&quot;Negative Exponential&quot;) plot(d, exp(-(d/0.8)^2), type=&quot;l&quot;, col=4, ylim=c(0,1), xlab=&quot;Distance (100 m)&quot;, ylab=&quot;P(detection)&quot;, main=&quot;Half-Normal&quot;) plot(d, 1-exp(-(d/0.8)^-4), type=&quot;l&quot;, col=4, ylim=c(0,1), xlab=&quot;Distance (100 m)&quot;, ylab=&quot;P(detection)&quot;, main=&quot;Hazard rate&quot;) Exercise Try different values of \\(b\\) to explore the different shapes of the Hazard rate function. Write your own code (plot(d, exp(-(d/&lt;tau&gt;)^&lt;b&gt;), type=&quot;l&quot;, ylim=c(0,1))), or run shiny::runApp(&quot;_shiny/distancefun.R&quot;). We will apply this new found knowledge to our bSims world: the observer is in the middle of the landscape, and each vocalization event is aither detected or not, depending on the distance. Units of tau are given on 100 m units, so that corresponding density estimates will refer to ha as the unit area. In this example, we want all individuals to be equally available, so we are going to override all behavioral aspects of the simulations by the initial_location argument when calling bsims_animate. We set density and tau high enough to detections in this example. tau &lt;- 2 set.seed(123) l &lt;- bsims_init() a &lt;- bsims_populate(l, density=10) b &lt;- bsims_animate(a, initial_location=TRUE) (o &lt;- bsims_detect(b, tau=tau)) ## bSims detections ## 1 km x 1 km ## stratification: H ## total abundance: 1013 ## no events, duration: 10 min ## detected: 128 seen/heard plot(o) 5.4 Distance sampling The distribution of the observed distances is a product of detectability and the distribution of the individuals with respect to the point where the observer is located. For point counts, area increases linearly with radial distance, implying a triangular distribution with respect to the point (\\(h(d)=\\pi 2 d /A=\\pi 2 d / \\pi r_{max}^2=2 d / r_{max}^2\\), where \\(A\\) is a circular survey area with truncation distance \\(r_{max}\\)). The product \\(g(d) h(d)\\) gives the density function of the observed distances. g &lt;- function(d, tau, b=2, hazard=FALSE) if (hazard) 1-exp(-(d/tau)^-b) else exp(-(d/tau)^b) h &lt;- function(d, rmax) 2*d/rmax^2 rmax &lt;- 4 d &lt;- seq(0, rmax, 0.01) plot(d, g(d, tau), type=&quot;l&quot;, col=4, ylim=c(0,1), xlab=&quot;d&quot;, ylab=&quot;g(d)&quot;, main=&quot;Prob. of detection&quot;) plot(d, h(d, rmax), type=&quot;l&quot;, col=4, xlab=&quot;d&quot;, ylab=&quot;h(d)&quot;, main=&quot;PDF of distances&quot;) plot(d, g(d, tau) * h(d, rmax), type=&quot;l&quot;, col=4, xlab=&quot;d&quot;, ylab=&quot;g(d) h(d)&quot;, main=&quot;Density of observed distances&quot;) The object da contains the distances to all the nests based on our bSims object, we use this to display the distribution of available distances: da &lt;- sqrt(rowSums(a$nests[,c(&quot;x&quot;, &quot;y&quot;)]^2)) hist(da[da &lt;= rmax], freq=FALSE, xlim=c(0, rmax), xlab=&quot;Available distances (d &lt;= r_max)&quot;, main=&quot;&quot;) curve(2*x/rmax^2, add=TRUE, col=2) The get_detections function returns a data frame with the detected events (in our case just the nest locations): $d is the distance, $a is the angle (in degrees, counter clock-wise from positive x axis). head(dt &lt;- get_detections(o)) The following code plots the probability density of the observed distances within the truncation distance \\(r_{max}\\), thus we need to standardize the \\(g(r) h(r)\\) function by the integral sum: f &lt;- function(d, tau, b=2, hazard=FALSE, rmax=1) g(d, tau, b, hazard) * h(d, rmax) tot &lt;- integrate(f, lower=0, upper=rmax, tau=tau, rmax=rmax)$value hist(dt$d[dt$d &lt;= rmax], freq=FALSE, xlim=c(0, rmax), xlab=&quot;Observed distances (r &lt;= rmax)&quot;, main=&quot;&quot;) curve(f(x, tau=tau, rmax=rmax) / tot, add=TRUE, col=2) In case of the Half-Normal, we can linearize the relationship by taking the log of the distance function: \\(log(g(d)) =log(e^{-(d/\\tau)^2})= -(d / \\tau)^2 = x \\frac{1}{\\tau^2} = 0 + x \\beta\\). Consequently, we can use GLM to fit a model with \\(x = -d^2\\) as predictor and no intercept, and estimate \\(\\hat{\\beta}\\) and \\(\\hat{\\tau}=\\sqrt{1/\\hat{\\beta}}\\). For this method to work, we need to know the observed and unobserved distances as well, which makes this approach of low utility in practice when location of unobserved individuals is unknown. But we can at least check our bSims data: dat &lt;- data.frame( distance=da, x=-da^2, detected=ifelse(rownames(o$nests) %in% dt$i, 1, 0)) summary(dat) ## distance x detected ## Min. :0.226 Min. :-49.33 Min. :0.000 ## 1st Qu.:2.876 1st Qu.:-23.74 1st Qu.:0.000 ## Median :3.965 Median :-15.72 Median :0.000 ## Mean :3.827 Mean :-16.69 Mean :0.126 ## 3rd Qu.:4.872 3rd Qu.: -8.27 3rd Qu.:0.000 ## Max. :7.024 Max. : -0.05 Max. :1.000 mod &lt;- glm(detected ~ x - 1, data=dat, family=binomial(link=&quot;log&quot;)) c(true=tau, estimate=sqrt(1/coef(mod))) ## true estimate.x ## 2.000 2.034 curve(exp(-(x/sqrt(1/coef(mod)))^2), xlim=c(0,max(dat$distance)), ylim=c(0,1), xlab=&quot;Distance (100 m)&quot;, ylab=&quot;P(detection)&quot;) curve(exp(-(x/tau)^2), lty=2, add=TRUE) rug(dat$distance[dat$detected == 0], side=1, col=4) rug(dat$distance[dat$detected == 1], side=3, col=2) legend(&quot;topright&quot;, bty=&quot;n&quot;, lty=c(2,1), legend=c(&quot;True&quot;, &quot;Estimated&quot;)) The Distance package offers various tools to fit models to observed distance data. See here for a tutorial. The following script fits the Half-Normal (key = &quot;hn&quot;) without ajustments (adjustment=NULL) to observed distance data from truncated point transect. It estimates \\(\\sigma = \\sqrt{\\tau}\\): dd &lt;- ds(dt$d, truncation = rmax, transect=&quot;point&quot;, key = &quot;hn&quot;, adjustment=NULL) ## Fitting half-normal key function ## Key only model: not constraining for monotonicity. ## AIC= 315.502 ## No survey area information supplied, only estimating detection function. c(true=tau, estimate=exp(dd$ddf$par)^2) ## true estimate ## 2.000 2.176 5.5 Average detection To calculate the average probability of detecting individuals within a circle with truncation distance \\(r_{max}\\), we need to integrate over the product of \\(g(r)\\) and \\(h(r)\\): \\(q(r_{max})=\\int_{0}^{r_{max}} g(d) h(d) dd\\). This gives the volume of pie dough cut at \\(r_{max}\\), compared to the volume of the cookie cutter (\\(\\pi r_{max}^2\\)). q &lt;- sapply(d[d &gt; 0], function(z) integrate(f, lower=0, upper=z, tau=tau, rmax=z)$value) plot(d, c(1, q), type=&quot;l&quot;, col=4, ylim=c(0,1), xlab=expression(r[max]), ylab=expression(q(r[max])), main=&quot;Average prob. of detection&quot;) For the Half-Normal detection function, the analytical solution for the average probability is \\(\\pi \\tau^2 [1-exp(-d^2/\\tau^2)] / (\\pi r_{max}^2)\\), where the denominator is a normalizing constant representing the volume of a cylinder of perfect detectability. To visualize this, here is the pie analogy for \\(\\tau=2\\) and \\(r_{max}=2\\): tau &lt;- 2 rmax &lt;- 2 w &lt;- 0.1 m &lt;- 2 plot(0, type=&quot;n&quot;, xlim=m*c(-rmax, rmax), ylim=c(-w, 1+w), axes=FALSE, ann=FALSE) yh &lt;- g(rmax, tau=tau) lines(seq(-rmax, rmax, rmax/100), g(abs(seq(-rmax, rmax, rmax/100)), tau=tau)) draw_ellipse(0, yh, rmax, w, lty=2) lines(-c(rmax, rmax), c(0, yh)) lines(c(rmax, rmax), c(0, yh)) draw_ellipse(0, 0, rmax, w) draw_ellipse(0, 1, rmax, w, border=4) lines(-c(rmax, rmax), c(yh, 1), col=4) lines(c(rmax, rmax), c(yh, 1), col=4) 5.6 Binned distances The cumulative density function for the Half-Normal distribution (\\(\\pi(r) = 1-e^{-(r/\\tau)^2}\\)) is used to calculate cell probabilities for binned distance data (the normalizing constant is the area of the integral \\(\\pi \\tau^2\\), instead of \\(\\pi r_{max}^2\\)). It captures the proportion of the observed distances relative to the whole volume of the observed distance density. In the pie analogy, this is the dough volume inside the cookie cutter, compared to the dough volume inside and outside of the cutter (that happens to be \\(\\pi \\tau^2\\) for the Half-Normal): plot(0, type=&quot;n&quot;, xlim=m*c(-rmax, rmax), ylim=c(-w, 1+w), axes=FALSE, ann=FALSE) yh &lt;- g(rmax, tau=tau) lines(seq(-m*rmax, m*rmax, rmax/(m*100)), g(seq(-m*rmax, m*rmax, rmax/(m*100)), tau=tau), col=2) lines(seq(-rmax, rmax, rmax/100), g(abs(seq(-rmax, rmax, rmax/100)), tau=tau)) draw_ellipse(0, yh, rmax, w, lty=2) lines(-c(rmax, rmax), c(0, yh)) lines(c(rmax, rmax), c(0, yh)) draw_ellipse(0, 0, rmax, w) In case of the Half-Normal distance function, \\(\\tau\\) is the effective detection radius (EDR). The effective detection radius is the distance from observer where the number of individuals missed within EDR (volume of ‘air’ in the cookie cutter above the dough) equals the number of individuals detected outside of EDR (dough volume outside the cookie cutter), EDR is the radius \\(r_e\\) where \\(q(r_e)=\\pi(r_e)\\): plot(0, type=&quot;n&quot;, xlim=m*c(-rmax, rmax), ylim=c(-w, 1+w), axes=FALSE, ann=FALSE) yh &lt;- g(rmax, tau=tau) lines(seq(-m*rmax, m*rmax, rmax/(m*100)), g(seq(-m*rmax, m*rmax, rmax/(m*100)), tau=tau), col=2) lines(seq(-rmax, rmax, rmax/100), g(abs(seq(-rmax, rmax, rmax/100)), tau=tau)) draw_ellipse(0, yh, rmax, w, lty=2) lines(-c(rmax, rmax), c(0, yh)) lines(c(rmax, rmax), c(0, yh)) draw_ellipse(0, 0, rmax, w) draw_ellipse(0, 1, rmax, w, border=4) lines(-c(rmax, rmax), c(yh, 1), col=4) lines(c(rmax, rmax), c(yh, 1), col=4) Exercise What would be a computational algorithm to calculate EDR for any distance function and truncation distance? Trye to explain how the code below is working. Why are EDRs different for different truncation distances? find_edr &lt;- function(dist_fun, ..., rmax=Inf) { ## integral function f &lt;- function(d, ...) dist_fun(d, ...) * 2*d*pi ## volume under dist_fun V &lt;- integrate(f, lower=0, upper=rmax, ...)$value u &lt;- function(edr) V - edr^2*pi uniroot(u, c(0, 1000))$root } find_edr(g, tau=1) ## [1] 1 find_edr(g, tau=10) ## [1] 10 find_edr(g, tau=1, b=1) ## [1] 1.414 find_edr(g, tau=1, b=4, hazard=TRUE) ## [1] 1.331 find_edr(g, tau=1, rmax=1) ## [1] 0.7951 The function \\(\\pi(r)\\) increases monotonically from 0 to 1: curve(1-exp(-(x/tau)^2), xlim=c(0, 5), ylim=c(0,1), col=4, ylab=expression(pi(d)), xlab=expression(d), main=&quot;Cumulative density&quot;) Here are binned distances for the bSims data, with expected proportions based on \\(\\pi()\\) cell probabilities (differences within the distance bins). The nice thing about this cumulative density formulation is that it applies equally to truncated and unlimited (not truncated) distance data, and the radius end point for a bin (stored in br) can be infinite: br &lt;- c(1, 2, 3, 4, 5, Inf) dat$bin &lt;- cut(da, c(0, br), include.lowest = TRUE) (counts &lt;- with(dat, table(bin, detected))) ## detected ## bin 0 1 ## [0,1] 7 27 ## (1,2] 42 53 ## (2,3] 111 32 ## (3,4] 227 13 ## (4,5] 287 2 ## (5,Inf] 211 1 pi_br &lt;- 1-exp(-(br/tau)^2) barplot(counts[,&quot;1&quot;]/sum(counts[,&quot;1&quot;]), space=0, col=NA, xlab=&quot;Distance bins (100 m)&quot;, ylab=&quot;Proportions&quot;) lines(seq_len(length(br))-0.5, diff(c(0, pi_br)), col=3) We can use the bsims_transcribe function for the same effect, and estimate \\(\\hat{\\tau}\\) based on the binned data: (tr &lt;- bsims_transcribe(o, rint=br)) ## bSims transcript ## 1 km x 1 km ## stratification: H ## total abundance: 1013 ## no events, duration: 10 min ## detected: 128 seen/heard ## 1st event detected by bins: ## [0-10 min] ## [0-100, 100-200, 200-300, 300-400, 400-500, 500+ m] tr$removal ## 0-10min ## 0-100m 27 ## 100-200m 53 ## 200-300m 32 ## 300-400m 13 ## 400-500m 2 ## 500+m 1 Y &lt;- matrix(drop(tr$removal), nrow=1) D &lt;- matrix(br, nrow=1) tauhat &lt;- exp(cmulti.fit(Y, D, type=&quot;dis&quot;)$coef) c(true=tau, estimate=tauhat) ## true estimate ## 2.000 2.067 Here are cumulative counts and the true end expected cumulative cell probabilities: plot(stepfun(1:6, c(0, cumsum(counts[,&quot;1&quot;])/sum(counts[,&quot;1&quot;]))), do.points=FALSE, main=&quot;Binned CDF&quot;, ylab=&quot;Cumulative probability&quot;, xlab=&quot;Bin radius end point (100 m)&quot;) curve(1-exp(-(x/tau)^2), col=2, add=TRUE) curve(1-exp(-(x/tauhat)^2), col=4, add=TRUE) legend(&quot;topleft&quot;, bty=&quot;n&quot;, lty=1, col=c(2, 4, 1), legend=c(&quot;True&quot;, &quot;Estimated&quot;, &quot;Empirical&quot;)) 5.7 Availability bias We have ignored availability so far when working with bSims, but can’t continue like that for real data. What this means, is that \\(g(0) &lt; 1\\), so detecting an individual 0 distance from the observer depends on an event (visual or auditory) that would trigger the detection. For example, if a perfecly camouflaged birds sits in silence, detection might be difficult. Movement, or a vocalization can, however, reveal the individual and its location. The phi and tau values are at the high end of plausible values for songbirds. The Density value is exaggerated, but this way we will have enough counts to prove our points using bSims: phi &lt;- 0.5 tau &lt;- 2 Den &lt;- 10 Now we go through the layers of our bSims world: initiating the landscape, populating the landscape by individuals, breath life into the virtual birds and let them sing, put in an observer and let the observation process begin. set.seed(2) l &lt;- bsims_init() a &lt;- bsims_populate(l, density=Den) b &lt;- bsims_animate(a, vocal_rate=phi) o &lt;- bsims_detect(b, tau=tau) Transcription is the process of turning the detections into a table showing new individuals detected by time intervals and distance bands, as defined by the tint and rint arguments, respectively. tint &lt;- c(1, 2, 3, 4, 5) rint &lt;- c(0.5, 1, 1.5, 2) # truncated at 200 m (tr &lt;- bsims_transcribe(o, tint=tint, rint=rint)) ## bSims transcript ## 1 km x 1 km ## stratification: H ## total abundance: 957 ## duration: 10 min ## detected: 282 heard ## 1st event detected by bins: ## [0-1, 1-2, 2-3, 3-4, 4-5 min] ## [0-50, 50-100, 100-150, 150-200 m] (rem &lt;- tr$removal) # binned new individuals ## 0-1min 1-2min 2-3min 3-4min 4-5min ## 0-50m 1 4 0 2 0 ## 50-100m 4 6 2 4 1 ## 100-150m 13 5 6 4 2 ## 150-200m 13 5 2 7 3 colSums(rem) ## 0-1min 1-2min 2-3min 3-4min 4-5min ## 31 20 10 17 6 rowSums(rem) ## 0-50m 50-100m 100-150m 150-200m ## 7 17 30 30 The plot method displays the detections presented as part of the tr object. plot(tr) The detection process and the transcription (following a prescribed protocol) is inseparable in the field. However, recordings made in the field can be processed by a number of different ways. Separating these processed gives the ability to make these conparisons on the exact same set of detections. 5.8 Estimating density with truncation We now fit the removal model to the data pooled by time intervals. p is the cumulative probability of availability for the total duration: fitp &lt;- cmulti.fit(matrix(colSums(rem), 1), matrix(tint, 1), type=&quot;rem&quot;) phihat &lt;- exp(fitp$coef) c(true=phi, estimate=exp(fitp$coef)) ## true estimate ## 0.5000 0.3301 (p &lt;- 1-exp(-max(tint)*phihat)) ## [1] 0.8081 The distance sampling model uses the distance binned counts, and a Half-Normal detection function, q is the cumulative probability of perceptibility within the area of truncation distance rmax: fitq &lt;- cmulti.fit(matrix(rowSums(rem), 1), matrix(rint, 1), type=&quot;dis&quot;) tauhat &lt;- exp(fitq$coef) c(true=tau, estimate=tauhat) ## true estimate ## 2.000 2.659 rmax &lt;- max(rint) (q &lt;- (tauhat^2/rmax^2) * (1-exp(-(rmax/tauhat)^2))) ## [1] 0.7638 The known Area, p, and q makes up the correction factor, which is used to estimate density based on \\(\\hat{D}=Y/(A \\hat{p}\\hat{q})\\): (A &lt;- pi * rmax^2) ## [1] 12.57 Dhat &lt;- sum(rem) / (A * p * q) c(true=Den, estimate=Dhat) ## true estimate ## 10.00 10.83 5.9 Unlimited distance We now change the distance bins to include the area outside of the previous rmax distance, making the counts unlimited distance counts: rint &lt;- c(0.5, 1, 1.5, 2, Inf) # unlimited (tr &lt;- bsims_transcribe(o, tint=tint, rint=rint)) ## bSims transcript ## 1 km x 1 km ## stratification: H ## total abundance: 957 ## duration: 10 min ## detected: 282 heard ## 1st event detected by bins: ## [0-1, 1-2, 2-3, 3-4, 4-5 min] ## [0-50, 50-100, 100-150, 150-200, 200+ m] (rem &lt;- tr$removal) # binned new individuals ## 0-1min 1-2min 2-3min 3-4min 4-5min ## 0-50m 1 4 0 2 0 ## 50-100m 4 6 2 4 1 ## 100-150m 13 5 6 4 2 ## 150-200m 13 5 2 7 3 ## 200+m 23 13 7 3 2 colSums(rem) ## 0-1min 1-2min 2-3min 3-4min 4-5min ## 54 33 17 20 8 rowSums(rem) ## 0-50m 50-100m 100-150m 150-200m 200+m ## 7 17 30 30 48 The removal model is basically the same, the only difference is that the counts can be higher due to detecting over larger area and thus potentially detecting more individuals: fitp &lt;- cmulti.fit(matrix(colSums(rem), 1), matrix(tint, 1), type=&quot;rem&quot;) phihat &lt;- exp(fitp$coef) c(true=phi, estimate=phihat) ## true estimate ## 0.5000 0.4285 (p &lt;- 1-exp(-max(tint)*phihat)) ## [1] 0.8826 The diatance sampling model also takes the extended data set. fitq &lt;- cmulti.fit(matrix(rowSums(rem), 1), matrix(rint, 1), type=&quot;dis&quot;) tauhat &lt;- exp(fitq$coef) c(true=tau, estimate=tauhat) ## true estimate ## 2.000 2.021 The problem is that our truncation distance is infinite, thus the area that we are sampling is also infinite. This does not make too much sense, and not at all hepful in estimating density (anything divided by infinity is 0). So we use EDR (tauhat for Half-Normal) and calculate the estimated effective area sampled (Ahat; \\(\\hat{A}=\\pi \\hat{\\tau}^2\\)). We also set q to be 1, because the logic behind EDR is that its volume equals the volume of the integral, in other words, it is an area that would give on average same count under perfect detection Finally, we estimate density using \\(\\hat{D}=Y/(\\hat{A} \\hat{p}1)\\) (Ahat &lt;- pi * tauhat^2) ## [1] 12.83 q &lt;- 1 Dhat &lt;- sum(rem) / (Ahat * p * q) c(true=Den, estimate=Dhat) ## true estimate ## 10.00 11.66 5.10 Replicating landscapes Remember, that we have used so far a single location. We set the density unreasonably high to have enough counts for a reasonable estimate. We can independently replicate the simulation for multiple landscapes and analyze the results to give justice to bSims under idealized conditions: phi &lt;- 0.5 tau &lt;- 1 Den &lt;- 1 tint &lt;- c(3, 5, 10) rint &lt;- c(0.5, 1, 1.5, Inf) sim_fun &lt;- function() { l &lt;- bsims_init() a &lt;- bsims_populate(l, density=Den) b &lt;- bsims_animate(a, vocal_rate=phi) o &lt;- bsims_detect(b, tau=tau) bsims_transcribe(o, tint=tint, rint=rint)$rem } B &lt;- 200 set.seed(123) res &lt;- pbapply::pbreplicate(B, sim_fun(), simplify=FALSE) Ddur &lt;- matrix(tint, B, length(tint), byrow=TRUE) Ydur1 &lt;- t(sapply(res, function(z) colSums(z))) Ydur2 &lt;- t(sapply(res, function(z) colSums(z[-nrow(z),]))) colSums(Ydur1) / sum(Ydur1) colSums(Ydur2) / sum(Ydur2) fitp1 &lt;- cmulti(Ydur1 | Ddur ~ 1, type=&quot;rem&quot;) fitp2 &lt;- cmulti(Ydur2 | Ddur ~ 1, type=&quot;rem&quot;) phihat1 &lt;- unname(exp(coef(fitp1))) phihat2 &lt;- unname(exp(coef(fitp2))) Ddis1 &lt;- matrix(rint, B, length(rint), byrow=TRUE) Ddis2 &lt;- matrix(rint[-length(rint)], B, length(rint)-1, byrow=TRUE) Ydis1 &lt;- t(sapply(res, function(z) rowSums(z))) Ydis2 &lt;- t(sapply(res, function(z) rowSums(z)[-length(rint)])) colSums(Ydis1) / sum(Ydis1) colSums(Ydis2) / sum(Ydis2) fitq1 &lt;- cmulti(Ydis1 | Ddis1 ~ 1, type=&quot;dis&quot;) fitq2 &lt;- cmulti(Ydis2 | Ddis2 ~ 1, type=&quot;dis&quot;) tauhat1 &lt;- unname(exp(fitq1$coef)) tauhat2 &lt;- unname(exp(fitq2$coef)) ## unlimited correction Apq1 &lt;- pi * tauhat1^2 * (1-exp(-max(tint)*phihat1)) * 1 rmax &lt;- max(rint[is.finite(rint)]) ## truncated correction Apq2 &lt;- pi * rmax^2 * (1-exp(-max(tint)*phihat2)) * (tauhat2^2/rmax^2) * (1-exp(-(rmax/tauhat2)^2)) round(rbind( phi=c(true=phi, unlimited=phihat1, truncated=phihat2), tau=c(true=tau, unlimited=tauhat1, truncated=tauhat2), D=c(Den, unlimited=mean(rowSums(Ydis1))/Apq1, truncated=mean(rowSums(Ydis2))/Apq2)), 4) ## true unlimited truncated ## phi 0.5 0.4835 0.4852 ## tau 1.0 1.0002 0.9681 ## D 1.0 1.0601 1.1048 Exercise If time permits, try different settings and time/distance intervals. 5.11 JOSM data Quickly organize the JOSM data: ## predictors x &lt;- josm$surveys x$FOR &lt;- x$Decid + x$Conif+ x$ConifWet # forest x$AHF &lt;- x$Agr + x$UrbInd + x$Roads # &#39;alienating&#39; human footprint x$WET &lt;- x$OpenWet + x$ConifWet + x$Water # wet + water cn &lt;- c(&quot;Open&quot;, &quot;Water&quot;, &quot;Agr&quot;, &quot;UrbInd&quot;, &quot;SoftLin&quot;, &quot;Roads&quot;, &quot;Decid&quot;, &quot;OpenWet&quot;, &quot;Conif&quot;, &quot;ConifWet&quot;) x$HAB &lt;- droplevels(find_max(x[,cn])$index) # drop empty levels levels(x$HAB)[levels(x$HAB) %in% c(&quot;OpenWet&quot;, &quot;Water&quot;, &quot;Open&quot;, &quot;Agr&quot;, &quot;UrbInd&quot;, &quot;Roads&quot;)] &lt;- &quot;Open&quot; levels(x$HAB)[levels(x$HAB) %in% c(&quot;Conif&quot;, &quot;ConifWet&quot;)] &lt;- &quot;Conif&quot; x$OBS &lt;- as.factor(x$ObserverID) ## time intervals yall_dur &lt;- Xtab(~ SiteID + Dur + SpeciesID, josm$counts[josm$counts$DetectType1 != &quot;V&quot;,]) yall_dur &lt;- yall_dur[sapply(yall_dur, function(z) sum(rowSums(z) &gt; 0)) &gt; 100] ## distance intervals yall_dis &lt;- Xtab(~ SiteID + Dis + SpeciesID, josm$counts[josm$counts$DetectType1 != &quot;V&quot;,]) yall_dis &lt;- yall_dis[sapply(yall_dis, function(z) sum(rowSums(z) &gt; 0)) &gt; 100] Pick our most abundant species again, and organize the data: spp &lt;- &quot;TEWA&quot; Ydur &lt;- as.matrix(yall_dur[[spp]]) Ddur &lt;- matrix(c(3, 5, 10), nrow(Ydur), 3, byrow=TRUE, dimnames=dimnames(Ydur)) stopifnot(all(rownames(x) == rownames(Ydur))) Ydis &lt;- as.matrix(yall_dis[[spp]]) Ddis &lt;- matrix(c(0.5, 1, Inf), nrow(Ydis), 3, byrow=TRUE, dimnames=dimnames(Ydis)) stopifnot(all(rownames(x) == rownames(Ydis))) colSums(Ydur) ## 0-3min 3-5min 5-10min ## 4262 558 764 colSums(Ydis) ## 0-50m 50-100m 100+m ## 2703 2470 411 We pick a removal models with DAY as covariate, and calculate \\(p(t)\\): Mdur &lt;- cmulti(Ydur | Ddur ~ DAY, x, type=&quot;rem&quot;) summary(Mdur) ## ## Call: ## cmulti(formula = Ydur | Ddur ~ DAY, data = x, type = &quot;rem&quot;) ## ## Removal Sampling (homogeneous singing rate) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.phi_(Intercept) 0.0784 0.2615 0.30 0.76427 ## log.phi_DAY -2.0910 0.5866 -3.56 0.00036 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -3.2e+03 ## BIC = 6.41e+03 phi &lt;- exp(model.matrix(Mdur) %*% coef(Mdur)) summary(phi) ## V1 ## Min. :0.377 ## 1st Qu.:0.402 ## Median :0.420 ## Mean :0.423 ## 3rd Qu.:0.448 ## Max. :0.477 p &lt;- 1-exp(-10*phi) We fit the intercept only distance sampling model next: Mdis0 &lt;- cmulti(Ydis | Ddis ~ 1, x, type=&quot;dis&quot;) summary(Mdis0) ## ## Call: ## cmulti(formula = Ydis | Ddis ~ 1, data = x, type = &quot;dis&quot;) ## ## Distance Sampling (half-normal, circular area) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.tau_(Intercept) -0.48272 0.00753 -64.1 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -3.82e+03 ## BIC = 7.65e+03 Let’s try a few covariates: continuous FORest cover covariate: sound attenuation increases with forest cover; discrete HABitat has 3 levels: open, deciduous forest, and coniferous forest (based on dominant land cover), because broad leaves and needles affect sound attenuation; finally, we use observer ID as categorical variable: observers might have different hearing abilities, training/experiance levels, good times, bad times, etc. Mdis1 &lt;- cmulti(Ydis | Ddis ~ FOR, x, type=&quot;dis&quot;) Mdis2 &lt;- cmulti(Ydis | Ddis ~ HAB, x, type=&quot;dis&quot;) We can look at AIC to find the best supported model: aic &lt;- AIC(Mdis0, Mdis1, Mdis2) aic$delta_AIC &lt;- aic$AIC - min(aic$AIC) aic[order(aic$AIC),] Mdis &lt;- get(rownames(aic)[aic$delta_AIC == 0]) summary(Mdis) ## ## Call: ## cmulti(formula = Ydis | Ddis ~ HAB, data = x, type = &quot;dis&quot;) ## ## Distance Sampling (half-normal, circular area) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.tau_(Intercept) -0.4497 0.0321 -14.02 &lt;2e-16 *** ## log.tau_HABDecid -0.0583 0.0336 -1.73 0.083 . ## log.tau_HABConif -0.0030 0.0343 -0.09 0.930 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -3.82e+03 ## BIC = 7.66e+03 Exercise Use OBS as predictor for tau and look at predicted EDRs. What is the practical issue with using observer as predictor? After finding the best model, we predict tau: tau &lt;- exp(model.matrix(Mdis) %*% coef(Mdis)) boxplot(tau ~ HAB, x) Finally, we calculate the correction factor for unlimited distances, and predict mean density: Apq &lt;- pi * tau^2 * p * 1 x$ytot &lt;- rowSums(Ydur) mean(x$ytot / Apq) ## [1] 1.039 Alternatively, we can use the log of the correction as an offset in log-linear models. This offset is called the QPAD offset: off &lt;- log(Apq) m &lt;- glm(ytot ~ 1, data=x, offset=off, family=poisson) exp(coef(m)) ## (Intercept) ## 1.025 Exercise Try distance sampling and density estimation for another species. Fit multiple GLMs with QPAD offsets and covariates affecting density, interpret the results and the visualize responses. Sometimes, a recording is made at the survey location that is listened to and transcribed in the lab using headphones and possibly a computer screen. This presents new challenges, and also new opportunities for analysis of count data – and is the topic of the next chapter. "],
["recordings.html", "Chapter 6 Dealing with Recordings 6.1 Introduction 6.2 Prerequisites 6.3 Paired sampling 6.4 Paired data 6.5 Availability 6.6 Distance sampling 6.7 Scaling constant 6.8 Data integration 6.9 ABMI data", " Chapter 6 Dealing with Recordings 6.1 Introduction Automated recording units (ARU) are increasingly being used for auditory surveys. There are numerous advantages for using ARUs, e.g. recordings can be stored in perpetuity to be transcribed later, ARUs can be programmed to record at select times and dates over long time periods that would be prohibitive using human observers. Bird point counts have been traditionally done by human observers. Combining ARU data with traditional point counts thus require an understanding of how the ARU based counts relate to counts made by human observer in the field. The best way to approach this question is by simultaneously sampling by two approaches: (1) human observers doing traditional point count by registering time and distance interval an individual bird was first detected, and (2) record the same session at the same location by an ARU to be identified/transcribed later in laboratory settings. 6.2 Prerequisites library(bSims) # simulations library(detect) # multinomial models library(mefa4) # count manipulation library(survival) # survival models library(paired) # paired sampling data load(&quot;_data/abmi/abmi.rda&quot;) # ABMI ARU data set 6.3 Paired sampling The expected value of the total count (single species) in a 10-minutes time interval using human observer (subscript \\(H\\)) based unlimited radius point count may be written as: \\(E[Y_{H}] = D A_{H} p_{H}\\) where \\(Y_{H}\\) is the count, \\(D\\) is population density, \\(A\\) is the area sampled, \\(p_{H}\\) is the probability that an average individual of the species is available for detection. The quantity \\(p_{H}\\) can be estimated based on removal sampling utilizing the multiple time intervals. \\(A_{H}\\) is often unknown, but can be estimated using the effective detection radius: \\(A_{H}=\\pi EDR_{H}^2\\). Human observer based EDR is estimated from distance sampling. The ARU based survey (subscript \\(R\\) for recorder) can distinguish individuals within distinct time intervals, but assigning these individuals is not yet possible using a single ARU. An ARU based count thus can be seen ans an unlimited radius point count where the effective area sampled is unknown. The expected value for an ARU based count for a given species may be written as: \\(E[Y_{R}] = D A_{R} p_{R}\\). \\(p_{R}\\) can be estimated based on removal sampling utilizing the multiple time intervals from the ARU based survey. The unknown sampling are can be written as \\(A_{R}=\\pi EDR_{R}^2\\). The problem is that ARU based EDR cannot directly be estimated from the data because of the lack of multiple distance bands or individual based distance information. The advantage of simultaneous sampling by human observers (H) and ARUs (A) is that population density (\\(D=D_{H}=D_{R}\\)) is identical by design. Possible mechanisms for differences in availability of bird individuals for detection (\\(p_{H}\\) vs. \\(p_{R}\\)) can include differences in how detections are made in the field vs. in laboratory (e.g. possibility of double checking). Both \\(p_{H}\\) and \\(p_{R}\\) can be estimated from the data, and the equivalence \\(p=p_{H}=p_{R}\\) can be tested. So for the sake of simplicity, we assume that human observer and ARU based \\(p\\)’s are equal. Dividing the expected values of the counts may be written as: \\[\\frac{E[Y_{R}]}{E[Y_{H}]} = \\frac{D A_{R} p}{D A_{R} p} = \\frac{\\pi EDR_{R}^2}{\\pi EDR_{H}^2} = \\frac{EDR_{R}^2}{EDR_{H}^2}\\] By substituting \\(EDR_{R}^2 = \\Delta^2 EDR_{H}^2\\) (and thus \\(EDR_{R} = \\Delta EDR_{H}\\)) we get: \\[\\frac{E[Y_{R}]}{E[Y_{H}]} = \\frac{\\Delta^2 EDR_{H}^2}{EDR_{H}^2} = \\Delta^2\\] This means that dividing the mean counts from ARU and human observed counts would give an estimate of the squared scaling constant (\\(\\Delta^2\\)) describing the relationship between the estimated \\(EDR_{H}\\) and the unknown \\(EDR_{R}\\). 6.4 Paired data Human observer surveys: 0-50, 50-100, &gt;100 m distance bands, 0-3, 3-5, 5-10 minutes time intervals. ARU surveys: unlimited distance, 10 minutes survey in 1-minute time intervals. paired$DISTANCE[paired$SurveyType == &quot;ARU&quot;] &lt;- &quot;ARU&quot; with(paired, ftable(SurveyType, Interval, DISTANCE)) ## DISTANCE &gt;100 m 0-49 m 50-100 m ARU ## SurveyType Interval ## ARU 0-3 min 0 0 0 3349 ## 3-5 min 0 0 0 675 ## 5-10 min 0 0 0 1296 ## UNK 0 0 0 3 ## HUM 0-3 min 877 1270 1387 0 ## 3-5 min 244 252 393 0 ## 5-10 min 481 429 645 0 ## UNK 7 20 12 0 Select a subset of species that we’ll work with: xt &lt;- as.matrix(Xtab(Count ~ PKEY + SPECIES, data=paired[paired$SurveyType == &quot;HUM&quot;,])) SPP &lt;- colnames(xt) ## number of &gt;0 counts ndis &lt;- colSums(xt &gt; 0) ## max count maxd &lt;- apply(xt, 2, max) nmin &lt;- 15 SPP &lt;- SPP[ndis &gt;= nmin &amp; maxd &gt; 1] SPP &lt;- SPP[!(SPP %in% c(&quot;CANG&quot;,&quot;COLO&quot;,&quot;COGO&quot;,&quot;COME&quot;,&quot;FRGU&quot;,&quot;BCFR&quot;,&quot;UNKN&quot;,&quot;RESQ&quot;, &quot;CORA&quot;,&quot;AMCR&quot;,&quot;WOSP&quot;,&quot;WWCR&quot;,&quot;PISI&quot;,&quot;EVGR&quot;, &quot;RUGR&quot;, &quot;SACR&quot;, &quot;NOFL&quot;))] SPP ## [1] &quot;ALFL&quot; &quot;AMRE&quot; &quot;AMRO&quot; &quot;BBWA&quot; &quot;BCCH&quot; &quot;BLBW&quot; &quot;BLJA&quot; &quot;BRCR&quot; &quot;CAWA&quot; &quot;CCSP&quot; ## [11] &quot;CEDW&quot; &quot;CHSP&quot; &quot;CMWA&quot; &quot;CONW&quot; &quot;COYE&quot; &quot;CSWA&quot; &quot;DEJU&quot; &quot;FOSP&quot; &quot;GRAJ&quot; &quot;GRYE&quot; ## [21] &quot;HAWO&quot; &quot;HETH&quot; &quot;LEFL&quot; &quot;LISP&quot; &quot;MAWA&quot; &quot;MOWA&quot; &quot;MYWA&quot; &quot;NAWA&quot; &quot;OCWA&quot; &quot;OVEN&quot; ## [31] &quot;PAWA&quot; &quot;PHVI&quot; &quot;PIWO&quot; &quot;RBGR&quot; &quot;RBNU&quot; &quot;RCKI&quot; &quot;REVI&quot; &quot;SOSP&quot; &quot;SWTH&quot; &quot;TEWA&quot; ## [41] &quot;WISN&quot; &quot;WIWR&quot; &quot;WTSP&quot; &quot;YBSA&quot; &quot;YRWA&quot; 6.5 Availability We estimated availability for human observer and ARU based counts using the time interval information. ARU based intervals were collapsed to the 0-3-5-10 minutes intervals to match the human observer based design. xtdurH &lt;- Xtab(Count ~ PKEY + Interval + SPECIES, paired[paired$SurveyType == &quot;HUM&quot;,]) xtdurH &lt;- xtdurH[SPP] xtdurR &lt;- Xtab(Count ~ PKEY + Interval + SPECIES, paired[paired$SurveyType == &quot;ARU&quot;,]) xtdurR &lt;- xtdurR[SPP] Ddur &lt;- matrix(c(3, 5, 10), nrow(xtdurH[[1]]), 3, byrow=TRUE) Ddur2 &lt;- rbind(Ddur, Ddur) xdur &lt;- nonDuplicated(paired, PKEY, TRUE) xx &lt;- xdur[rownames(xtdurR[[1]]),] We estimated availability for species with at least 15 detections in both subsets of the data (making sure that the total count for at least some locations exceeded 1). We analyzed the human observer and ARU based data in a single model using survey type as a dummy variable. We tested if the estimate corresponding to survey type differed significantly from 0 using 95% confidence intervals. The following table lists singing rates (phi 1/minute), probability of singing in a 10-minutes interval (p10), number of detections (n), and whether or not the confidence limits for the survey type estimate (\\(\\beta_1\\)) contained 0 (i.e. not significant survey effect). mdurR &lt;- list() mdurH &lt;- list() mdurHR &lt;- list() mdurHR1 &lt;- list() for (spp in SPP) { yR &lt;- as.matrix(xtdurR[[spp]])[,c(&quot;0-3 min&quot;,&quot;3-5 min&quot;,&quot;5-10 min&quot;)] yH &lt;- as.matrix(xtdurH[[spp]])[,c(&quot;0-3 min&quot;,&quot;3-5 min&quot;,&quot;5-10 min&quot;)] yHR &lt;- rbind(yH, yR) mdurR[[spp]] &lt;- cmulti(yR | Ddur ~ 1, type = &quot;rem&quot;) mdurH[[spp]] &lt;- cmulti(yH | Ddur ~ 1, type = &quot;rem&quot;) aru01 &lt;- rep(0:1, each=nrow(yH)) mdurHR[[spp]] &lt;- cmulti(yHR | Ddur2 ~ 1, type = &quot;rem&quot;) mdurHR1[[spp]] &lt;- cmulti(yHR | Ddur2 ~ aru01, type = &quot;rem&quot;) } cfR &lt;- sapply(mdurR, coef) cfH &lt;- sapply(mdurH, coef) cfHR &lt;- sapply(mdurHR, coef) cfHR1 &lt;- t(sapply(mdurHR1, coef)) names(cfR) &lt;- names(cfH) &lt;- names(cfHR) &lt;- names(cfHR1) &lt;- SPP phiR &lt;- exp(cfR) phiH &lt;- exp(cfH) phiHR &lt;- exp(cfHR) ## confidence interval for survey type effect ci &lt;- t(sapply(mdurHR1, function(z) confint(z)[2,])) ## does CI contain 0? table(0 %[]% ci) ## ## FALSE TRUE ## 4 41 plot(phiR ~ phiH, ylim=c(0, max(phiH, phiR)), xlim=c(0, max(phiH, phiR)), pch=c(21, 19)[(0 %[]% ci) + 1], xlab=expression(phi[H]), ylab=expression(phi[R]), cex=0.5+2*phiHR) abline(0,1) Exercise Which \\(\\phi\\) estimate should we use? Can we use phiHR? Isn’t that cheating to double the sample size? Think about what we are conditioning on when estimating \\(\\phi\\), and what makes samples independent. 6.6 Distance sampling We estimate EDR from human observer based counts: ## Data for EDR estimation xtdis &lt;- Xtab(Count ~ PKEY + DISTANCE + SPECIES, data=paired[paired$SurveyType == &quot;HUM&quot;,]) xtdis &lt;- xtdis[SPP] for (i in seq_len(length(xtdis))) xtdis[[i]] &lt;- as.matrix(xtdis[[i]][,c(&quot;0-49 m&quot;, &quot;50-100 m&quot;, &quot;&gt;100 m&quot;)]) head(xtdis$YRWA) ## 0-49 m 50-100 m &gt;100 m ## 05-041-01_1 0 0 0 ## 05-041-02_1 0 0 0 ## 05-041-05_1 0 0 0 ## 05-041-06_1 0 0 0 ## 05-041-07_1 0 1 0 ## 05-041-08_1 0 0 0 ## distance radii Ddis &lt;- matrix(c(0.5, 1, Inf), nrow(xtdis[[1]]), 3, byrow=TRUE) head(Ddis) ## [,1] [,2] [,3] ## [1,] 0.5 1 Inf ## [2,] 0.5 1 Inf ## [3,] 0.5 1 Inf ## [4,] 0.5 1 Inf ## [5,] 0.5 1 Inf ## [6,] 0.5 1 Inf ## predictors xdis &lt;- nonDuplicated(paired, PKEY, TRUE) xdis &lt;- xdis[rownames(xtdis[[1]]),] Fitting distance sampling models for each species: mdis &lt;- pblapply(xtdis, function(Y) { cmulti(Y | Ddis ~ 1, xdis, type = &quot;dis&quot;) }) tauH &lt;- sapply(mdis, function(z) unname(exp(coef(z)))) edrH &lt;- 100 * tauH round(sort(edrH)) ## CEDW CMWA BBWA BLBW BRCR NAWA LEFL AMRE HAWO PHVI YRWA MAWA CSWA BCCH OCWA ## 34 36 39 41 42 45 46 51 52 53 53 54 55 56 59 ## CAWA COYE TEWA MOWA PAWA YBSA LISP MYWA CCSP DEJU CHSP CONW OVEN REVI GRAJ ## 61 62 63 64 65 67 69 69 70 70 73 74 77 81 82 ## RBGR RCKI RBNU SWTH AMRO WTSP WIWR ALFL SOSP BLJA FOSP HETH GRYE WISN PIWO ## 85 86 88 91 92 99 101 102 114 129 130 138 158 164 167 hist(edrH) 6.7 Scaling constant Counts are often modeled in a log-linear Poisson GLM. We used GLM to estimate the unknown scaling constant from simultaneous (paired) surveys. The Poisson mean for a count made at site \\(i\\) by human observer is \\(\\lambda_{i,H} = D_{i} \\pi EDR_H^2 p\\). \\(EDR_H\\) and \\(p\\) are estimated using distance sampling and removal sampling, respectively. Those estimates are used to calculate a correction factor \\(C = \\pi EDR_H^2 p\\) which is used as an offset on the log scale as \\(log(\\lambda_{i,H}) = log(D_{i}) + log(C) = \\beta_0 + log(C)\\), where \\(\\beta_0\\) is the intercept in the GLM model. Following the arguments above, the Poisson mean for an ARU based count made at site \\(i\\) is \\(\\lambda_{i,R} = D_{i} \\pi \\Delta^2 EDR_H^2 p = D_{i} \\Delta^2 C\\). On the log scale, this becomes \\(log(\\lambda_{i,R}) = log(D_{i}) + log(\\Delta^2) + log(C) = \\beta_0 + \\beta_1 + log(C)\\), where \\(\\beta_1\\) is a contrast for ARU type surveys in the log-linear model. We used survey type as a binary variable (\\(x_i\\)) with value 0 for human observers and value 1 for ARUs. So the Poisson model is generalized as: \\(log(\\lambda_{i}) = \\beta_0 + x_i \\beta_1 + log(C)\\). \\(\\Delta\\) can be calculated from \\(\\beta_1\\) as \\(\\Delta = \\sqrt{e^{\\beta_i}}\\). We used the Poisson GLM model describe before to estimate the \\(\\beta_1\\) coefficient corresponding to survey type as binary predictor variable, and an offset term incorporating human observer based effective area sampled and availability. phi &lt;- phiHR tau &lt;- tauH Y &lt;- as.matrix(Xtab(Count ~ PKEYm + SPECIES, paired)) X &lt;- nonDuplicated(paired, PKEYm, TRUE) X &lt;- X[rownames(Y),] X$distur &lt;- ifelse(X$Disturbance != &quot;Undisturbed&quot;, 1, 0) X$SurveyType &lt;- relevel(X$SurveyType, &quot;HUM&quot;) library(lme4) mods &lt;- list() aictab &lt;- list() Delta &lt;- matrix(NA, length(SPP), 3) dimnames(Delta) &lt;- list(SPP, c(&quot;est&quot;, &quot;lcl&quot;, &quot;ucl&quot;)) #spp &lt;- &quot;ALFL&quot; for (spp in SPP) { y &lt;- Y[,spp] C &lt;- tau[spp]^2 * pi * (1-exp(-phi[spp])) off &lt;- rep(log(C), nrow(X)) mod0 &lt;- glm(y ~ 1, X, offset=off, family=poisson) mod1 &lt;- glm(y ~ SurveyType, X, offset=off, family=poisson) mod2 &lt;- glm(y ~ SurveyType + distur, X, offset=off, family=poisson) aic &lt;- AIC(mod0, mod1, mod2) aic$delta_AIC &lt;- aic$AIC - min(aic$AIC) aictab[[spp]] &lt;- aic Best &lt;- get(rownames(aic)[aic$delta_AIC == 0]) #summary(Best) mods[[spp]] &lt;- Best ## this is Monte Carlo based CI, no need for Delta method bb &lt;- MASS::mvrnorm(10^4, coef(mod1), vcov(mod1)) Delta[spp,] &lt;- c(sqrt(exp(coef(mod1)[&quot;SurveyTypeARU&quot;])), quantile(sqrt(exp(bb[,&quot;SurveyTypeARU&quot;])), c(0.025, 0.975))) } aic_support &lt;- t(sapply(aictab, function(z) z[,3])) round(aic_support) ## [,1] [,2] [,3] ## ALFL 247 244 0 ## AMRE 65 66 0 ## AMRO 26 21 0 ## BBWA 22 23 0 ## BCCH 28 29 0 ## BLBW 16 18 0 ## BLJA 5 7 0 ## BRCR 27 29 0 ## CAWA 18 20 0 ## CCSP 68 68 0 ## CEDW 9 11 0 ## CHSP 6 5 0 ## CMWA 23 25 0 ## CONW 20 21 0 ## COYE 14 16 0 ## CSWA 22 23 0 ## DEJU 102 99 0 ## FOSP 97 98 0 ## GRAJ 0 0 0 ## GRYE 6 8 0 ## HAWO 0 1 1 ## HETH 19 17 0 ## LEFL 0 2 4 ## LISP 50 52 0 ## MAWA 0 1 0 ## MOWA 0 2 3 ## MYWA 34 36 0 ## NAWA 0 1 1 ## OCWA 132 133 0 ## OVEN 378 375 0 ## PAWA 126 128 0 ## PHVI 0 2 3 ## PIWO 8 10 0 ## RBGR 0 2 1 ## RBNU 37 38 0 ## RCKI 13 9 0 ## REVI 23 21 0 ## SOSP 65 66 0 ## SWTH 37 39 0 ## TEWA 22 15 0 ## WISN 1 3 0 ## WIWR 35 35 0 ## WTSP 167 168 0 ## YBSA 19 18 0 ## YRWA 26 10 0 The following table show the estimate of \\(\\Delta\\) for each species, and the corresponding estimates of effective detection radius (EDR) in meters and effective area sampled (\\(A\\)) in ha: But wait, if we started from expected values, shouldn’t ratio of the mean counts give us \\(\\Delta^2\\)? Let’s see if we can get a similar \\(\\Delta\\) value from mean counts: (gm &lt;- groupMeans(Y[,SPP], 1, X$SurveyType)) ## ALFL AMRE AMRO BBWA BCCH BLBW BLJA BRCR CAWA ## ARU 0.2152 0.2200 0.1418 0.1809 0.0978 0.1149 0.1271 0.1663 0.06846 ## HUM 0.2910 0.2029 0.2200 0.2054 0.1198 0.1271 0.1222 0.1516 0.06846 ## CCSP CEDW CHSP CMWA CONW COYE CSWA DEJU FOSP ## ARU 0.04401 0.1711 0.4890 0.07335 0.07824 0.05623 0.2127 0.2274 0.06601 ## HUM 0.07090 0.1711 0.5844 0.06357 0.10024 0.06112 0.2421 0.3154 0.05379 ## GRAJ GRYE HAWO HETH LEFL LISP MAWA MOWA MYWA ## ARU 0.2249 0.08313 0.04401 0.5428 0.08802 0.07824 0.4132 0.2494 0.3252 ## HUM 0.2714 0.09535 0.06357 0.6479 0.09780 0.07579 0.4597 0.2616 0.3276 ## NAWA OCWA OVEN PAWA PHVI PIWO RBGR RBNU RCKI REVI ## ARU 0.1222 0.110 1.127 0.1345 0.07579 0.1516 0.10758 0.1540 0.2958 0.9291 ## HUM 0.1002 0.132 1.306 0.1345 0.07579 0.1345 0.09535 0.1785 0.3961 1.0636 ## SOSP SWTH TEWA WISN WIWR WTSP YBSA YRWA ## ARU 0.05623 0.5281 0.7433 0.1076 0.3081 1.560 0.1418 0.05623 ## HUM 0.06846 0.5403 0.9389 0.1002 0.3667 1.643 0.1883 0.15159 Delta_summary$Delta.emp &lt;- sqrt(gm[&quot;ARU&quot;,] / gm[&quot;HUM&quot;,]) plot(Delta.est ~ Delta.emp, Delta_summary, col=c(2,1)[(1 %[]% Delta[,-1]) + 1]) abline(0, 1) abline(h=1, v=1, lty=2) It looks like the fancy modeling was all for nothing, theory prevailed. But it is always nice when things work out as expected. We can also see that \\(\\Delta\\) (especially the significant ones) tended to be less than 1, indicating that overall EDR for ARUs is slightly smaller that for human point counts. \\(\\Delta\\) was significantly different from 1 only for relatively few species. Exercise Can we pool all species’ data together to estimate an overall \\(\\Delta\\) value? Would that characterize this particular ARU type well enough? What are some of the arguments against this pooling? What might be driving the variation across species? 6.8 Data integration Now we will pretend that we have no paired design. See how well fixed effects can handle the data integration without calibration. i &lt;- sample(levels(X$PKEY), floor(nlevels(X$PKEY)/2)) ss &lt;- c(which(X$PKEY %in% i), which(!(X$PKEY %in% i))) mods2 &lt;- list() for (spp in SPP) { y &lt;- Y[ss,spp] C &lt;- tau[spp]^2 * pi * (1-exp(-phi[spp])) off &lt;- rep(log(C), length(ss)) mod &lt;- glm(y ~ SurveyType, X[ss,], offset=off, family=poisson) mods2[[spp]] &lt;- mod } Delta_summary$Delta.fix &lt;- sapply(mods2, function(z) { sqrt(exp(coef(z)[2])) }) plot(Delta.fix ~ Delta.emp, Delta_summary) abline(0, 1) abline(h=1, v=1, lty=2) Exercise Use the script below to push the fixed effects method to the limit and see where it fails. We will explore the following two situations: (1) sample size and number of detections is small, (2) sampling is biased with respect to habitat strata. X$open &lt;- ifelse(X$Class_Name %in% c(&quot;Open Herb/Grass&quot;, &quot;Open coniferous&quot;,&quot;Open Mature Deciduous&quot;,&quot;Open Mixed&quot;, &quot;Open Northern&quot;,&quot;Open Young Deciduous&quot;, &quot;Open Young Mixed&quot;,&quot;Poorly Drained&quot;), 1, 0) ## proportion of samples from ARUs (original) prop_aru &lt;- 0.5 ## proportion of ARU samples coming from open habitats prop_open &lt;- 0.6 n_aru &lt;- round(nrow(X) * prop_aru) n_hum &lt;- nrow(X) - n_aru w_aru &lt;- prop_open*X$open + (1-prop_open)*(1-X$open) w_hum &lt;- (1-prop_open)*X$open + prop_open*(1-X$open) id_aru &lt;- sample(which(X$SurveyType == &quot;ARU&quot;), n_aru, replace=TRUE, prob=w_aru[X$SurveyType == &quot;ARU&quot;]) id_hum &lt;- sample(which(X$SurveyType == &quot;HUM&quot;), n_hum, replace=TRUE, prob=w_hum[X$SurveyType == &quot;HUM&quot;]) ss &lt;- c(id_aru, id_hum) addmargins(with(X[ss,], table(open, SurveyType))) mods3 &lt;- list() for (spp in SPP) { y &lt;- Y[ss,spp] C &lt;- tau[spp]^2 * pi * (1-exp(-phi[spp])) off &lt;- rep(log(C), length(ss)) mod &lt;- glm(y ~ SurveyType, X[ss,], offset=off, family=poisson) mods3[[spp]] &lt;- mod } Est &lt;- sapply(mods3, function(z) sqrt(exp(coef(z)[2]))) plot(Est ~ Delta.emp, Delta_summary) abline(0, 1) abline(h=1, v=1, lty=2) abline(lm(Est ~ Delta.emp, Delta_summary), col=2) 6.9 ABMI data ABMI ARU data is a subset of cca. 1000 recordings done at each survey station over the course of about 5 months (deployed in early March, retrieved in an of July). Recordings are of 1 or 3 minutes in length. Early season and midnight recordings are 1-min long, breeding season morning surveys are 3-min long to better capture the dawn chorus. ## we&#39;ll use this for predictions q &lt;- c(0.025, 0.975) qq &lt;- quantile(abmi$ToY, q) ToY &lt;- seq(qq[1], qq[2], 1) qq3 &lt;- quantile(abmi$ToY[abmi$Duration == 3], q) tpred &lt;- 1 # duration to use for prediction tmp &lt;- nonDuplicated(abmi, visit) plot(ToD ~ ToY, tmp, col=ifelse(tmp$Duration == 3, 2, 1), pch=19, cex=0.5) We will compare results based on different subsets of the data. First, we’ll compile the 3-min surveys (breeding season, morning) and fit removal models. Make survey x interval tables for species based on 3-min samples: need to drop visits that are less than 3-mins: y3 &lt;- Xtab(~ visit + det1 + SpeciesID, data=abmi[abmi$Duration == 3 &amp; !(abmi$SpeciesID %in% c(&quot;NONE&quot;,&quot;SNI&quot;, &quot;VNA&quot;, &quot;DNC&quot;, &quot;PNA&quot;)),], drop.unused.levels=TRUE) D3 &lt;- matrix(1:3, nrow(y3[[1]]), 3, byrow=TRUE) x3 &lt;- droplevels(nonDuplicated(abmi, visit, TRUE)[rownames(y3[[1]]), c(&quot;pkey&quot;, &quot;visit&quot;, &quot;ToY&quot;, &quot;ToYc&quot;, &quot;ToD&quot;, &quot;ToDx&quot;, &quot;ToDc&quot;)]) Pick a species and fit removal models: spp &lt;- &quot;Ovenbird&quot; # mirgratory #spp &lt;- &quot;BorealChickadee&quot; # winter resident ## use morning samples only Y &lt;- as.matrix(y3[[spp]]) Y &lt;- Y[x3$ToDc == &quot;Morning&quot;,] D &lt;- D3[x3$ToDc == &quot;Morning&quot;,] ## fit a conventional model with polynomial ToY effect z &lt;- cmulti(Y | D ~ ToY + I(ToY^2) + I(ToY^3), x3[x3$ToDc == &quot;Morning&quot;,], type=&quot;rem&quot;) summary(z) ## ## Call: ## cmulti(formula = Y | D ~ ToY + I(ToY^2) + I(ToY^3), data = x3[x3$ToDc == ## &quot;Morning&quot;, ], type = &quot;rem&quot;) ## ## Removal Sampling (homogeneous singing rate) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.phi_(Intercept) 4.78e-01 8.42e-51 5.68e+49 &lt;2e-16 *** ## log.phi_ToY 3.20e-04 8.42e-51 3.80e+46 &lt;2e-16 *** ## log.phi_I(ToY^2) 2.70e-05 8.42e-55 3.21e+49 &lt;2e-16 *** ## log.phi_I(ToY^3) -1.83e-07 8.42e-55 -2.17e+47 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -873 ## BIC = 1.77e+03 X &lt;- model.matrix(~ ToY + I(ToY^2) + I(ToY^3), data.frame(ToY=ToY)) phi &lt;- exp(X %*% coef(z)) prem &lt;- 1-exp(-tpred*phi) ## fit a mixture model with polynomial ToY effect z &lt;- cmulti(Y | D ~ ToY + I(ToY^2) + I(ToY^3), x3[x3$ToDc == &quot;Morning&quot;,], type=&quot;mix&quot;) summary(z) ## ## Call: ## cmulti(formula = Y | D ~ ToY + I(ToY^2) + I(ToY^3), data = x3[x3$ToDc == ## &quot;Morning&quot;, ], type = &quot;mix&quot;) ## ## Removal Sampling (heterogeneous singing rate) ## Conditional Maximum Likelihood estimates ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## log.phi -2.98e-01 8.42e-51 -3.54e+49 &lt;2e-16 *** ## logit.c_(Intercept) -2.42e-01 8.42e-51 -2.87e+49 &lt;2e-16 *** ## logit.c_ToY -8.16e-03 8.42e-51 -9.69e+47 &lt;2e-16 *** ## logit.c_I(ToY^2) 2.80e-06 8.42e-55 3.32e+48 &lt;2e-16 *** ## logit.c_I(ToY^3) 3.07e-07 8.42e-55 3.65e+47 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-likelihood: -848 ## BIC = 1.73e+03 chat &lt;- plogis(X %*% coef(z)[-1]) phi &lt;- exp(coef(z)[1]) pmix &lt;- 1-chat*exp(-tpred*phi) Let’s see how does extrapolation look like: plot(pmix ~ ToY, type=&quot;l&quot;, ylim=c(0,1)) lines(prem ~ ToY, col=4) abline(v=qq3, lty=2) Next, we make survey x species table based on 1st 1-min surveys by excluding counts after 1-min: y1 &lt;- Xtab(~ visit + SpeciesID, data=abmi[is.na(abmi$det1) | abmi$det1 == &quot;0-1min&quot;,], cdrop=c(&quot;NONE&quot;,&quot;SNI&quot;, &quot;VNA&quot;, &quot;DNC&quot;, &quot;PNA&quot;)) x1 &lt;- droplevels(nonDuplicated(abmi, visit, TRUE)[rownames(y1), c(&quot;pkey&quot;, &quot;visit&quot;, &quot;ToY&quot;, &quot;ToYc&quot;, &quot;ToD&quot;, &quot;ToDx&quot;, &quot;ToDc&quot;)]) We fit GAM model to detection-nondetection data, and we condition on known occurrences at the species over the breeding season (bracketed by qq3). First, let’s explore species’ responses: s &lt;- colSums(y1[x1$ToY &gt;= 150, ] &gt; 0) SPP &lt;- colnames(y1)[s &gt;= 100] P &lt;- matrix(0, length(ToY), length(SPP)) colnames(P) &lt;- SPP for (sppi in SPP) { x1$y &lt;- ifelse(y1[,sppi] &gt; 0, 1, 0) tmp &lt;- with(x1[x1$ToY %[]% qq3, ], table(pkey, y)) tmp &lt;- tmp[tmp[,&quot;1&quot;] &gt; 0,] x1$occ &lt;- x1$pkey %in% rownames(tmp) m &lt;- mgcv::gam(y ~ s(ToY), x1[x1$ToDc == &quot;Morning&quot; &amp; x1$occ,], family=poisson) P[,sppi] &lt;- predict(m, newdata=data.frame(ToY=ToY), type=&quot;response&quot;) } matplot(ToY, P, type=&quot;l&quot;, ylim=c(0,1), lty=1, col=&quot;#00000044&quot;) Single species models continued: ## species detections x1$y &lt;- ifelse(y1[,spp] &gt; 0, 1, 0) ## finding locations with at least 1 detections tmp &lt;- with(x1[x1$ToY %[]% qq3, ], table(pkey, y)) tmp &lt;- tmp[tmp[,&quot;1&quot;] &gt; 0,] head(tmp) ## y ## pkey 0 1 ## 1086_2017_SE 4 1 ## 121_2016_SE 2 4 ## 121_2016_SW 2 4 ## 1210_2017_SW 5 1 ## 1211_2017_NE 5 1 ## 1211_2017_SE 4 2 x1$occ &lt;- x1$pkey %in% rownames(tmp) table(detection=x1$y, occupancy=x1$occ) ## occupancy ## detection FALSE TRUE ## 0 15235 3388 ## 1 19 1090 ## fit GAM to all the 1-min data m &lt;- mgcv::gam(y ~ s(ToY), x1[x1$ToDc == &quot;Morning&quot; &amp; x1$occ,], family=binomial) pgam1 &lt;- predict(m, newdata=data.frame(ToY=ToY), type=&quot;response&quot;) ## fit GAM to 1-min data in the breeding season m &lt;- mgcv::gam(y ~ s(ToY), x1[x1$ToDc == &quot;Morning&quot; &amp; x1$occ &amp; x1$visit %in% levels(x3$visit),], family=binomial) pgam3 &lt;- predict(m, newdata=data.frame(ToY=ToY), type=&quot;response&quot;) Let’s use time-to-1st-detection data to fit survival model: x1$y &lt;- ifelse(y1[,spp] &gt; 0, 1, 0) tmp &lt;- with(x1[x1$ToY %[]% qq3, ], table(pkey, y)) tmp &lt;- tmp[tmp[,&quot;1&quot;] &gt; 0,] x1$occ &lt;- x1$pkey %in% rownames(tmp) ## subset to known occurrence sites x1 &lt;- droplevels(x1[x1$occ,]) ## we use times from raw data table xx &lt;- droplevels(abmi[abmi$SpeciesID == spp &amp; !is.na(abmi$int1),]) Events need to be organized into a survival object: we treat nondetections as censored events at 60 sec. x1$time &lt;- xx$int1[match(x1$visit, xx$visit)] x1$time[is.na(x1$time)] &lt;- 60 #&#39; time cannot be 0, so we use 1 sec instead x1$time[x1$time == 0] &lt;- 1 #&#39; We give time in minutes, so we get rate as events/min. x1$sv &lt;- Surv(x1$time/60, x1$y) head(x1$sv) ## [1] 1.0000+ 1.0000+ 1.0000+ 1.0000+ 0.7833 1.0000+ tail(x1$sv) ## [1] 1.0000+ 1.0000+ 0.2833 1.0000+ 1.0000+ 1.0000+ Fit a series of survival models: mods &lt;- list(m0 = survreg(sv ~ 1, x1, dist=&quot;exponential&quot;), m1 = survreg(sv ~ ToY, x1, dist=&quot;exponential&quot;), m2 = survreg(sv ~ ToY + I(ToY^2), x1, dist=&quot;exponential&quot;), m3 = survreg(sv ~ ToY + I(ToY^2) + I(ToY^3), x1, dist=&quot;exponential&quot;)) aic &lt;- data.frame(df=sapply(mods, function(z) length(coef(z))), AIC=sapply(mods, AIC)) aic$delta_AIC &lt;- aic$AIC - min(aic$AIC) aic The package survreg fits accelerated failure models, not proportional hazards models, so the coefficients are logarithms of ratios of survival times, and a positive coefficient means longer survival. mb &lt;- mods[[which.min(aic$AIC)]] summary(mb) ## ## Call: ## survreg(formula = sv ~ ToY + I(ToY^2) + I(ToY^3), data = x1, ## dist = &quot;exponential&quot;) ## Value Std. Error z p ## (Intercept) 1.64e+02 2.54e+01 6.46 1.1e-10 ## ToY -2.75e+00 4.80e-01 -5.73 9.9e-09 ## I(ToY^2) 1.51e-02 3.01e-03 5.01 5.4e-07 ## I(ToY^3) -2.67e-05 6.27e-06 -4.26 2.0e-05 ## ## Scale fixed at 1 ## ## Exponential distribution ## Loglik(model)= -1906 Loglik(intercept only)= -2455 ## Chisq= 1098 on 3 degrees of freedom, p= 1e-237 ## Number of Newton-Raphson Iterations: 9 ## n= 4478 ## survival times summary(st &lt;- predict(mb, data.frame(ToY=ToY))) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 2 5 56731 142 2189645 ## event rate per unit (1 min) time summary(phi &lt;- 1 / st) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0072 0.2053 0.2754 0.5175 0.7249 ## probability of at least 1 event per 10 time units (mins) summary(psurv &lt;- 1-exp(-tpred*phi)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0072 0.1856 0.2157 0.4040 0.5156 plot(prem ~ ToY, type=&quot;l&quot;, ylim=c(0,1), main=spp, ylab=&quot;Relative availability&quot;) abline(v=qq3, lty=2, col=&quot;grey&quot;) lines(pmix ~ ToY, col=3) lines(pgam1 ~ ToY, col=2) lines(pgam3 ~ ToY, col=4) lines(psurv ~ ToY, col=5) rug(jitter(x1$ToY), side=1, col=2) rug(jitter(x3$ToY), side=3, col=4) legend(100, 0.7, bty=&quot;n&quot;, lty=1, col=c(1,3,2,4,5), cex=0.6, legend=c(&quot;Conv. removal&quot;, &quot;Mix. removal&quot;, &quot;GAM 1min&quot;, &quot;GAM 3mins&quot;, &quot;Survival&quot;)) Exercise Discuss why the GAM/survival model estimates are different from removal models. Compare these models and phenology curves for a different species, e.g. a winter resident. "],
["assumptions.html", "Chapter 7 A Closer Look at Assumptions 7.1 Intro 7.2 Prerequisites 7.3 bSims runs", " Chapter 7 A Closer Look at Assumptions 7.1 Intro So far, bSims were used to make an idealized world. Real situations might be different from our assumed worlds. In this chapter, we will review how sensitive the various assumptions are, and how violating these assumptions might affect the estimates. 7.2 Prerequisites library(bSims) # simulations library(detect) # multinomial models source(&quot;functions.R&quot;) # some useful stuff 7.3 bSims runs Work in pairs, each group can select an assumption: distance measurement error distance function misspecification effect of truncation distance, number of distance bins effect of total duration and time intervals movement heard vs. heard and seen 1st event vf 1st detection (duration, singing rate) spatial pattern random, uniform, clustered Take a look at various assumptions by following these steps: try to anticipate how a particular setting (violation of assumption) might affect estimates of \\(\\phi\\), \\(\\tau\\), and density, run shiny::runApp(&quot;_shiny/bsims1.R&quot;) and play with that particular setting, apply the setting to the code below and replicate the bSims landscapes (keep a reference, and add 2 values, one in the mid range and one extreme), summarize/visualize how these changes affect estimates of availability, detectability and population density. phi &lt;- 0.5 tau &lt;- 1 Den &lt;- 2 tint &lt;- c(3, 5, 10) rint &lt;- c(0.5, 1, 1.5, Inf) B &lt;- 100 l &lt;- bsims_init() sim_fun0 &lt;- function() { a &lt;- bsims_populate(l, density=Den) b &lt;- bsims_animate(a, vocal_rate=phi) o &lt;- bsims_detect(b, tau=tau) tr &lt;- bsims_transcribe(o, tint=tint, rint=rint) estimate_bsims(tr$rem) } sim_fun1 &lt;- function() { a &lt;- bsims_populate(l, density=Den, xyfun=function(d) { (1-exp(-d^2/1^2) + dlnorm(d, 2)/dlnorm(2,2)) / 2 }, margin=2) b &lt;- bsims_animate(a, vocal_rate=phi) o &lt;- bsims_detect(b, tau=tau) tr &lt;- bsims_transcribe(o, tint=tint, rint=rint) estimate_bsims(tr$rem) } sim_fun2 &lt;- function() { a &lt;- bsims_populate(l, density=Den, xyfun=function(d) { exp(-d^2/1^2) + 0.5*(1-exp(-d^2/4^2)) }, margin=2) b &lt;- bsims_animate(a, vocal_rate=phi) o &lt;- bsims_detect(b, tau=tau) tr &lt;- bsims_transcribe(o, tint=tint, rint=rint) estimate_bsims(tr$rem) } set.seed(123) res0 &lt;- pbapply::pbreplicate(B, sim_fun0(), simplify=FALSE) res1 &lt;- pbapply::pbreplicate(B, sim_fun1(), simplify=FALSE) res2 &lt;- pbapply::pbreplicate(B, sim_fun2(), simplify=FALSE) summary(summarize_bsims(res0)) summary(summarize_bsims(res1)) summary(summarize_bsims(res2)) Here are the Visual detections from the JOSM data all combined and by species: load(&quot;_data/josm/josm.rda&quot;) # JOSM data 100*table(josm$counts$DetectType1)/sum(table(josm$counts$DetectType1)) ## ## C S V ## 17.528 79.829 2.643 aa &lt;- table(josm$counts$SpeciesID, josm$counts$DetectType1) bb &lt;- aa[,&quot;V&quot;]/rowSums(aa) hist(bb) sort(bb[bb &gt; 0.2]) ## ATTW EAKI TRES BAEA OSPR BOGU AMWI MALL MERL RBGU ## 0.2414 0.2500 0.2520 0.3333 0.3333 0.3663 0.4444 0.4615 0.5000 0.5000 ## SPGR SSHA AMKE BLBW CAGU RNDU NOHA BLTE BUFF BWTE ## 0.5000 0.5714 0.6364 0.6452 0.6667 0.8462 0.8571 0.9310 1.0000 1.0000 ## COGO COHA DCCO GWTE HOLA NHOW NSHO RTHU WWSC CANV ## 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 ## NOPI ## 1.0000 "],
["roadsides.html", "Chapter 8 Understanding Roadside Surveys", " Chapter 8 Understanding Roadside Surveys directional diff in signal transmission "],
["extras.html", "Chapter 9 Miscellaneous Topics These are just reminders, to be deleted later 9.1 Binomial model and censoring 9.2 Optimal partitioning 9.3 Optilevels 9.4 N-mixture models 9.5 Estimating abundance", " Chapter 9 Miscellaneous Topics model selection and conditional likelihood variance/bias trade off error propagation MCMC? N-mixture ideas phylogenetic and life history/trait stuff PIF methods These are just reminders, to be deleted later You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 1. If you do not manually label them, there will be automatic labels anyway. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 9.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 9.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 9.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 9.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa 9.1 Binomial model and censoring Try cloglog with a rare species, like BOCH #spp &lt;- &quot;OVEN&quot; # which species spp &lt;- &quot;BOCH&quot; # which species #spp &lt;- &quot;CAWA&quot; # which species x &lt;- data.frame( josm$surveys, y=as.numeric(ytot[rownames(x), spp])) x$y01 &lt;- ifelse(x$y &gt; 0, 1, 0) table(x$y) mP &lt;- glm(y ~ Decid * ConifWet, x, family=poisson) mBc &lt;- glm(y01 ~ Decid * ConifWet, x, family=binomial(&quot;cloglog&quot;)) mBl &lt;- glm(y01 ~ Decid * ConifWet, x, family=binomial(&quot;logit&quot;)) coef(mP) coef(mBc) coef(mBl) plot(fitted(mBc) ~ fitted(mP), col=4, ylim=c(0, max(fitted(mP))), xlim=c(0, max(fitted(mP)))) points(exp(model.matrix(mBc) %*% coef(mBc)) ~ fitted(mP), col=2) abline(0,1) 9.2 Optimal partitioning oc &lt;- opticut(as.matrix(ytot) ~ 1, strata = x$HAB, dist=&quot;poisson&quot;) plot(oc) 9.3 Optilevels When we have categorical or compositional (when e.g. proportions add up to 1, also called the unit sum constraint) data, we often want to simplify and merge classes or add up columns. We can do this based on the structural understanding of these land cover classes (call all treed classes Forest, like what we did for FOR, WET and AHF). Alternatively, we can let the data (the birds) tell us how to merge the classes. The algorithm does the following: fit model with all classes, order estimates for each class from smallest to largest, merge classes that are near each others, 2 at a time, moving from smallest to largest, compare \\(\\Delta\\)AIC or \\(\\Delta\\)BIC values for the merged models and pick the smallest, treat this best merged model as an input in step 1 and star over until \\(\\Delta\\) is negative (no improvement). Here is the code for simplifying categories using the opticut::optilevels function: M &lt;- model.matrix(~HAB-1, x) colnames(M) &lt;- levels(x$HAB) ol1 &lt;- optilevels(x$y, M, dist=&quot;poisson&quot;) sort(exp(coef(bestmodel(ol1)))) ## estimates exp(ol1$coef) ## optimal classification ol1$rank data.frame(combined_levels=ol1$levels[[length(ol1$levels)]]) Here is the code for simplifying compositional data: ol2 &lt;- optilevels(x$y, x[,cn], dist=&quot;poisson&quot;) sort(exp(coef(bestmodel(ol2)))) ## estimates exp(ol2$coef) ## optimal classification ol2$rank head(groupSums(as.matrix(x[,cn]), 2, ol2$levels[[length(ol2$levels)]])) 9.4 N-mixture models 9.5 Estimating abundance Exponential model, bSims data Note: $visits is not yet exposed set.seed(1) phi &lt;- 0.5 Den &lt;- 1 l &lt;- bsims_init() a &lt;- bsims_populate(l, density=Den) b &lt;- bsims_animate(a, vocal_rate=phi) tint &lt;- 1:5 (tr &lt;- bsims_transcribe(b, tint=tint)) Multiple-visit stuff for bSims: the counting of new individuals resets for each interval (also: needs equal intervals) tr$visits v &lt;- get_events(b, vocal_only=TRUE) v &lt;- v[v$t &lt;= max(tint),] v1 &lt;- v[!duplicated(v$i),] tmp &lt;- v1 tmp$o &lt;- seq_len(nrow(v1)) plot(o ~ t, tmp, type=&quot;n&quot;, ylab=&quot;Individuals&quot;, main=&quot;Vocalization events&quot;, ylim=c(1, nrow(b$nests)), xlim=c(0,max(tint))) for (i in tmp$o) { tmp2 &lt;- v[v$i == v1$i[i],] lines(c(tmp2$t[1], max(tint)), c(i,i), col=&quot;grey&quot;) points(tmp2$t, rep(i, nrow(tmp2)), cex=0.5) points(tmp2$t[1], i, pch=19, cex=0.5) } plot(o ~ t, tmp, type=&quot;n&quot;, ylab=&quot;Individuals&quot;, main=&quot;Vocalization events&quot;, ylim=c(1, nrow(b$nests)), xlim=c(0,max(tint))) for (j in seq_along(tint)) { ii &lt;- if (j == 1) c(0, tint[j]) else c(tint[j-1], tint[j]) vv &lt;- v[v$t &gt; ii[1] &amp; v$t &lt;= ii[2],] tmp &lt;- vv[!duplicated(vv$i),] tmp$o &lt;- seq_len(nrow(tmp)) if (nrow(tmp)) { for (i in tmp$o) { tmp2 &lt;- vv[vv$i == tmp$i[i],] lines(c(tmp2$t[1], ii[2]), c(i,i), col=&quot;grey&quot;) points(tmp2$t, rep(i, nrow(tmp2)), cex=0.5) points(tmp2$t[1], i, pch=19, cex=0.5) } } } library(unmarked) f &lt;- function() { a &lt;- bsims_populate(l, density=Den) b &lt;- bsims_animate(a, vocal_rate=phi, move_rate=0) tr &lt;- bsims_transcribe(b, tint=tint) drop(tr$visits) } Den &lt;- 0.01 #ymx &lt;- tr$visits (ymx &lt;- t(replicate(10, f()))) ## highly dependent on K when Den is higher nmix &lt;- pcount(~1 ~1, unmarkedFramePCount(y=ymx), K=1000) coef(nmix) plogis(coef(nmix)[2]) exp(coef(nmix)[1]) Den * 100 ## distance function density # check http://seankross.com/notes/dpqr/ need rmax for rdist .g &lt;- function(d, tau, b=2, hazard=FALSE) if (hazard) 1-exp(-(d/tau)^-b) else exp(-(d/tau)^b) .f &lt;- function(d, tau, b=2, hazard=FALSE) .g(d, tau, b, hazard) * 2*d # this is g*h / integral ddist &lt;- function(x, tau, b=1, hazard=FALSE, rmax=Inf, log = FALSE){ V &lt;- integrate(.f, lower=0, upper=rmax, tau=tau, b=b, hazard=hazard)$value out &lt;- .f(x, tau=tau, b=b, hazard=hazard) / V out[x &gt; rmax] &lt;- 0 if (log) log(out) else out } pdist &lt;- function(q, tau, b=1, hazard=FALSE, lower.tail = TRUE, log.p = FALSE){} qdist &lt;- function(p, tau, b=1, hazard=FALSE, lower.tail = TRUE, log.p = FALSE){} # # make output vector # loop over out and accept reject based on weights as ddist # can do in batches: star with n, continue with rejected n, etc rdist &lt;- function(n, tau, b=1, hazard=FALSE, rmax=Inf){ q99 &lt;- uniroot(function(d) ddist(d)) } tau &lt;- 2.3 V &lt;- integrate(.f, lower=0, upper=Inf, tau=tau)$value d &lt;- runif(10^4, 0, 10) d &lt;- d[rbinom(10^4, 1, .f(d, tau)/V) &gt; 0] hist(d) nll &lt;- function(tau) { -sum(ddist(d, tau, log=TRUE)) } optimize(nll, c(0, 100)) # once done: generate distances with rdist, and fit model with ddist # see if AIC etc works as expected "]
]
