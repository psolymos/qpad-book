# Organizing and Processing Point Count Data {#pcdata}

> All data are messy, but some are missing

## Introduction

It is often called _data processing_, _data munging_,
_data wrangling_, _data cleaning_.
None of these expressions capture the dread associated with the actual activity.
Luckily, this book and course is not about data manipulation,
and we can gladly skip these steps. But keep in mind,
in real life, you'll often have to worry about at least four things
that can go wrong:

1. space (e.g. wrong UTM zones, errors),
2. time (ISO format please),
3. taxonomy (unknowns, mis-identifications),
4. something else (if there were no errors, check again).

This chapter sets you up for the required data skills for the
rest of the book/course.

## Prerequisites

```{r data-load_josm_data}
library(mefa4)                # data manipulation
library(raster)               # reading raster files
library(sp)                   # spatial data manipulation
load("_data/josm/josm.rda")   # load bird data
rr <- stack("_data/josm/landcover-hfi2016.grd") # rasters
```

## R basics {#rbasics}

This short document is intended to help you brush up your R skills.
If you feel that these R basics are not very familiar,
I suggest to take a look at some introductory R books,
sich as this preprint version of Norman Matloff's *The Art of R Programming* book: http://heather.cs.ucdavis.edu/~matloff/132/NSPpart.pdf, check out Chapters 1--6.

R is a great calculator:

```{r eval=FALSE}
1 + 2
```

Assign a value and print an object using `=` or `<-` (preferred in this book):

```{r eval=FALSE}
(x = 2) # shorthand for print
print(x)
x == 2 # logical operator, not assignment
y <- x + 0.5
y # another way to print
```

Logical operators come handy:

```{r eval=FALSE}
x == y # equal
x != y # not eaqual
x < y # smaller than
x >= y # greater than or equal
```

Vectors and sequences are created most often by the functions
`c`, `:`, `seq`, and `rep`:

```{r eval=FALSE}
x <- c(1, 2, 3)
x
1:3
seq(1, 3, by = 1)

rep(1, 5)
rep(1:2, 5)
rep(1:2, each = 5)
```

When doing operations with vectors remember that values of
the shorter object are recycled:

```{r eval=FALSE}
x + 0.5
x * c(10, 11, 12, 13)
```

Indexing and ordering vectors is a a fundamental skill:

```{r eval=FALSE}
x[1]
x[c(1, 1, 1)] # a way of repeatig values
x[1:2]
x[x != 2]
x[x == 2]
x[x > 1 & x < 3]
order(x, decreasing=TRUE)
x[order(x, decreasing=TRUE)]
rev(x) # reverse
```

See how `NA` values can influence
sorting character vectors:

```{r eval=FALSE}
z <- c("b", "a", "c", NA)
z[z == "a"]
z[!is.na(z) & z == "a"]
z[is.na(z) | z == "a"]
is.na(z)
which(is.na(z))
sort(z)
sort(z, na.last=TRUE)
```

There are a few special values:

```{r eval=FALSE}
as.numeric(c("1", "a")) # NA: not available (missing or invalid)
0/0 # NaN: not a number
1/0 # Inf
-1/0 # -Inf
```

Matrices and arrays are vectors with dimensions, elements are in same mode: 


```{r eval=FALSE}
(m <- matrix(1:12, 4, 3))
matrix(1:12, 4, 3, byrow=TRUE)

array(1:12, c(2, 2, 3))
```

Many objects have attributes:

```{r eval=FALSE}
dim(m)
dim(m) <- NULL
m
dim(m) <- c(4, 3)
m
dimnames(m) <- list(letters[1:4], LETTERS[1:3])
m
attributes(m)
```

Matrice and indices:

```{r eval=FALSE}
m[1:2,]
m[1,2]
m[,2]
m[,2,drop=FALSE]
m[2]

m[rownames(m) == "c",]
m[rownames(m) != "c",]
m[rownames(m) %in% c("a", "c", "e"),]
m[!(rownames(m) %in% c("a", "c", "e")),]
```

Lists and indexing:

```{r eval=FALSE}
l <- list(m = m, x = x, z = z)
l
l$ddd <- sqrt(l$x)
l[2:3]
l[["ddd"]]
```

Data frames are often required for statistical modeling.
A data frame is a list where length of elements match and elements 
can be in different mode.


```{r eval=FALSE}
d <- data.frame(x = x, sqrt_x = sqrt(x))
d
```

Inspect structure of R objects:

```{r eval=FALSE}
str(x)
str(z)
str(m)
str(l)
str(d)
str(as.data.frame(m))
str(as.list(d))
```

Get summaries of these objects:

```{r eval=FALSE}
summary(x)
summary(z)
summary(m)
summary(l)
summary(d)
```


## JOSM data set

The data is based on the project _Cause-Effect Monitoring Migratory Landbirds at Regional Scales: 
understand how boreal songbirds are affected by human activity in the oil sands area_ (\@ref(fig:data-1) and \@ref(fig:data-2), [@mahon2016]).
JOSM stands for Joint Oil Sands Monitoring.
Look at the source code in the `_data/josm` directory of the book
if you are interested in data processing details.
We skip that for now.

```{r data-1,echo=FALSE,fig.cap='JOSM bird data survey locations from [@mahon2016].',out.width='80%'}
include_graphics("images/mahon-2016-fig-1.png")
```

```{r data-2,echo=FALSE,fig.cap='Survey area boundary, habitat types and human footprint mapping [@mahon2016].',out.width='80%'}
include_graphics("images/mahon-2016-fig-2.png")
```

Surveys were spatially replicated because:

- we want to make inferences about a population,
- full census is out of reach,
- thus we take a sample of the population
- that is representative and random.
- Ideally, sample size should be as large as possible,
- it reduces variability and 
- increases statistical power.

Survey locations were pucked based on various criteria:

- stratification (land cover),
- gradients (disturbance levels),
- random location (control for unmeasured effects),
- take into account historical surveys (avoid, or revisit),
- access, cost (clusters).

The `josm` obejct is a list with 3 elements:

- `surveys`: data frame with survey specific information,
- `species`: lookup table for species,
- `counts`: individual counts by survey and species.

```{r data-josm_data}
names(josm)
```

Species info: species codes, common and scientific names. The table could also contain
taxonomic, trait, etc. information as well.

```{r data-josm_species}
head(josm$species)
```

At the survey level, we have coordinates, date/time info,
variables capturing survey conditions, and land cover info extracted from 1 km$^2$ resolution rasters.

```{r data-josm_surveys}
colnames(josm$surveys)
```

The count table contains one row for each unique individual 
of a species (`SpeciesID` links to the species lookup table)
observed during a survey (`StationID` links to the survey attribute table).
Check the data dictionary in `_data/josm` folder for a detailed explanation of each column.

```{r data-josm_counts}
str(josm$counts)
```

## Cross tabulating species counts

Take the following dummy data frame (long format):

```{r data-dummy_data}
(d <- data.frame(
  sample=factor(paste0("S", c(1,1,1,2,2)), paste0("S", 1:3)),
  species=c("BTNW", "OVEN", "CANG", "AMRO", "CANG"),
  abundance=c(1, 1, 2, 1, 1),
  behavior=rep(c("heard","seen"), c(4, 1))))
str(d)
```

We want to add up the `abundance`s for each sample (rows) and species (column):

```{r data-xtab_1}
(y <- Xtab(abundance ~ sample + species, d))
```

`y` is a sparse matrix, that is a very compact representation:

```{r data-xtab_matrix}
object.size(d[,1:3])
object.size(y)
```

Notice that we have 3 rows, but `d$sample` did not have an `S3` value, but it was a level.
We can drop such unused levels, but it is generally not recommended, and we need to be careful
not to drop samples where no species was detected (this can happen quite often depending on timing of
surveys)

```{r data-xtab_2}
Xtab(abundance ~ sample + species, d, drop.unused.levels = TRUE)
```

A sparse matrix can be converted to ordinary matrix

```{r data-xtab_as_matrix}
as.matrix(y)
```


The nice thing about this cross tabulation is that we can finter the records without
changing the structure (rows, columns) of the table:

```{r data-xtab_3}
Xtab(abundance ~ sample + species, d[d$behavior == "heard",])
Xtab(abundance ~ sample + species, d[d$behavior == "seen",])
```

Now let's do this for the real data. We have no abundance column, because 
each row stands for exactly one individual. We can add a column with 1's,
or we can just count the number of rows by using only the right-hand-side of the
formula in `Xtab`. `ytot` will be our total count matrix for now.

We also want to filter the records to contain only `S`ongs and `C`alls, without
`V`visual detections:

```{r data-beh}
table(josm$counts$DetectType1, useNA="always")
```

We use `SiteID` for row names, because only 1 station and visit was done at each site:

```{r data-xtab_4}
ytot <- Xtab(~ SiteID + SpeciesID, josm$counts[josm$counts$DetectType1 != "V",])
```

See how not storing 0's affect size compared to the long formar and an ordinary wide matrix

```{r data-xtab_matrix_2}
## 2-column data frame as reference
tmp <- as.numeric(object.size(
  josm$counts[josm$counts$DetectType1 != "V", c("StationID", "SpeciesID")]))
## spare matrix
as.numeric(object.size(ytot)) / tmp
## dense matrix
as.numeric(object.size(as.matrix(ytot))) / tmp
## matrix fill
sum(ytot > 0) / prod(dim(ytot))
```

Check if counts are as expected:

```{r}
max(ytot) # this is interesting
sort(apply(as.matrix(ytot), 2, max)) # it is CANG
## lyover (FO) flock (FL) beyond 100m distance
head(josm$counts[
  josm$counts$SiteID == rownames(ytot)[which(ytot[,"CANG"] == 200)] &
  josm$counts$SpeciesID == "CANG",])
```

We can check overall mean counts:

```{r data-mean_counts}
round(sort(colMeans(ytot)), 4)
```

## Joining species data with predictors

Let's join the species counts with the survey attributes. 
This is how we can prepare the input data for regression analysis.

```{r data-join}
spp <- "OVEN" # which species
josm$species[spp,]

compare_sets(rownames(josm$surveys),rownames(ytot))

x <- josm$surveys
x$y <- as.numeric(ytot[rownames(x), spp])
```

## Explore predictor variables

Put the locations on the map:

```{r data-xy}
xy <- x[,c("Longitude", "Latitude")]
coordinates(xy) <- ~ Longitude + Latitude
proj4string(xy) <- "+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs"
xy <- spTransform(xy, proj4string(rr))
col <- colorRampPalette(c("lightgrey", "blue"))(100)
plot(rr[["Water"]], col=col, axes=FALSE, box=FALSE, legend=FALSE)
plot(xy, add=TRUE, pch=19, cex=0.5)
```


```{block2, type='rmdexercise'}
**Exercise**

Explore the data to understand the distributions and associations.
Use `summary`, `table`, `hist`, `plot` (bivariate, scatterplot matrix), etc.
```

## Derived variables

Add up some of the compositional variables into meaningful units:

```{r data-deriv1}
x$FOR <- x$Decid + x$Conif+ x$ConifWet # forest
x$AHF <- x$Agr + x$UrbInd + x$Roads # 'alienating' human footprint
x$WET <- x$OpenWet + x$ConifWet + x$Water # wet + water
```

Classify surveys locations based on dominant land cover type:

```{r data-deriv2}
cn <- c("Open", "Water", "Agr", "UrbInd", "SoftLin", "Roads", "Decid", 
  "OpenWet", "Conif", "ConifWet")
h <- find_max(x[,cn])
hist(h$value)
table(h$index)
x$HAB <- droplevels(h$index) # drop empty levels
```


