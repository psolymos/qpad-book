[
["index.html", "Point count data analysis: How to violate assumptions and get away with it Foreword 0.1 About the book and the course 0.2 About the author 0.3 Summary of course objectives 0.4 Installing 0.5 These are just reminders, to be deleted later", " Point count data analysis: How to violate assumptions and get away with it Peter Solymos 2019-06-05 Foreword This book provides material for the workshop Analysis of point-count data in the presence of variable survey methodologies and detection error at the AOS 2019 conference by Peter Solymos. The book and related materials in this repository is the basis of a full day workshop (8 hours long with 3 breaks). Prior exposure to R language is necessary (i.e. basic R object types and their manipulation, such as arrays, data frames, indexing) because this is not covered as part of the course. Check this intro. 0.1 About the book and the course You’ll learn how to analyze your point count data when it combines different methodologies/protocols/technologies, how to violate assumptions and get away with it. 0.2 About the author Ecologist (molluscs, birds), pretty good at stats (modeling, detectability, data cloning, multivariate), R programmer (vegan, detect, ResourceSelection, pbapply), sometimes I teach (like today). 0.3 Summary of course objectives This course is aimed towards ornithologists analyzing field observations, who are often faced by data heterogeneities due to field sampling protocols changing from one project to another, or through time over the lifespan of projects, or trying to combine ‘legacy’ data sets with new data collected by recording units. Such heterogeneities can bias analyses when data sets are integrated inadequately, or can lead to information loss when filtered and standardized to common standards. Accounting for these issues is important for better inference regarding status and trend of bird species and communities. Analysts of such ‘messy’ data sets need to feel comfortable with manipulating the data, need a full understanding the mechanics of the models being used (i.e. critically interpreting the results and acknowledging assumptions and limitations), and should be able to make informed choices when faced with methodological challenges. The course emphasizes critical thinking and active learning. Participants will be asked to take part in the analysis: first hand analytics experience from start to finish. We will use publicly available data sets to demonstrate the data manipulation and analysis. We will use freely available and open-source R packages. The expected outcome of the course is a solid foundation for further professional development via increased confidence in applying these methods for field observations. 0.4 Installing The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) ## clean up bookdown::clean_book(TRUE) ## rendering the book bookdown::render_book(&#39;index.Rmd&#39;, &#39;bookdown::pdf_book&#39;) bookdown::render_book(&#39;index.Rmd&#39;, &#39;bookdown::gitbook&#39;) bookdown::render_book(&#39;index.Rmd&#39;, &#39;bookdown::epub_book&#39;) To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. 0.5 These are just reminders, to be deleted later You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 1. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 0.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 0.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 0.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 0.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2019) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["intro.html", "Chapter 1 Introduction 1.1 Apples and oranges 1.2 Apples to apples 1.3 Effects can be significant 1.4 So what is a point count? 1.5 Questions we want to answer using point counts 1.6 Standardization by design 1.7 Protocols do vary 1.8 Pop quiz 1.9 Moving away from standards 1.10 Model based approaches 1.11 Models come with assumptions 1.12 Assumptions are everywhere 1.13 Pop quiz 1.14 The hard truth 1.15 Our approach", " Chapter 1 Introduction All assumptions are violated, but some are more than others 1.1 Apples and oranges “A comparison of apples and oranges occurs when two items or groups of items are compared that cannot be practically compared.” [Wikipedia] How we measure things can have big impact on our results. You might say: I saw 5 robins (walking down the road), I might say: I only saw one (sitting on my porch) 1.2 Apples to apples Effort: area of the physical space searched, amount of time spent, number of individuals identified. Experience, skill, “sensitivity”: number of years in field work, eye sight, hearing ability, mic sensitivity. The goal is to make our measurements comparable. 1.3 Effects can be significant 10-min unlimited count ~300% increase over 3-min 50-m count. Average across 54 species of boreal songbirds. 1.4 So what is a point count? A trained observer records all the birds seen and heard from a point count station for a set period of time within a defined distance radius. 1.5 Questions we want to answer using point counts How many? (Abundance, density, population size) Is this location part of the range? (0/1) How is abundance changing in space? (Distribution) How is abundance changing in time? (Trend) What is the effect of a treatment on abundance? 1.6 Standardization by design Have a set of standards/recommendations that people will follow to maximize efficiency in the numbers of birds and species counted, minimize extraneous variability in the counts. But programs started to deviate from standards: “For example, only 3% of 196,000 point counts conducted during the period 1992–2011 across Alaska and Canada followed the standards recommended for the count period and count radius.” 1.7 Protocols do vary Survey methodology variation (colors) among contributed projects in the Boreal Avian Modelling (BAM) data base as of 2014. 1.8 Pop quiz In what regard can protocols differ? What drives protocol variation among projects? Why have we abandoned following protocols? 1.9 Moving away from standards Detection probabilities might vary even with fixed effort (we’ll cover this more later), programs might have their own goals and constraints (access, training, etc). 1.10 Model based approaches Less labour intensive methods for unmarked populations has come to the forefront: double observer (Nichols et al. 2000), distance sampling (Buckland et al. 2001), removal sampling (Farnsworth et al. 2002), multiple visit occupancy (MacKenzie et al. 2002), multiple visit abundance (Royle 2004). 1.11 Models come with assumptions Population is closed during multiple visits, observers are independent, all individuals emit cues with identical rates, spatial distribution of individuals is uniform, etc. (we will investigate this further in depth). 1.12 Assumptions are everywhere Although assumptions are everywhere, we are really good at ignoring them: Relativistic time dilation is negligible (as long as we are not on a space station), samples are independent. 1.13 Pop quiz Can you mention some other common assumptions? Can you explain why we neglect/violate assumptions? 1.14 The hard truth Assumptions are violated in many ways, because we seek simplicity. The main question we have to ask: does it matter in practice? 1.15 Our approach We will introduce a concept, understand how we can infer it from data, then we recreate the situation in silico, and see how the outcome changes as we make different assumptions. It is guaranteed that we violate every assumption we make. To get away with it, we need to understand how much is too much. “All assumptions are violated, but some are more than others.” "],
["pcdata.html", "Chapter 2 Organizing and Processing Point Count Data 2.1 JOSM (Joint Oil Sands Monitoring) data 2.2 Cross tabulating species counts 2.3 Joining species data with predictors 2.4 Explore predictor variables", " Chapter 2 Organizing and Processing Point Count Data All data are messy, but some are missing It is often called data processing, data munging, data wrangling, data cleaning. None of these expressions capture the dread associated with the actual activity. Luckily, there are only 4 things that can get messed up: space (e.g. wrong UTM zones), time (ISO format please), taxonomy (UNK, mis-ID), something else (if there were no errors, check again). 2.1 JOSM (Joint Oil Sands Monitoring) data Look at the source code in the _data/josm directory of the book if you are interested in data processing details. We skip that for now. include_graphics(&quot;./images/mahon-2016-fig-1.png&quot;) Cause-Effect Monitoring Migratory Landbirds at Regional Scales: understand how boreal songbirds are affected by human activity in the oil sands area. include_graphics(&quot;./images/mahon-2016-fig-2.png&quot;) Survey area boundary (\\(r\\)=2.5 km circle), habitat type and human footprint mapping, and clustered point count site locations. Surveys were spatially replicated because: we want to make inferences about a population, full census is out of reach, thus we take a sample of the population that is representative and random. Ideally, sample size should be as large as possible, it reduces variability and increases statistical power. Survey locations were pucked based on various criteria: stratification (land cover), gradients (disturbance levels), random location (control for unmeasured effects), take into account historical surveys (avoid, or revisit), access, cost (clusters). The josm obejct is a list with 3 elements: surveys: data frame with survey specific information, species: lookup table for species, counts: individual counts by survey and species. library(mefa4) load(&quot;./_data/josm/josm.rda&quot;) names(josm) ## [1] &quot;surveys&quot; &quot;species&quot; &quot;counts&quot; Species info: species codes, common and scientific names. The table could also contain taxonomic, trait, etc. information as well. head(josm$species) At the survey level, we have coordinates, date/time info, variables capturing survey conditions, and land cover info extracted from 1 km\\(^2\\) resolution rasters. colnames(josm$surveys) ## [1] &quot;SiteID&quot; &quot;SurveyArea&quot; &quot;Longitude&quot; ## [4] &quot;Latitude&quot; &quot;Date&quot; &quot;StationID&quot; ## [7] &quot;ObserverID&quot; &quot;TimeStart&quot; &quot;VisitID&quot; ## [10] &quot;WindStart&quot; &quot;PrecipStart&quot; &quot;TempStart&quot; ## [13] &quot;CloudStart&quot; &quot;WindEnd&quot; &quot;PrecipEnd&quot; ## [16] &quot;TempEnd&quot; &quot;CloudEnd&quot; &quot;TimeFin&quot; ## [19] &quot;Noise&quot; &quot;OvernightRain&quot; &quot;DateTime&quot; ## [22] &quot;SunRiseTime&quot; &quot;SunRiseFrac&quot; &quot;TSSR&quot; ## [25] &quot;OrdinalDay&quot; &quot;DAY&quot; &quot;Open&quot; ## [28] &quot;Water&quot; &quot;Agr&quot; &quot;UrbInd&quot; ## [31] &quot;SoftLin&quot; &quot;Roads&quot; &quot;Decid&quot; ## [34] &quot;OpenWet&quot; &quot;Conif&quot; &quot;ConifWet&quot; The count table contains one row for each unique individual of a species (SpeciesID links to the species lookup table) observed during a survey (StationID links to the survey attribute table). Check the data dictionary in _data/josm folder for a detailed explanation of each column. str(josm$counts) ## &#39;data.frame&#39;: 52372 obs. of 18 variables: ## $ ObservationID: Factor w/ 57024 levels &quot;CL10102-130622-001&quot;,..: 1 2 3 4 5 6 8 9 10 11 ... ## $ SiteID : Factor w/ 4569 levels &quot;CL10102&quot;,&quot;CL10106&quot;,..: 1 1 1 1 1 1 1 1 2 2 ... ## $ StationID : Factor w/ 4569 levels &quot;CL10102-1&quot;,&quot;CL10106-1&quot;,..: 1 1 1 1 1 1 1 1 2 2 ... ## $ TimeInterval : int 1 1 1 1 5 5 1 1 1 1 ... ## $ Direction : int 1 2 2 2 1 4 4 4 1 1 ... ## $ Distance : int 1 2 2 1 3 3 2 1 1 1 ... ## $ DetectType1 : Factor w/ 3 levels &quot;C&quot;,&quot;S&quot;,&quot;V&quot;: 2 2 2 2 1 1 2 2 2 2 ... ## $ DetectType2 : Factor w/ 3 levels &quot;C&quot;,&quot;S&quot;,&quot;V&quot;: NA NA NA NA NA NA NA NA NA NA ... ## $ DetectType3 : Factor w/ 3 levels &quot;C&quot;,&quot;S&quot;,&quot;V&quot;: NA NA NA NA NA NA NA NA NA NA ... ## $ Sex : Factor w/ 4 levels &quot;F&quot;,&quot;M&quot;,&quot;P&quot;,&quot;U&quot;: 2 2 2 2 4 4 2 2 2 2 ... ## $ Age : Factor w/ 6 levels &quot;A&quot;,&quot;F&quot;,&quot;J&quot;,&quot;JUV&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Activity1 : Factor w/ 17 levels &quot;BE&quot;,&quot;CF&quot;,&quot;CH&quot;,..: 5 5 5 5 NA NA NA 5 5 NA ... ## $ Activity2 : Factor w/ 17 levels &quot;48&quot;,&quot;BE&quot;,&quot;CF&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## $ Activity3 : Factor w/ 7 levels &quot;CF&quot;,&quot;DC&quot;,&quot;DR&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## $ ActivityNote : Factor w/ 959 levels &quot;AGITATED&quot;,&quot;AGITATED CALLING&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## $ Dur : Factor w/ 3 levels &quot;0-3min&quot;,&quot;3-5min&quot;,..: 1 1 1 1 3 3 1 1 1 1 ... ## $ Dis : Factor w/ 3 levels &quot;0-50m&quot;,&quot;50-100min&quot;,..: 1 2 2 1 3 3 2 1 1 1 ... ## $ SpeciesID : Factor w/ 150 levels &quot;ALFL&quot;,&quot;AMBI&quot;,..: 107 95 95 107 46 43 140 95 125 38 ... 2.2 Cross tabulating species counts Take the following dummy data frame (long format): (d &lt;- data.frame( sample=factor(paste0(&quot;S&quot;, c(1,1,1,2,2)), paste0(&quot;S&quot;, 1:3)), species=c(&quot;BTNW&quot;, &quot;OVEN&quot;, &quot;CANG&quot;, &quot;AMRO&quot;, &quot;CANG&quot;), abundance=c(1, 1, 2, 1, 1), behavior=rep(c(&quot;heard&quot;,&quot;seen&quot;), c(4, 1)))) str(d) ## &#39;data.frame&#39;: 5 obs. of 4 variables: ## $ sample : Factor w/ 3 levels &quot;S1&quot;,&quot;S2&quot;,&quot;S3&quot;: 1 1 1 2 2 ## $ species : Factor w/ 4 levels &quot;AMRO&quot;,&quot;BTNW&quot;,..: 2 4 3 1 3 ## $ abundance: num 1 1 2 1 1 ## $ behavior : Factor w/ 2 levels &quot;heard&quot;,&quot;seen&quot;: 1 1 1 1 2 We want to add up the abundances for each sample (rows) and species (column): (y &lt;- Xtab(abundance ~ sample + species, d)) ## 3 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## AMRO BTNW CANG OVEN ## S1 . 1 2 1 ## S2 1 . 1 . ## S3 . . . . y is a sparse matrix, that is a very compact representation: object.size(d[,1:3]) ## 2328 bytes object.size(y) ## 2160 bytes Notice that we have 3 rows, but d$sample did not have an S3 value, but it was a level. We can drop such unused levels, but it is generally not recommended, and we need to be careful not to drop samples where no species was detected (this can happen quite often depending on timing of surveys) Xtab(abundance ~ sample + species, d, drop.unused.levels = TRUE) ## 2 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## AMRO BTNW CANG OVEN ## S1 . 1 2 1 ## S2 1 . 1 . A sparse matrix can be converted to ordinary matrix as.matrix(y) ## AMRO BTNW CANG OVEN ## S1 0 1 2 1 ## S2 1 0 1 0 ## S3 0 0 0 0 The nice thing about this cross tabulation is that we can finter the records without changing the structure (rows, columns) of the table: Xtab(abundance ~ sample + species, d[d$behavior == &quot;heard&quot;,]) ## 3 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## AMRO BTNW CANG OVEN ## S1 . 1 2 1 ## S2 1 . . . ## S3 . . . . Xtab(abundance ~ sample + species, d[d$behavior == &quot;seen&quot;,]) ## 3 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## AMRO BTNW CANG OVEN ## S1 . . . . ## S2 . . 1 . ## S3 . . . . Now let’s do this for the real data. We have no abundance column, because each row stands for exactly one individual. We can add a column with 1’s, or we can just count the number of rows by using only the right-hand-side of the formula in Xtab. ytot will be our total count matrix for now. We also want to filter the records to contain only Songs and Calls, without Vvisual detections: table(josm$counts$DetectType1, useNA=&quot;always&quot;) ## ## C S V &lt;NA&gt; ## 9180 41808 1384 0 We use SiteID for row names, because only 1 station and visit was done at each site: ytot &lt;- Xtab(~ SiteID + SpeciesID , josm$counts[josm$counts$DetectType1 != &quot;V&quot;,]) See how not storing 0’s affect size compared to the long formar and an ordinary wide matrix ## 2-column data frame as reference tmp &lt;- as.numeric(object.size( josm$counts[josm$counts$DetectType1 != &quot;V&quot;, c(&quot;StationID&quot;, &quot;SpeciesID&quot;)])) ## spare matrix as.numeric(object.size(ytot)) / tmp ## [1] 0.1366 ## dense matrix as.numeric(object.size(as.matrix(ytot))) / tmp ## [1] 1.106 ## matrix fill sum(ytot &gt; 0) / prod(dim(ytot)) ## [1] 0.04911 Check if counts are as expected: max(ytot) # this is interesting ## [1] 200 sort(apply(as.matrix(ytot), 2, max)) # it is CANG ## BUFF BWTE COGO COHA DCCO GWTE HOLA NHOW NSHO RTHU WWSC CANV NOPI ## 0 0 0 0 0 0 0 0 0 0 0 0 0 ## AMBI AMCO AMGO BAEA BAOR BEKI BOWA CONI CSWA EAPH GBHE GCTH GGOW ## 1 1 1 1 1 1 1 1 1 1 1 1 1 ## GHOW HOWR LEOW MERL NESP NOGO NOHA NSWO PBGR RBGU RTHA SAVS SPSA ## 1 1 1 1 1 1 1 1 1 1 1 1 1 ## WBNU BRBL CAGU MYWA SNBU VEER AMKE AMWI BADO BARS BBWO BHCO BLBW ## 1 1 1 1 1 1 2 2 2 2 2 2 2 ## BLPW BLTE BWHA COGR DOWO EAKI HAWO KILL LEYE NAWA NOPO OSFL OSPR ## 2 2 2 2 2 2 2 2 2 2 2 2 2 ## PIWO PUFI RNDU SORA SSHA COSN AMCR AMRO ATTW BHVI BOCH BRCR BTNW ## 2 2 2 2 2 2 3 3 3 3 3 3 3 ## CMWA FOSP FRGU GCKI MAWR MOWA NOFL PHVI SACR SOSA SOSP SPGR TRES ## 3 3 3 3 3 3 3 3 3 3 3 3 3 ## WETA WIWA WIWR YBSA FOTE BAWW BBWA BCCH BLJA CAWA CONW COTE GRYE ## 3 3 3 3 3 4 4 4 4 4 4 4 4 ## NOWA NRWS OCWA REVI RNGR RUBL RWBL WAVI WEWP WISN YBFL YWAR ALFL ## 4 4 4 4 4 4 4 4 4 4 4 4 5 ## AMRE CHSP CORA EVGR HETH LCSP RBGR RBNU RCKI SWSP CCSP COYE DEJU ## 5 5 5 5 5 5 5 5 5 5 6 6 6 ## LEFL LISP MAWA OVEN RUGR SWTH BOGU MALL GRAJ PAWA WTSP YRWA COLO ## 6 6 6 6 6 6 7 7 8 8 8 8 9 ## TEWA AMPI WWCR CEDW PISI RECR CANG ## 12 12 20 23 50 51 200 ## lyover (FO) flock (FL) beyond 100m distance head(josm$counts[ josm$counts$SiteID == rownames(ytot)[which(ytot[,&quot;CANG&quot;] == 200)] &amp; josm$counts$SpeciesID == &quot;CANG&quot;,]) We can check overall mean counts round(sort(colMeans(ytot)), 4) ## BUFF BWTE COGO COHA DCCO GWTE HOLA NHOW NSHO ## 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 ## RTHU WWSC CANV NOPI GBHE GCTH GHOW LEOW NOHA ## 0.0000 0.0000 0.0000 0.0000 0.0002 0.0002 0.0002 0.0002 0.0002 ## RBGU BRBL CAGU AMCO BAEA BARS NESP NOGO NOPO ## 0.0002 0.0002 0.0002 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 ## NSWO RNDU SNBU VEER BEKI CSWA MERL SAVS SSHA ## 0.0004 0.0004 0.0004 0.0004 0.0007 0.0007 0.0007 0.0007 0.0007 ## MYWA AMKE BAOR OSPR SPGR WBNU AMGO AMWI BOWA ## 0.0007 0.0009 0.0009 0.0009 0.0009 0.0009 0.0011 0.0011 0.0011 ## CONI EAPH HOWR NRWS BLTE COGR EAKI GGOW NAWA ## 0.0011 0.0011 0.0011 0.0011 0.0013 0.0013 0.0013 0.0013 0.0013 ## COSN COTE FRGU MAWR FOTE KILL RTHA BADO BLBW ## 0.0013 0.0015 0.0015 0.0015 0.0015 0.0018 0.0020 0.0024 0.0024 ## AMBI PBGR SPSA AMPI BHCO BWHA SOSP RUBL MALL ## 0.0028 0.0028 0.0028 0.0028 0.0031 0.0037 0.0042 0.0044 0.0046 ## PUFI DOWO SORA LEYE ATTW HAWO RNGR BBWO BLJA ## 0.0048 0.0059 0.0068 0.0094 0.0096 0.0101 0.0101 0.0107 0.0134 ## BOGU AMCR EVGR RWBL OSFL LCSP TRES FOSP WEWP ## 0.0140 0.0166 0.0169 0.0169 0.0186 0.0193 0.0201 0.0217 0.0232 ## WIWA PIWO RECR SOSA YWAR GCKI BLPW CAWA SACR ## 0.0236 0.0256 0.0269 0.0269 0.0291 0.0304 0.0306 0.0315 0.0322 ## BTNW NOWA OCWA BRCR CCSP COLO PHVI CONW CEDW ## 0.0335 0.0341 0.0359 0.0381 0.0385 0.0387 0.0394 0.0429 0.0449 ## RUGR MOWA WAVI BCCH BOCH NOFL SWSP GRYE WWCR ## 0.0475 0.0477 0.0582 0.0593 0.0593 0.0622 0.0659 0.0685 0.0751 ## AMRO RBNU BBWA CMWA BHVI COYE YBFL YBSA AMRE ## 0.0757 0.0766 0.0810 0.0812 0.0814 0.0814 0.0873 0.0878 0.0889 ## BAWW LEFL WETA WISN CORA WIWR ALFL MAWA PISI ## 0.0963 0.0974 0.1086 0.1280 0.1401 0.1466 0.1582 0.1727 0.1775 ## RBGR LISP DEJU GRAJ CANG PAWA REVI RCKI HETH ## 0.1832 0.2169 0.2725 0.2898 0.3018 0.3053 0.3344 0.3898 0.4344 ## CHSP SWTH WTSP OVEN YRWA TEWA ## 0.4460 0.7402 0.8091 0.8831 0.8934 1.2221 2.3 Joining species data with predictors Let’s join the species counts with the survey attributes. This is how we can prepare the input data for regression analysis. spp &lt;- &quot;OVEN&quot; # which species josm$species[spp,] compare_sets(rownames(josm$surveys),rownames(ytot)) ## xlength ylength intersect union xbutnoty ybutnotx ## labels 4569 4569 4569 4569 0 0 ## unique 4569 4569 4569 4569 0 0 x &lt;- josm$surveys x$y &lt;- as.numeric(ytot[rownames(x), spp]) 2.4 Explore predictor variables Locations library(raster) library(sp) rr &lt;- stack(&quot;./_data/josm/landcover-hfi2016.grd&quot;) #&#39; Define CRS NAD83 for our sites xy &lt;- x[,c(&quot;Longitude&quot;, &quot;Latitude&quot;)] coordinates(xy) &lt;- ~ Longitude + Latitude proj4string(xy) &lt;- &quot;+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs&quot; xy &lt;- spTransform(xy, proj4string(rr)) col &lt;- colorRampPalette(c(&quot;lightgrey&quot;, &quot;blue&quot;))(100) plot(rr[[&quot;Water&quot;]], col=col, axes=FALSE, box=FALSE) plot(xy, add=TRUE, pch=19, cex=0.5) cn &lt;- c(&quot;Open&quot;, &quot;Water&quot;, &quot;Agr&quot;, &quot;UrbInd&quot;, &quot;SoftLin&quot;, &quot;Roads&quot;, &quot;Decid&quot;, &quot;OpenWet&quot;, &quot;Conif&quot;, &quot;ConifWet&quot;) #plot(x[,cn]) Add here: those kinds of transformations that are needed for regression need to add absolute links to figures??? Exercise: play with the data to understand the distributions use summary, table, hist, plot This is an exercise. This is a note. This is a warning. "],
["regression.html", "Chapter 3 A Primer in Regression Techniques", " Chapter 3 A Primer in Regression Techniques All models are wrong, but some are useful – Box lm, glm main effects, interactions, offsets lasso, brt, boot/bagging, glmm conditional and marginal effects maybe opticut cloglog motivation "],
["behavior.html", "Chapter 4 Behavioral Complexities", " Chapter 4 Behavioral Complexities Behaviour related stuff constant p (time as covariate) time varying p finite mix time varying p/c rate, count, time-to-event "],
["detection.html", "Chapter 5 The Detection Process", " Chapter 5 The Detection Process EDR, tau constant truncated, unlimited variable tau: habitat effect (continuous case?) discrete: land cover, observer effects contrast fixed effects with offsets – motivation for ARU "],
["recordings.html", "Chapter 6 Dealing with Recordings", " Chapter 6 Dealing with Recordings integration challenges calibration (exponential/cloglog approximation) fixed effects paired sensor sensitivity - EDR "],
["assumptions.html", "Chapter 7 A Closer Look at Assumptions", " Chapter 7 A Closer Look at Assumptions break thos assumptions "],
["roadsides.html", "Chapter 8 Understanding Roadside Surveys", " Chapter 8 Understanding Roadside Surveys directional diff in signal transmission "],
["extras.html", "Chapter 9 Miscellaneous Topics", " Chapter 9 Miscellaneous Topics model selection and conditional likelihood variance/bias trade off error propagation MCMC? N-mixture ideas phylogenetic and life history/trait stuff PIF methods "],
["references.html", "References", " References "]
]
